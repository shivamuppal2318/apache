{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HUYXLAeUuBG",
        "outputId": "0f80ea9d-e49c-4665-cbe5-d23ba8ff67ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjxcKIXv4R22"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZrvYQuY3gwG"
      },
      "source": [
        "data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load and preprocess dataset\n",
        "data = pd.read_csv('/content/Appended_DataFrame.csv')  # Update with your path\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove headers, replace formulas, and reformat lists for consistency\n",
        "    text = re.sub(r'#\\s+', '', text)  # Remove markdown headers\n",
        "    text = re.sub(r'\\$[^\\$]*\\$', 'The formula is given for reference', text)  # Replace dollar signs with placeholder\n",
        "    text = re.sub(r'\\\\\\[[^\\]]+\\\\\\]', 'The formula is given for reference', text)  # Replace \\[...\\] notation\n",
        "    text = re.sub(r'\\\\\\([^\\)]+\\\\\\)', 'The formula is given for reference', text)  # Replace \\( ... \\) notation\n",
        "    text = re.sub(r'[-•]\\s*', '', text)  # Remove bullet points\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Normalize whitespace\n",
        "    return text\n",
        "\n",
        "# Apply cleaning function to both 'input_text' and 'target_text'\n",
        "data['input_text'] = data['input_text'].apply(clean_text)\n",
        "data['target_text'] = data['target_text'].apply(clean_text)\n",
        "\n",
        "# Create Hugging Face Dataset\n",
        "dataset = Dataset.from_pandas(data)\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Tokenization function\n",
        "def preprocess_data(examples):\n",
        "    model_inputs = tokenizer(examples['input_text'], max_length=512, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(text_target=examples['target_text'], max_length=150, truncation=True, padding=\"max_length\")\n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_dataset = dataset.map(preprocess_data, batched=True)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./t5-text-to-text-model\",\n",
        "    eval_strategy=\"steps\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_steps=500,\n",
        "    save_steps=1000,\n",
        "    eval_steps=1000,\n",
        "    warmup_steps=500,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    eval_dataset=tokenized_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "trainer.save_model(\"./t5-text-to-text-models\")\n",
        "tokenizer.save_pretrained(\"./t5-text-to-text-models\")\n",
        "\n",
        "# Post-processing function for generated text\n",
        "def post_process_generated(text):\n",
        "    # Replace any remaining placeholders or list markers with a standardized format\n",
        "    text = text.replace('- ', '').replace('• ', '')  # Remove list bullets\n",
        "    text = re.sub(r'\\[ \\|\\|T\\|\\|\\]', 'The formula is given for reference', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Normalize whitespace\n",
        "    return text\n",
        "\n",
        "# Example: Generating output with post-processing\n",
        "def generate_and_process_text(input_text):\n",
        "    # Generate output\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
        "    outputs = model.generate(input_ids['input_ids'], max_length=300, num_beams=5, do_sample=True, temperature=0.7)\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "\n",
        "    # Apply post-processing\n",
        "    standardized_text = post_process_generated(generated_text)\n",
        "    return standardized_text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472,
          "referenced_widgets": [
            "2be751ffa7e54eac9f33c9d9a41c9b7f",
            "a499f0944d9b40a7859b247202804dbf",
            "cb4ec8df99874ba99bc0fd58c51100f3",
            "41a47a1e3d3a41dfa1e20955e4cc9fb3",
            "d7043368f7924fff9d9913112ba0a621",
            "e82a5cd5631b43f1bbd5939b8348db70",
            "529dd4c98e33465cbbac79dee268b11f",
            "6684db7c08924c9db346f5d1a85c7bf1",
            "1beae3538a354dd0b288225c86ef4552",
            "15e557ceb6b34435912ee042e3e254ce",
            "4dc80394ff5b4ea78dd2823f1ca362f8",
            "c5fdf389ba3d462183ea436421ccf03e",
            "31f985188c084e4c8160eb77f2a24923",
            "4f80833f2eca48d3b28d7c1f9b8507e3",
            "e17e0cd72a3543918599f0df078cbf1a",
            "b71aeb916d994c6abe88af6cf5c931ff",
            "f990a171a540447693c9656a35934c2d",
            "b12675307ca94efab642c3a9c7103e30",
            "832076281efc44849d870da8a138ba46",
            "e80872b417f64eb1968f54beb0331b76",
            "cb411dec5b84409c8483df24a1499f92",
            "0038fa46ec8943558032271cf7adf426",
            "511ec04965c04886aed63dd77ffdbeb0",
            "bfaab799ab064fdcb29ae3a1822335f4",
            "b48c281dc2c5423585c9aeedd70a6d94",
            "3f97cdb9813f409db003dcdbcf611086",
            "e3beed58c2b34e968b537bc831392f1c",
            "4b914b17c60f47f08f011e8d88e7843c",
            "791a20d89da24cba90f2bf3ebc1bc53c",
            "fc50cdaa4cee4ef2bba9fc4bf9d53db3",
            "b60f06da99d44f9d9edecb3a0f6e7e30",
            "acd32c7a771c42e3b9e55d32114c4514",
            "ad44de52f49e4a1490ff1ec802c76ffd",
            "764cdb0d489646cb8447745c76b9e2e8",
            "a7be8bb0d93f461a9c97942aaf2aa9a8",
            "e2db5d58ae924efe9818cadfe1bc4d76",
            "d474f84f396b4857be788ce9c10d4636",
            "8cef8103fb4c49ffbcfd66685d29857e",
            "87cfae86f8a449baa65808d96891a9fa",
            "2fd90303b52942838f0226eab2e01105",
            "187554b2792a468591e5f5060373ae0e",
            "6d4cacc0eb0b40ee80dcd6bf50ac13bd",
            "e41778f495e6448da8c709ce5d4d99dc",
            "77feb887c4d54a94a6d417888bdd202f",
            "3a7aad14e9b745e9839e58c13012f8ea",
            "4a9e68ad6dec4e779c74960dffa659fb",
            "07d3417818fe4de7a149cfd9e955ea51",
            "576601504fab49b5a1a7f606fe0fa068",
            "84564f5761ed486b9f204f5745818c00",
            "43b31bcc4c4742be9f7021bb4f900046",
            "cd6b7446965a456aa348dd73e5fcc74a",
            "534a48d856e44e8c95f8882f8d2100cc",
            "eb197c37f0904b7e8f62e6b4ba0ea824",
            "36ef4a6bcd5748a594c5456559e051dc",
            "abe70b5e933d454186a08bbdfbdee6d5",
            "4a66667e40594cc3a5633ffb382528b1",
            "d1b2366cb2b340e59d4be5595cb60386",
            "a4ce80304364449088444e7ff1b045d9",
            "87f7bc3a5ace408aabde43a227c4ba8b",
            "420530e8e7db489c812110453346ae95",
            "827678ed8472458eb87bff3900192650",
            "271622ac2ba94c84b373e8e3d7a095f0",
            "7befd46485f742518b18eb299973aa83",
            "1434a0287f42405d9cf97e23a53d95ee",
            "e761bcda8c804b2192970f0803696eb2",
            "940352a1030a4ad58398826b29d3d408",
            "73ff14f5da5d4497a82e55279cf7c6b9",
            "0035381b00cb4ac9b208272050a3c38b",
            "0f576a0c5ad741f6a82e57bbca766133",
            "a0d8f05d0f7b40778f08dd0508e01b95",
            "5aa096f7cf7143e59ae1555b5605cef3",
            "8c89f46af55449ea865388d5f0a3fc47",
            "034e4963fe684f0fbfc23536de44cb25",
            "1a54b74525be4537b769d90d79b9b4f5",
            "38d2d6ef3c6b4b1ab5a806012fc0109b",
            "692e173ac3bf427bb27ff20989c7bf1a",
            "93dcc3f0a1ce491b826b57b6df1e48e6"
          ]
        },
        "id": "uaGFDoweKVqT",
        "outputId": "23aa089f-4d61-42da-f105-d11a8852faeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2be751ffa7e54eac9f33c9d9a41c9b7f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5fdf389ba3d462183ea436421ccf03e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "511ec04965c04886aed63dd77ffdbeb0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "764cdb0d489646cb8447745c76b9e2e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a7aad14e9b745e9839e58c13012f8ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a66667e40594cc3a5633ffb382528b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/10500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "73ff14f5da5d4497a82e55279cf7c6b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1313' max='1313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1313/1313 10:58, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.003800</td>\n",
              "      <td>0.000325</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"./t5-text-to-text-models\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"./t5-text-to-text-models\")\n"
      ],
      "metadata": {
        "id": "7O5cP8kLDlAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"/content/Appended_DataFrame.csv\")"
      ],
      "metadata": {
        "id": "m1n-UIJaDuFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bert_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-a7HmTzoc1AM",
        "outputId": "a04b1cba-8ac9-48b3-9040-108bfdb38f4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bert_score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.5.0+cu121)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.2.2)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.44.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.66.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (24.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2024.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.24.7)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.19.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
            "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bert_score\n",
            "Successfully installed bert_score-0.3.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "33CPHZ0qD00Z",
        "outputId": "8037e02e-ef35-42b1-9579-e35d84ee53c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              input_text  \\\n",
              "0      # Introduction to Quantum Field Theory\\n\\nQuan...   \n",
              "1      # Introduction to Group Theory\\n\\nGroup Theory...   \n",
              "2      # Introduction to Optimization Theory\\n\\nOptim...   \n",
              "3      # Introduction to Heisenberg Uncertainty Princ...   \n",
              "4      # Introduction to Topology\\n\\nTopology is a to...   \n",
              "...                                                  ...   \n",
              "10495  ## Basics of Simulated Annealing\\n\\nSimulated ...   \n",
              "10496  ## Basics of Cloud Storage\\n\\nCloud Storage is...   \n",
              "10497  ## Basics of Genetic Algorithms\\n\\nGenetic Alg...   \n",
              "10498  ## Basics of Dimensionality Reduction\\n\\nDimen...   \n",
              "10499  ## Basics of Bayesian Inference\\n\\nBayesian In...   \n",
              "\n",
              "                                             target_text  \n",
              "0      Introduction to Quantum Field Theory. Quantum ...  \n",
              "1      Introduction to Group Theory. Group Theory is ...  \n",
              "2      Introduction to Optimization Theory. Optimizat...  \n",
              "3      Introduction to Heisenberg Uncertainty Princip...  \n",
              "4      Introduction to Topology. Topology is a topic ...  \n",
              "...                                                  ...  \n",
              "10495  Basics of Simulated Annealing. Simulated Annea...  \n",
              "10496  Basics of Cloud Storage. Cloud Storage focuses...  \n",
              "10497  Basics of Genetic Algorithms. Genetic Algorith...  \n",
              "10498  Basics of Dimensionality Reduction. Dimensiona...  \n",
              "10499  Basics of Bayesian Inference. Bayesian Inferen...  \n",
              "\n",
              "[10500 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-45be023a-e38a-4f38-a7f7-f90e89a17f99\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input_text</th>\n",
              "      <th>target_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td># Introduction to Quantum Field Theory\\n\\nQuan...</td>\n",
              "      <td>Introduction to Quantum Field Theory. Quantum ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td># Introduction to Group Theory\\n\\nGroup Theory...</td>\n",
              "      <td>Introduction to Group Theory. Group Theory is ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td># Introduction to Optimization Theory\\n\\nOptim...</td>\n",
              "      <td>Introduction to Optimization Theory. Optimizat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td># Introduction to Heisenberg Uncertainty Princ...</td>\n",
              "      <td>Introduction to Heisenberg Uncertainty Princip...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td># Introduction to Topology\\n\\nTopology is a to...</td>\n",
              "      <td>Introduction to Topology. Topology is a topic ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10495</th>\n",
              "      <td>## Basics of Simulated Annealing\\n\\nSimulated ...</td>\n",
              "      <td>Basics of Simulated Annealing. Simulated Annea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10496</th>\n",
              "      <td>## Basics of Cloud Storage\\n\\nCloud Storage is...</td>\n",
              "      <td>Basics of Cloud Storage. Cloud Storage focuses...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10497</th>\n",
              "      <td>## Basics of Genetic Algorithms\\n\\nGenetic Alg...</td>\n",
              "      <td>Basics of Genetic Algorithms. Genetic Algorith...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10498</th>\n",
              "      <td>## Basics of Dimensionality Reduction\\n\\nDimen...</td>\n",
              "      <td>Basics of Dimensionality Reduction. Dimensiona...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10499</th>\n",
              "      <td>## Basics of Bayesian Inference\\n\\nBayesian In...</td>\n",
              "      <td>Basics of Bayesian Inference. Bayesian Inferen...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10500 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-45be023a-e38a-4f38-a7f7-f90e89a17f99')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-45be023a-e38a-4f38-a7f7-f90e89a17f99 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-45be023a-e38a-4f38-a7f7-f90e89a17f99');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-14f37714-8659-41ed-89e2-8076737aba80\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-14f37714-8659-41ed-89e2-8076737aba80')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-14f37714-8659-41ed-89e2-8076737aba80 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_19426599-4d36-4482-bbf9-8f33827b4ecc\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_19426599-4d36-4482-bbf9-8f33827b4ecc button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10500,\n  \"fields\": [\n    {\n      \"column\": \"input_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10355,\n        \"samples\": [\n          \"## Basics of Artificial Intelligence\\n\\nArtificial Intelligence is a fundamental concept in the field of computing, focusing on solving problems or building models. It is often applied in various fields and tasks.\\n\\n### Key Properties\\n\\n- Logistic Regression\\n- Random Forest\\n- Decision Trees\",\n          \"## Basics of Recurrent Neural Networks\\n\\nRecurrent Neural Networks is a fundamental concept in the field of computing, focusing on solving problems or building models. It is often applied in various fields and tasks.\\n\\n### Key Techniques\\n\\n- XGBoost\\n- Random Forest\\n- YOLO\",\n          \"## Basics of Big-O Notation\\n\\nBig-O Notation is a fundamental concept in the field of computing, focusing on solving problems or building models. It is often applied in various fields and tasks.\\n\\n### Key Properties\\n\\n- Support Vector Machines\\n- XGBoost\\n- Logistic Regression\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10082,\n        \"samples\": [\n          \"Basics of Hyperparameter Tuning. Hyperparameter Tuning focuses on solving problems and building models. Popular Algorithms. Decision Trees. K-Means. YOLO.\",\n          \"Introduction to Chaos Theory. Chaos Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Control Systems. Artificial Intelligence. Decision Making.\",\n          \"Basics of Time Complexity. Time Complexity focuses on solving problems and building models. Methods. Random Forest. XGBoost. Logistic Regression.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from bert_score import score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"./t5-text-to-text-models\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"./t5-text-to-text-models\")\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/Appended_DataFrame.csv')\n",
        "\n",
        "# Store the BERTScore F1 results\n",
        "bert_scores = []\n",
        "\n",
        "# Generate predictions and calculate BERTScore for each row in the dataset\n",
        "for index, row in data.iterrows():\n",
        "    # Tokenize the input text and generate output\n",
        "    input_ids = tokenizer(row['input_text'], return_tensors=\"pt\", max_length=512, truncation=True).input_ids\n",
        "    outputs = model.generate(input_ids, max_length=150, num_beams=5, do_sample=True)\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Calculate BERTScore between generated text and target text\n",
        "    P, R, F1 = score([generated_text], [row['target_text']], lang=\"en\", model_type=\"bert-base-uncased\")\n",
        "    bert_score_f1 = F1.mean().item()\n",
        "    bert_scores.append(bert_score_f1)  # Store the F1 score from BERTScore\n",
        "\n",
        "    # Print each sample's details\n",
        "    print(f\"Sample {index + 1}:\")\n",
        "    print(f\"Input Text:\\n{row['input_text']}\")\n",
        "    print(f\"Generated Text:\\n{generated_text}\")\n",
        "    print(f\"Target Text:\\n{row['target_text']}\")\n",
        "    print(f\"BERTScore F1: {bert_score_f1:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Plot the BERTScore F1 scores for each sample\n",
        "plt.plot(range(1, len(bert_scores) + 1), bert_scores, marker=\"o\", linestyle=\"-\")\n",
        "plt.xlabel(\"Sample Number\")\n",
        "plt.ylabel(\"BERTScore F1 Score\")\n",
        "plt.title(\"BERTScore F1 Accuracy for Each Sample in the Dataset\")\n",
        "plt.xticks(range(1, len(bert_scores) + 1, max(1, len(bert_scores) // 10)))  # Adjust ticks for readability\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "467a6a99fabf4f5f8c37bd87a97f2d37",
            "9c26c656d07a47d78bcc452b5c3285af",
            "e7a7ac5bccf6499a90c95d6771f13910",
            "57ca60d1a4b84e1fae7034caced12099",
            "bbf74723661044fda0a832298552d5cb",
            "973ed182b1f14ed6be7970d4e2678e79",
            "01a242f2cd274a3daa5a1a21617ea663",
            "6621b54218854e4d9f9a1b6cb5a75d8d",
            "15eec6e76b63413c8cb8bd00379d621f",
            "808ad8ff0c8a4a0eab823b32fdd6503e",
            "ae2a59e7982542f6ba7ebab55ab9cbe6",
            "6a0d0097137f4d94b490d3acfdb2cc71",
            "e14e81d7562f4f70b72bfaf7f919e445",
            "0fc8e57d2d1648c19a93fff6bcb95468",
            "7c0cbf6d9f9040cfa170b3f43f3d8e2a",
            "0a2b483525164d7c8350a6e94c36573e",
            "1cf77318d87341ab8edeb59e8f2b0aaa",
            "3e9446d67e57430596a427551daf5cae",
            "c5f26cc305d54274824df5118487f26a",
            "05d7396bf5f348fe97f5d2986c537aee",
            "f827b768769a4d52a874af47b65acce3",
            "4e53a114668941ebadc2f38864a909d4",
            "3b65afa1aafb4460b909a60b45c83ff5",
            "97019f1cd07c494683833977b79094b2",
            "83179dd508d641739dee74ba81362093",
            "438556270ef741f0b54f3413e37f1a09",
            "48f907265fcf4b08a0fe0113faeac155",
            "8d8634f690bc4939a4df56a4b1496286",
            "b877e6a785a94012b23d2b4453576a7f",
            "75bd744823944abdbc7d2ff3905a5f9c",
            "9fe5c99d30dc433bb7c244ccd13b691d",
            "f39326c0bf07459fb35a98edcc6c4dcb",
            "25e08146d407465b9253ccb4cdb718eb",
            "37ece95b21ab4deaa2f8c615724a266c",
            "4901624e2277455f9bcf171362842650",
            "2bcbefe6986a4c45b4c3ea414f8833f1",
            "44f42dc002ca48c296f95c7854c4354a",
            "0d6255c74c4247d787c8bd3845eaa9b0",
            "a8ea451a4c3e44869794c25926936dfe",
            "cfdc521b550b40ba8334ee269f39980d",
            "6010a85bb46e45fab35dcc9fc5c3ce42",
            "9580fbaefcca47e4a031a292c4810b4e",
            "3cfa03ce118d42f782ff629d8b9f9360",
            "696e483a17c94da0b0921e0883615dea",
            "6fe0fa471d5c4da38ddd7103e6405173",
            "24e79d9b468c4b34bfa9e98c8d62deb4",
            "2b2410badb0546339ebb6818a9284a53",
            "51865a4bcd2b4ca793a1a35a7ca58bf3",
            "d078e1e5003a4a02972203414f9a2e2c",
            "c1906ea5a4e64ce78ed2468dee20eb6a",
            "e405d885425a44fe9c20d5335fa44d9d",
            "99b8e5c1ae7b44848b6662693f6ab098",
            "151429d9919847119871bde878c50097",
            "a353d6f3d8a64c47950545ea644d730c",
            "b69e58038c764c4d92f8de1c0b361f23"
          ]
        },
        "id": "EWdZed-gYVAc",
        "outputId": "c6298a60-e1b9-4a6c-d5f2-e0fd87e2cb81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "467a6a99fabf4f5f8c37bd87a97f2d37"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a0d0097137f4d94b490d3acfdb2cc71"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b65afa1aafb4460b909a60b45c83ff5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37ece95b21ab4deaa2f8c615724a266c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6fe0fa471d5c4da38ddd7103e6405173"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 1:\n",
            "Input Text:\n",
            "# Introduction to Quantum Field Theory\n",
            "\n",
            "Quantum Field Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ ||T|| = \\sup_{||x||=1} ||T(x)|| \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Cryptography\n",
            "- Polynomial Algebra\n",
            "- Number Theory\n",
            "Generated Text:\n",
            "Introduction to Quantum Field Theory. Quantum Field Theory is a topic that involves deep mathematical concepts. Applications. - Cryptography. - Polynomial Algebra. - Number Theory.\n",
            "Target Text:\n",
            "Introduction to Quantum Field Theory. Quantum Field Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Cryptography. Polynomial Algebra. Number Theory.\n",
            "BERTScore F1: 0.8829\n",
            "--------------------------------------------------\n",
            "Sample 2:\n",
            "Input Text:\n",
            "# Introduction to Group Theory\n",
            "\n",
            "Group Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ T = T^i_j \\partial_i dx^j \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Quantum Mechanics\n",
            "- Particle Physics\n",
            "- Quantum Electrodynamics\n",
            "Generated Text:\n",
            "Introduction to Group Theory. Group Theory is a topic that involves deep mathematical concepts. Applications. - Quantum Mechanics. - Particle Physics. Quantum Electrodynamics.\n",
            "Target Text:\n",
            "Introduction to Group Theory. Group Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Quantum Mechanics. Particle Physics. Quantum Electrodynamics.\n",
            "BERTScore F1: 0.8995\n",
            "--------------------------------------------------\n",
            "Sample 3:\n",
            "Input Text:\n",
            "# Introduction to Optimization Theory\n",
            "\n",
            "Optimization Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\frac{\\partial u}{\\partial x} = \\frac{\\partial v}{\\partial y}, \\quad \\frac{\\partial u}{\\partial y} = -\\frac{\\partial v}{\\partial x} \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Cryptography\n",
            "- Polynomial Algebra\n",
            "- Number Theory\n",
            "Generated Text:\n",
            "Introduction to Optimization Theory. Optimization Theory is a topic that involves deep mathematical concepts. Applications. - Cryptography. - Polynomial Algebra. - Number Theory.\n",
            "Target Text:\n",
            "Introduction to Optimization Theory. Optimization Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Cryptography. Polynomial Algebra. Number Theory.\n",
            "BERTScore F1: 0.8762\n",
            "--------------------------------------------------\n",
            "Sample 4:\n",
            "Input Text:\n",
            "# Introduction to Heisenberg Uncertainty Principle\n",
            "\n",
            "Heisenberg Uncertainty Principle is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ L = T - V \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Cryptography\n",
            "- Polynomial Algebra\n",
            "- Number Theory\n",
            "Generated Text:\n",
            "Introduction to Heisenberg Uncertainty Principle. Heisenberg Uncertainty Principle is a topic that involves deep mathematical concepts. Applications. - Cryptography. - Polynomial Algebra. - Number Theory.\n",
            "Target Text:\n",
            "Introduction to Heisenberg Uncertainty Principle. Heisenberg Uncertainty Principle is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Cryptography. Polynomial Algebra. Number Theory.\n",
            "BERTScore F1: 0.8945\n",
            "--------------------------------------------------\n",
            "Sample 5:\n",
            "Input Text:\n",
            "# Introduction to Topology\n",
            "\n",
            "Topology is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\mu(\\cup A_i) = \\sum \\mu(A_i) \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Cryptography\n",
            "- Polynomial Algebra\n",
            "- Number Theory\n",
            "Generated Text:\n",
            "Introduction to Topology. Topology is a topic that involves deep mathematical concepts. Applications. - Cryptography. - Polynomial Algebra. - Number Theory.\n",
            "Target Text:\n",
            "Introduction to Topology. Topology is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Cryptography. Polynomial Algebra. Number Theory.\n",
            "BERTScore F1: 0.8694\n",
            "--------------------------------------------------\n",
            "Sample 6:\n",
            "Input Text:\n",
            "# Introduction to Measure Theory\n",
            "\n",
            "Measure Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ (G, \\cdot) \\quad \\text{such that } a\\cdot(b\\cdot c) = (a\\cdot b)\\cdot c \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Signal Processing\n",
            "- Image Processing\n",
            "- Quantum Mechanics\n",
            "Generated Text:\n",
            "Introduction to Measure Theory. Measure Theory is a topic that involves deep mathematical concepts. Applications. - Signal Processing. - Image Processing. - Quantum Mechanics.\n",
            "Target Text:\n",
            "Introduction to Measure Theory. Measure Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Signal Processing. Image Processing. Quantum Mechanics.\n",
            "BERTScore F1: 0.8798\n",
            "--------------------------------------------------\n",
            "Sample 7:\n",
            "Input Text:\n",
            "# Introduction to Covariant Derivatives\n",
            "\n",
            "Covariant Derivatives is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ (\\Omega, \\mathcal{F}, P) \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Control Systems\n",
            "- Artificial Intelligence\n",
            "- Decision Making\n",
            "Generated Text:\n",
            "Introduction to Covariant Derivatives. Covariant Derivatives is a topic that involves deep mathematical concepts. Applications. - Control Systems. - Artificial Intelligence. - Decision Making.\n",
            "Target Text:\n",
            "Introduction to Covariant Derivatives. Covariant Derivatives is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Control Systems. Artificial Intelligence. Decision Making.\n",
            "BERTScore F1: 0.8923\n",
            "--------------------------------------------------\n",
            "Sample 8:\n",
            "Input Text:\n",
            "# Introduction to Probability Spaces\n",
            "\n",
            "Probability Spaces is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\min_{x \\in X} f(x) \\quad \\text{subject to constraints} \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Machine Learning\n",
            "- Operations Research\n",
            "- Economics\n",
            "Generated Text:\n",
            "Introduction to Probability Spaces. Probability Spaces is a topic that involves deep mathematical concepts. Applications. Machine Learning. Operations Research. Economics.\n",
            "Target Text:\n",
            "Introduction to Probability Spaces. Probability Spaces is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Machine Learning. Operations Research. Economics.\n",
            "BERTScore F1: 0.9197\n",
            "--------------------------------------------------\n",
            "Sample 9:\n",
            "Input Text:\n",
            "# Introduction to Galois Theory\n",
            "\n",
            "Galois Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ (R, +, \\cdot) \\quad \\text{such that} \\quad a + b = b + a, \\ a\\cdot(b + c) = a\\cdot b + a\\cdot c \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Quantum Mechanics\n",
            "- Particle Physics\n",
            "- Quantum Electrodynamics\n",
            "Generated Text:\n",
            "Introduction to Galois Theory. Galois Theory is a topic that involves deep mathematical concepts. Applications. - Quantum Mechanics. - Particle Physics. Quantum Electrodynamics.\n",
            "Target Text:\n",
            "Introduction to Galois Theory. Galois Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Quantum Mechanics. Particle Physics. Quantum Electrodynamics.\n",
            "BERTScore F1: 0.9046\n",
            "--------------------------------------------------\n",
            "Sample 10:\n",
            "Input Text:\n",
            "# Introduction to Differential Forms\n",
            "\n",
            "Differential Forms is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ T = T^i_j \\partial_i dx^j \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Cryptography\n",
            "- Polynomial Algebra\n",
            "- Number Theory\n",
            "Generated Text:\n",
            "Introduction to Differential Forms. Differential Forms is a topic that involves deep mathematical concepts. Applications. - Cryptography. - Polynomial Algebra. - Number Theory.\n",
            "Target Text:\n",
            "Introduction to Differential Forms. Differential Forms is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Cryptography. Polynomial Algebra. Number Theory.\n",
            "BERTScore F1: 0.8778\n",
            "--------------------------------------------------\n",
            "Sample 11:\n",
            "Input Text:\n",
            "# Introduction to Chaos Theory\n",
            "\n",
            "Chaos Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\mu(\\cup A_i) = \\sum \\mu(A_i) \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Cryptography\n",
            "- Polynomial Algebra\n",
            "- Number Theory\n",
            "Generated Text:\n",
            "Introduction to Chaos Theory. Chaos Theory is a topic that involves deep mathematical concepts. Applications. - Cryptography. - Polynomial Algebra. - Number Theory.\n",
            "Target Text:\n",
            "Introduction to Chaos Theory. Chaos Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Cryptography. Polynomial Algebra. Number Theory.\n",
            "BERTScore F1: 0.8758\n",
            "--------------------------------------------------\n",
            "Sample 12:\n",
            "Input Text:\n",
            "# Introduction to Variational Calculus\n",
            "\n",
            "Variational Calculus is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ (G, \\cdot) \\quad \\text{such that } a\\cdot(b\\cdot c) = (a\\cdot b)\\cdot c \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Machine Learning\n",
            "- Operations Research\n",
            "- Economics\n",
            "Generated Text:\n",
            "Introduction to Variational Calculus. Variational Calculus is a topic that involves deep mathematical concepts. Applications. Machine Learning. Operations Research. Economics.\n",
            "Target Text:\n",
            "Introduction to Variational Calculus. Variational Calculus is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Machine Learning. Operations Research. Economics.\n",
            "BERTScore F1: 0.9261\n",
            "--------------------------------------------------\n",
            "Sample 13:\n",
            "Input Text:\n",
            "# Introduction to Algebraic Geometry\n",
            "\n",
            "Algebraic Geometry is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\omega = f(x,y) dx + g(x,y) dy \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Cryptography\n",
            "- Polynomial Algebra\n",
            "- Number Theory\n",
            "Generated Text:\n",
            "Introduction to Algebraic Geometry. Algebraic Geometry is a topic that involves deep mathematical concepts. Applications. - Cryptography. - Polynomial Algebra. - Number Theory.\n",
            "Target Text:\n",
            "Introduction to Algebraic Geometry. Algebraic Geometry is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Cryptography. Polynomial Algebra. Number Theory.\n",
            "BERTScore F1: 0.8776\n",
            "--------------------------------------------------\n",
            "Sample 14:\n",
            "Input Text:\n",
            "# Introduction to Optimization Theory\n",
            "\n",
            "Optimization Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ (R, +, \\cdot) \\quad \\text{such that} \\quad a + b = b + a, \\ a\\cdot(b + c) = a\\cdot b + a\\cdot c \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Quantum Mechanics\n",
            "- Particle Physics\n",
            "- Quantum Electrodynamics\n",
            "Generated Text:\n",
            "Introduction to Optimization Theory. Optimization Theory is a topic that involves deep mathematical concepts. Applications. - Quantum Mechanics. - Particle Physics. Quantum Electrodynamics.\n",
            "Target Text:\n",
            "Introduction to Optimization Theory. Optimization Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Quantum Mechanics. Particle Physics. Quantum Electrodynamics.\n",
            "BERTScore F1: 0.8992\n",
            "--------------------------------------------------\n",
            "Sample 15:\n",
            "Input Text:\n",
            "# Introduction to Knot Theory\n",
            "\n",
            "Knot Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\nabla_v X = v^i \\left( \\frac{\\partial X^j}{\\partial x^i} + \\Gamma^j_{ik} X^k \\right) e_j \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Machine Learning\n",
            "- Operations Research\n",
            "- Economics\n",
            "Generated Text:\n",
            "Introduction to Knot Theory. Knot Theory is a topic that involves deep mathematical concepts. Applications. Machine Learning. Operations Research. Economics.\n",
            "Target Text:\n",
            "Introduction to Knot Theory. Knot Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Machine Learning. Operations Research. Economics.\n",
            "BERTScore F1: 0.9172\n",
            "--------------------------------------------------\n",
            "Sample 16:\n",
            "Input Text:\n",
            "# Introduction to Green's Theorem\n",
            "\n",
            "Green's Theorem is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\hat{\\phi}(x,t) = \\int \\frac{d^3k}{(2\\pi)^3} \\frac{1}{\\sqrt{2E_k}} \\left( a_k e^{-ikx} + a_k^\\dagger e^{ikx} \\right) \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Control Systems\n",
            "- Artificial Intelligence\n",
            "- Decision Making\n",
            "Generated Text:\n",
            "Introduction to Green's Theorem. Green's Theorem is a topic that involves deep mathematical concepts. Applications. - Control Systems. - Artificial Intelligence. - Decision Making.\n",
            "Target Text:\n",
            "Introduction to Green's Theorem. Green's Theorem is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Control Systems. Artificial Intelligence. Decision Making.\n",
            "BERTScore F1: 0.8898\n",
            "--------------------------------------------------\n",
            "Sample 17:\n",
            "Input Text:\n",
            "# Introduction to Variational Calculus\n",
            "\n",
            "Variational Calculus is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ (\\Omega, \\mathcal{F}, P) \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Physics\n",
            "- Differential Geometry\n",
            "- Algebraic Geometry\n",
            "Generated Text:\n",
            "Introduction to Variational Calculus. Variational Calculus is a topic that involves deep mathematical concepts. Applications. - Physics. - Differential Geometry. - Algebraic Geometry.\n",
            "Target Text:\n",
            "Introduction to Variational Calculus. Variational Calculus is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Physics. Differential Geometry. Algebraic Geometry.\n",
            "BERTScore F1: 0.8788\n",
            "--------------------------------------------------\n",
            "Sample 18:\n",
            "Input Text:\n",
            "# Introduction to Fourier Transform\n",
            "\n",
            "Fourier Transform is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\frac{\\partial u}{\\partial x} = \\frac{\\partial v}{\\partial y}, \\quad \\frac{\\partial u}{\\partial y} = -\\frac{\\partial v}{\\partial x} \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Quantum Mechanics\n",
            "- Particle Physics\n",
            "- Quantum Electrodynamics\n",
            "Generated Text:\n",
            "Introduction to Fourier Transform. Fourier Transform is a topic that involves deep mathematical concepts. Applications. - Quantum Mechanics. - Particle Physics. Quantum Electrodynamics.\n",
            "Target Text:\n",
            "Introduction to Fourier Transform. Fourier Transform is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Quantum Mechanics. Particle Physics. Quantum Electrodynamics.\n",
            "BERTScore F1: 0.9020\n",
            "--------------------------------------------------\n",
            "Sample 19:\n",
            "Input Text:\n",
            "# Introduction to Topology\n",
            "\n",
            "Topology is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\hat{\\phi}(x,t) = \\int \\frac{d^3k}{(2\\pi)^3} \\frac{1}{\\sqrt{2E_k}} \\left( a_k e^{-ikx} + a_k^\\dagger e^{ikx} \\right) \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Signal Processing\n",
            "- Image Processing\n",
            "- Quantum Mechanics\n",
            "Generated Text:\n",
            "Introduction to Topology. Topology is a topic that involves deep mathematical concepts. Applications. - Signal Processing. - Image Processing. Quantum Mechanics.\n",
            "Target Text:\n",
            "Introduction to Topology. Topology is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Signal Processing. Image Processing. Quantum Mechanics.\n",
            "BERTScore F1: 0.8845\n",
            "--------------------------------------------------\n",
            "Sample 20:\n",
            "Input Text:\n",
            "# Introduction to Fuzzy Logic\n",
            "\n",
            "Fuzzy Logic is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\nabla_v X = v^i \\left( \\frac{\\partial X^j}{\\partial x^i} + \\Gamma^j_{ik} X^k \\right) e_j \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Quantum Mechanics\n",
            "- Particle Physics\n",
            "- Quantum Electrodynamics\n",
            "Generated Text:\n",
            "Introduction to Fuzzy Logic. Fuzzy Logic is a topic that involves deep mathematical concepts. Applications. - Quantum Mechanics. - Particle Physics. Quantum Electrodynamics.\n",
            "Target Text:\n",
            "Introduction to Fuzzy Logic. Fuzzy Logic is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Quantum Mechanics. Particle Physics. Quantum Electrodynamics.\n",
            "BERTScore F1: 0.9007\n",
            "--------------------------------------------------\n",
            "Sample 21:\n",
            "Input Text:\n",
            "# Introduction to Chaos Theory\n",
            "\n",
            "Chaos Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\mu_A(x) \\in [0,1] \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Physics\n",
            "- Differential Geometry\n",
            "- Algebraic Geometry\n",
            "Generated Text:\n",
            "Introduction to Chaos Theory. Chaos Theory is a topic that involves deep mathematical concepts. Applications. - Physics. - Differential Geometry. - Algebraic Geometry.\n",
            "Target Text:\n",
            "Introduction to Chaos Theory. Chaos Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Physics. Differential Geometry. Algebraic Geometry.\n",
            "BERTScore F1: 0.8695\n",
            "--------------------------------------------------\n",
            "Sample 22:\n",
            "Input Text:\n",
            "# Introduction to Quantum Field Theory\n",
            "\n",
            "Quantum Field Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ ||T|| = \\sup_{||x||=1} ||T(x)|| \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Quantum Mechanics\n",
            "- Particle Physics\n",
            "- Quantum Electrodynamics\n",
            "Generated Text:\n",
            "Introduction to Quantum Field Theory. Quantum Field Theory is a topic that involves deep mathematical concepts. Applications. - Quantum Mechanics. - Particle Physics. Quantum Electrodynamics.\n",
            "Target Text:\n",
            "Introduction to Quantum Field Theory. Quantum Field Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Quantum Mechanics. Particle Physics. Quantum Electrodynamics.\n",
            "BERTScore F1: 0.9030\n",
            "--------------------------------------------------\n",
            "Sample 23:\n",
            "Input Text:\n",
            "# Introduction to Fourier Transform\n",
            "\n",
            "Fourier Transform is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\mu_A(x) \\in [0,1] \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Classical Mechanics\n",
            "- Quantum Mechanics\n",
            "- Control Systems\n",
            "Generated Text:\n",
            "Introduction to Fourier Transform. Fourier Transform is a topic that involves deep mathematical concepts. Applications. Classical Mechanics. - Quantum Mechanics. - Control Systems.\n",
            "Target Text:\n",
            "Introduction to Fourier Transform. Fourier Transform is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Classical Mechanics. Quantum Mechanics. Control Systems.\n",
            "BERTScore F1: 0.8910\n",
            "--------------------------------------------------\n",
            "Sample 24:\n",
            "Input Text:\n",
            "# Introduction to Optimization Theory\n",
            "\n",
            "Optimization Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ T = T^i_j \\partial_i dx^j \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Machine Learning\n",
            "- Operations Research\n",
            "- Economics\n",
            "Generated Text:\n",
            "Introduction to Optimization Theory. Optimization Theory is a topic that involves deep mathematical concepts.\n",
            "Target Text:\n",
            "Introduction to Optimization Theory. Optimization Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Machine Learning. Operations Research. Economics.\n",
            "BERTScore F1: 0.8169\n",
            "--------------------------------------------------\n",
            "Sample 25:\n",
            "Input Text:\n",
            "# Introduction to Hyperbolic Geometry\n",
            "\n",
            "Hyperbolic Geometry is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\mu(\\cup A_i) = \\sum \\mu(A_i) \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Classical Mechanics\n",
            "- Quantum Mechanics\n",
            "- Control Systems\n",
            "Generated Text:\n",
            "Introduction to Hyperbolic Geometry. Hyperbolic Geometry is a topic that involves deep mathematical concepts. Applications. Classical Mechanics. - Quantum Mechanics. - Control Systems.\n",
            "Target Text:\n",
            "Introduction to Hyperbolic Geometry. Hyperbolic Geometry is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Classical Mechanics. Quantum Mechanics. Control Systems.\n",
            "BERTScore F1: 0.8960\n",
            "--------------------------------------------------\n",
            "Sample 26:\n",
            "Input Text:\n",
            "# Introduction to Covariant Derivatives\n",
            "\n",
            "Covariant Derivatives is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ (\\Omega, \\mathcal{F}, P) \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Physics\n",
            "- Differential Geometry\n",
            "- Algebraic Geometry\n",
            "Generated Text:\n",
            "Introduction to Covariant Derivatives. Covariant Derivatives is a topic that involves deep mathematical concepts. Applications. - Physics. - Differential Geometry. - Algebraic Geometry.\n",
            "Target Text:\n",
            "Introduction to Covariant Derivatives. Covariant Derivatives is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Physics. Differential Geometry. Algebraic Geometry.\n",
            "BERTScore F1: 0.8876\n",
            "--------------------------------------------------\n",
            "Sample 27:\n",
            "Input Text:\n",
            "# Introduction to Hyperbolic Geometry\n",
            "\n",
            "Hyperbolic Geometry is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ V(I) = \\{x \\in \\mathbb{A}^n \\mid f(x) = 0, \\forall f \\in I\\} \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Machine Learning\n",
            "- Operations Research\n",
            "- Economics\n",
            "Generated Text:\n",
            "Introduction to Hyperbolic Geometry. Hyperbolic Geometry is a topic that involves deep mathematical concepts. Applications. Machine Learning. Operations Research. Economics.\n",
            "Target Text:\n",
            "Introduction to Hyperbolic Geometry. Hyperbolic Geometry is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Machine Learning. Operations Research. Economics.\n",
            "BERTScore F1: 0.9262\n",
            "--------------------------------------------------\n",
            "Sample 28:\n",
            "Input Text:\n",
            "# Introduction to Galois Theory\n",
            "\n",
            "Galois Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ (\\Omega, \\mathcal{F}, P) \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Cryptography\n",
            "- Polynomial Algebra\n",
            "- Number Theory\n",
            "Generated Text:\n",
            "Introduction to Galois Theory. Galois Theory is a topic that involves deep mathematical concepts. Applications. - Cryptography. - Polynomial Algebra. - Number Theory.\n",
            "Target Text:\n",
            "Introduction to Galois Theory. Galois Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Cryptography. Polynomial Algebra. Number Theory.\n",
            "BERTScore F1: 0.8822\n",
            "--------------------------------------------------\n",
            "Sample 29:\n",
            "Input Text:\n",
            "# Introduction to Galois Theory\n",
            "\n",
            "Galois Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\Delta x \\Delta p \\geq \\frac{h}{4\\pi} \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Physics\n",
            "- Differential Geometry\n",
            "- Algebraic Geometry\n",
            "Generated Text:\n",
            "Introduction to Galois Theory. Galois Theory is a topic that involves deep mathematical concepts. Applications. Physics. Differential Geometry. - Algebraic Geometry.\n",
            "Target Text:\n",
            "Introduction to Galois Theory. Galois Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Physics. Differential Geometry. Algebraic Geometry.\n",
            "BERTScore F1: 0.9047\n",
            "--------------------------------------------------\n",
            "Sample 30:\n",
            "Input Text:\n",
            "# Introduction to Cauchy-Riemann Equations\n",
            "\n",
            "Cauchy-Riemann Equations is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ L = T - V \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Physics\n",
            "- Differential Geometry\n",
            "- Algebraic Geometry\n",
            "Generated Text:\n",
            "Introduction to Cauchy-Riemann Equations. Cauchy-Riemann Equations is a topic that involves deep mathematical concepts. Applications. Physics. Differential Geometry. - Algebraic Geometry.\n",
            "Target Text:\n",
            "Introduction to Cauchy-Riemann Equations. Cauchy-Riemann Equations is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Physics. Differential Geometry. Algebraic Geometry.\n",
            "BERTScore F1: 0.9234\n",
            "--------------------------------------------------\n",
            "Sample 31:\n",
            "Input Text:\n",
            "# Introduction to Heisenberg Uncertainty Principle\n",
            "\n",
            "Heisenberg Uncertainty Principle is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ L = T - V \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Quantum Mechanics\n",
            "- Particle Physics\n",
            "- Quantum Electrodynamics\n",
            "Generated Text:\n",
            "Introduction to Heisenberg Uncertainty Principle. Heisenberg Uncertainty Principle is a topic that involves deep mathematical concepts. Applications. - Quantum Mechanics. - Particle Physics. Quantum Electrodynamics.\n",
            "Target Text:\n",
            "Introduction to Heisenberg Uncertainty Principle. Heisenberg Uncertainty Principle is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Quantum Mechanics. Particle Physics. Quantum Electrodynamics.\n",
            "BERTScore F1: 0.9133\n",
            "--------------------------------------------------\n",
            "Sample 32:\n",
            "Input Text:\n",
            "# Introduction to Galois Theory\n",
            "\n",
            "Galois Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ G = \\text{exp}(\\mathfrak{g}) \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Signal Processing\n",
            "- Image Processing\n",
            "- Quantum Mechanics\n",
            "Generated Text:\n",
            "Introduction to Galois Theory. Galois Theory is a topic that involves deep mathematical concepts.\n",
            "Target Text:\n",
            "Introduction to Galois Theory. Galois Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Signal Processing. Image Processing. Quantum Mechanics.\n",
            "BERTScore F1: 0.8142\n",
            "--------------------------------------------------\n",
            "Sample 33:\n",
            "Input Text:\n",
            "# Introduction to Spectral Graph Theory\n",
            "\n",
            "Spectral Graph Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ (G, \\cdot) \\quad \\text{such that } a\\cdot(b\\cdot c) = (a\\cdot b)\\cdot c \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Signal Processing\n",
            "- Image Processing\n",
            "- Quantum Mechanics\n",
            "Generated Text:\n",
            "Introduction to Spectral Graph Theory. Spectral Graph Theory is a topic that involves deep mathematical concepts. Applications. - Signal Processing. - Image Processing. - Quantum Mechanics.\n",
            "Target Text:\n",
            "Introduction to Spectral Graph Theory. Spectral Graph Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Signal Processing. Image Processing. Quantum Mechanics.\n",
            "BERTScore F1: 0.8833\n",
            "--------------------------------------------------\n",
            "Sample 34:\n",
            "Input Text:\n",
            "# Introduction to Algebraic Geometry\n",
            "\n",
            "Algebraic Geometry is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ L = T - V \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Cryptography\n",
            "- Polynomial Algebra\n",
            "- Number Theory\n",
            "Generated Text:\n",
            "Introduction to Algebraic Geometry. Algebraic Geometry is a topic that involves deep mathematical concepts. Applications. Cryptography. Polynomial Algebra. - Number Theory.\n",
            "Target Text:\n",
            "Introduction to Algebraic Geometry. Algebraic Geometry is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Cryptography. Polynomial Algebra. Number Theory.\n",
            "BERTScore F1: 0.9050\n",
            "--------------------------------------------------\n",
            "Sample 35:\n",
            "Input Text:\n",
            "# Introduction to Chaos Theory\n",
            "\n",
            "Chaos Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\nabla_v X = v^i \\left( \\frac{\\partial X^j}{\\partial x^i} + \\Gamma^j_{ik} X^k \\right) e_j \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Control Systems\n",
            "- Artificial Intelligence\n",
            "- Decision Making\n",
            "Generated Text:\n",
            "Introduction to Chaos Theory. Chaos Theory is a topic that involves deep mathematical concepts. Applications. - Control Systems. - Artificial Intelligence. - Decision Making.\n",
            "Target Text:\n",
            "Introduction to Chaos Theory. Chaos Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Control Systems. Artificial Intelligence. Decision Making.\n",
            "BERTScore F1: 0.8752\n",
            "--------------------------------------------------\n",
            "Sample 36:\n",
            "Input Text:\n",
            "# Introduction to Game Theory\n",
            "\n",
            "Game Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ L = T - V \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Signal Processing\n",
            "- Image Processing\n",
            "- Quantum Mechanics\n",
            "Generated Text:\n",
            "Introduction to Game Theory. Game Theory is a topic that involves deep mathematical concepts. Applications. - Signal Processing. - Image Processing. - Quantum Mechanics.\n",
            "Target Text:\n",
            "Introduction to Game Theory. Game Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Signal Processing. Image Processing. Quantum Mechanics.\n",
            "BERTScore F1: 0.8774\n",
            "--------------------------------------------------\n",
            "Sample 37:\n",
            "Input Text:\n",
            "# Introduction to Chaos Theory\n",
            "\n",
            "Chaos Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\nabla_v X = v^i \\left( \\frac{\\partial X^j}{\\partial x^i} + \\Gamma^j_{ik} X^k \\right) e_j \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Quantum Mechanics\n",
            "- Particle Physics\n",
            "- Quantum Electrodynamics\n",
            "Generated Text:\n",
            "Introduction to Chaos Theory. Chaos Theory is a topic that involves deep mathematical concepts. Applications. - Quantum Mechanics. - Particle Physics. Quantum Electrodynamics.\n",
            "Target Text:\n",
            "Introduction to Chaos Theory. Chaos Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Quantum Mechanics. Particle Physics. Quantum Electrodynamics.\n",
            "BERTScore F1: 0.8983\n",
            "--------------------------------------------------\n",
            "Sample 38:\n",
            "Input Text:\n",
            "# Introduction to Knot Theory\n",
            "\n",
            "Knot Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ (R, +, \\cdot) \\quad \\text{such that} \\quad a + b = b + a, \\ a\\cdot(b + c) = a\\cdot b + a\\cdot c \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Quantum Mechanics\n",
            "- Particle Physics\n",
            "- Quantum Electrodynamics\n",
            "Generated Text:\n",
            "Introduction to Knot Theory. Knot Theory is a topic that involves deep mathematical concepts. Applications. - Quantum Mechanics. - Particle Physics. Quantum Electrodynamics.\n",
            "Target Text:\n",
            "Introduction to Knot Theory. Knot Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Quantum Mechanics. Particle Physics. Quantum Electrodynamics.\n",
            "BERTScore F1: 0.9003\n",
            "--------------------------------------------------\n",
            "Sample 39:\n",
            "Input Text:\n",
            "# Introduction to Algebraic Geometry\n",
            "\n",
            "Algebraic Geometry is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\nabla_v X = v^i \\left( \\frac{\\partial X^j}{\\partial x^i} + \\Gamma^j_{ik} X^k \\right) e_j \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Control Systems\n",
            "- Artificial Intelligence\n",
            "- Decision Making\n",
            "Generated Text:\n",
            "Introduction to Algebraic Geometry. Algebraic Geometry is a topic that involves deep mathematical concepts. Applications. - Control Systems. - Artificial Intelligence. - Decision Making.\n",
            "Target Text:\n",
            "Introduction to Algebraic Geometry. Algebraic Geometry is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Control Systems. Artificial Intelligence. Decision Making.\n",
            "BERTScore F1: 0.8758\n",
            "--------------------------------------------------\n",
            "Sample 40:\n",
            "Input Text:\n",
            "# Introduction to Euler-Lagrange Equation\n",
            "\n",
            "Euler-Lagrange Equation is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\frac{\\partial u}{\\partial x} = \\frac{\\partial v}{\\partial y}, \\quad \\frac{\\partial u}{\\partial y} = -\\frac{\\partial v}{\\partial x} \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Control Systems\n",
            "- Artificial Intelligence\n",
            "- Decision Making\n",
            "Generated Text:\n",
            "Introduction to Euler-Lagrange Equation. Euler-Lagrange Equation is a topic that involves deep mathematical concepts. Applications. - Control Systems. - Artificial Intelligence. - Decision Making.\n",
            "Target Text:\n",
            "Introduction to Euler-Lagrange Equation. Euler-Lagrange Equation is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Control Systems. Artificial Intelligence. Decision Making.\n",
            "BERTScore F1: 0.9058\n",
            "--------------------------------------------------\n",
            "Sample 41:\n",
            "Input Text:\n",
            "# Introduction to Tensor Calculus\n",
            "\n",
            "Tensor Calculus is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ G = \\text{exp}(\\mathfrak{g}) \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Cryptography\n",
            "- Polynomial Algebra\n",
            "- Number Theory\n",
            "Generated Text:\n",
            "Introduction to Tensor Calculus. Tensor Calculus is a topic that involves deep mathematical concepts. Applications. - Cryptography. - Polynomial Algebra. - Number Theory.\n",
            "Target Text:\n",
            "Introduction to Tensor Calculus. Tensor Calculus is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Cryptography. Polynomial Algebra. Number Theory.\n",
            "BERTScore F1: 0.8790\n",
            "--------------------------------------------------\n",
            "Sample 42:\n",
            "Input Text:\n",
            "# Introduction to Game Theory\n",
            "\n",
            "Game Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\Delta x \\Delta p \\geq \\frac{h}{4\\pi} \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Physics\n",
            "- Differential Geometry\n",
            "- Algebraic Geometry\n",
            "Generated Text:\n",
            "Introduction to Game Theory. Game Theory is a topic that involves deep mathematical concepts. Applications. - Physics. - Differential Geometry. - Algebraic Geometry.\n",
            "Target Text:\n",
            "Introduction to Game Theory. Game Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Physics. Differential Geometry. Algebraic Geometry.\n",
            "BERTScore F1: 0.8696\n",
            "--------------------------------------------------\n",
            "Sample 43:\n",
            "Input Text:\n",
            "# Introduction to Fourier Transform\n",
            "\n",
            "Fourier Transform is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ (G, \\cdot) \\quad \\text{such that } a\\cdot(b\\cdot c) = (a\\cdot b)\\cdot c \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Classical Mechanics\n",
            "- Quantum Mechanics\n",
            "- Control Systems\n",
            "Generated Text:\n",
            "Introduction to Fourier Transform. Fourier Transform is a topic that involves deep mathematical concepts. Applications. Classical Mechanics. - Quantum Mechanics. - Control Systems.\n",
            "Target Text:\n",
            "Introduction to Fourier Transform. Fourier Transform is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Classical Mechanics. Quantum Mechanics. Control Systems.\n",
            "BERTScore F1: 0.8910\n",
            "--------------------------------------------------\n",
            "Sample 44:\n",
            "Input Text:\n",
            "# Introduction to Functional Analysis\n",
            "\n",
            "Functional Analysis is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ V(I) = \\{x \\in \\mathbb{A}^n \\mid f(x) = 0, \\forall f \\in I\\} \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Signal Processing\n",
            "- Image Processing\n",
            "- Quantum Mechanics\n",
            "Generated Text:\n",
            "Introduction to Functional Analysis. Functional Analysis is a topic that involves deep mathematical concepts. Applications. - Signal Processing. - Image Processing. - Quantum Mechanics.\n",
            "Target Text:\n",
            "Introduction to Functional Analysis. Functional Analysis is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Signal Processing. Image Processing. Quantum Mechanics.\n",
            "BERTScore F1: 0.8793\n",
            "--------------------------------------------------\n",
            "Sample 45:\n",
            "Input Text:\n",
            "# Introduction to Combinatorial Optimization\n",
            "\n",
            "Combinatorial Optimization is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ (\\Omega, \\mathcal{F}, P) \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Control Systems\n",
            "- Artificial Intelligence\n",
            "- Decision Making\n",
            "Generated Text:\n",
            "Introduction to Combinatorial Optimization. Combinatorial Optimization is a topic that involves deep mathematical concepts. Applications. - Control Systems. - Artificial Intelligence. - Decision Making.\n",
            "Target Text:\n",
            "Introduction to Combinatorial Optimization. Combinatorial Optimization is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Control Systems. Artificial Intelligence. Decision Making.\n",
            "BERTScore F1: 0.8855\n",
            "--------------------------------------------------\n",
            "Sample 46:\n",
            "Input Text:\n",
            "# Introduction to Covariant Derivatives\n",
            "\n",
            "Covariant Derivatives is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\nabla_v X = v^i \\left( \\frac{\\partial X^j}{\\partial x^i} + \\Gamma^j_{ik} X^k \\right) e_j \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Physics\n",
            "- Differential Geometry\n",
            "- Algebraic Geometry\n",
            "Generated Text:\n",
            "Introduction to Covariant Derivatives. Covariant Derivatives is a topic that involves deep mathematical concepts. Applications. - Physics. - Differential Geometry. - Algebraic Geometry.\n",
            "Target Text:\n",
            "Introduction to Covariant Derivatives. Covariant Derivatives is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Physics. Differential Geometry. Algebraic Geometry.\n",
            "BERTScore F1: 0.8876\n",
            "--------------------------------------------------\n",
            "Sample 47:\n",
            "Input Text:\n",
            "# Introduction to Optimization Theory\n",
            "\n",
            "Optimization Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ T = T^i_j \\partial_i dx^j \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Quantum Mechanics\n",
            "- Particle Physics\n",
            "- Quantum Electrodynamics\n",
            "Generated Text:\n",
            "Introduction to Optimization Theory. Optimization Theory is a topic that involves deep mathematical concepts. Applications. - Quantum Mechanics. - Particle Physics. Quantum Electrodynamics.\n",
            "Target Text:\n",
            "Introduction to Optimization Theory. Optimization Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Quantum Mechanics. Particle Physics. Quantum Electrodynamics.\n",
            "BERTScore F1: 0.8992\n",
            "--------------------------------------------------\n",
            "Sample 48:\n",
            "Input Text:\n",
            "# Introduction to Variational Calculus\n",
            "\n",
            "Variational Calculus is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ ||T|| = \\sup_{||x||=1} ||T(x)|| \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Signal Processing\n",
            "- Image Processing\n",
            "- Quantum Mechanics\n",
            "Generated Text:\n",
            "Introduction to Variational Calculus. Variational Calculus is a topic that involves deep mathematical concepts. Applications. - Signal Processing. - Image Processing. - Quantum Mechanics.\n",
            "Target Text:\n",
            "Introduction to Variational Calculus. Variational Calculus is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Signal Processing. Image Processing. Quantum Mechanics.\n",
            "BERTScore F1: 0.8846\n",
            "--------------------------------------------------\n",
            "Sample 49:\n",
            "Input Text:\n",
            "# Introduction to Covariant Derivatives\n",
            "\n",
            "Covariant Derivatives is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\mu_A(x) \\in [0,1] \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Signal Processing\n",
            "- Image Processing\n",
            "- Quantum Mechanics\n",
            "Generated Text:\n",
            "Introduction to Covariant Derivatives. Covariant Derivatives is a topic that involves deep mathematical concepts. Applications. - Signal Processing. - Image Processing. - Quantum Mechanics.\n",
            "Target Text:\n",
            "Introduction to Covariant Derivatives. Covariant Derivatives is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Signal Processing. Image Processing. Quantum Mechanics.\n",
            "BERTScore F1: 0.8941\n",
            "--------------------------------------------------\n",
            "Sample 50:\n",
            "Input Text:\n",
            "# Introduction to Rings in Abstract Algebra\n",
            "\n",
            "Rings in Abstract Algebra is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\hat{\\phi}(x,t) = \\int \\frac{d^3k}{(2\\pi)^3} \\frac{1}{\\sqrt{2E_k}} \\left( a_k e^{-ikx} + a_k^\\dagger e^{ikx} \\right) \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Signal Processing\n",
            "- Image Processing\n",
            "- Quantum Mechanics\n",
            "Generated Text:\n",
            "Introduction to Rings in Abstract Algebra. Rings in Abstract Algebra is a topic that involves deep mathematical concepts. Applications. - Signal Processing. - Image Processing. Quantum Mechanics.\n",
            "Target Text:\n",
            "Introduction to Rings in Abstract Algebra. Rings in Abstract Algebra is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Signal Processing. Image Processing. Quantum Mechanics.\n",
            "BERTScore F1: 0.9017\n",
            "--------------------------------------------------\n",
            "Sample 51:\n",
            "Input Text:\n",
            "# Introduction to Fuzzy Logic\n",
            "\n",
            "Fuzzy Logic is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\text{Gal}(E/F) \\quad \\text{where } E \\text{ is an extension field of } F \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Machine Learning\n",
            "- Operations Research\n",
            "- Economics\n",
            "Generated Text:\n",
            "Introduction to Fuzzy Logic. Fuzzy Logic is a topic that involves deep mathematical concepts. [ textGal(E/F) quad textwhere  E text is an extension field of  F. ] ## Applications - Machine Learning. Operations Research. Economics.\n",
            "Target Text:\n",
            "Introduction to Fuzzy Logic. Fuzzy Logic is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Machine Learning. Operations Research. Economics.\n",
            "BERTScore F1: 0.7640\n",
            "--------------------------------------------------\n",
            "Sample 52:\n",
            "Input Text:\n",
            "# Introduction to Covariant Derivatives\n",
            "\n",
            "Covariant Derivatives is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\mu(\\cup A_i) = \\sum \\mu(A_i) \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Classical Mechanics\n",
            "- Quantum Mechanics\n",
            "- Control Systems\n",
            "Generated Text:\n",
            "Introduction to Covariant Derivatives. Covariant Derivatives is a topic that involves deep mathematical concepts. Applications. Classical Mechanics. - Quantum Mechanics. - Control Systems.\n",
            "Target Text:\n",
            "Introduction to Covariant Derivatives. Covariant Derivatives is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Classical Mechanics. Quantum Mechanics. Control Systems.\n",
            "BERTScore F1: 0.9039\n",
            "--------------------------------------------------\n",
            "Sample 53:\n",
            "Input Text:\n",
            "# Introduction to Green's Theorem\n",
            "\n",
            "Green's Theorem is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\text{Gal}(E/F) \\quad \\text{where } E \\text{ is an extension field of } F \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Control Systems\n",
            "- Artificial Intelligence\n",
            "- Decision Making\n",
            "Generated Text:\n",
            "Introduction to Green's Theorem. Green's Theorem is a topic that involves deep mathematical concepts. [ textGal(E/F) quad textwhere  E text is an extension field of  F. ] ## Applications - Control Systems. - Artificial Intelligence. - Decision Making.\n",
            "Target Text:\n",
            "Introduction to Green's Theorem. Green's Theorem is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Control Systems. Artificial Intelligence. Decision Making.\n",
            "BERTScore F1: 0.7706\n",
            "--------------------------------------------------\n",
            "Sample 54:\n",
            "Input Text:\n",
            "# Introduction to Rings in Abstract Algebra\n",
            "\n",
            "Rings in Abstract Algebra is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\min_{x \\in X} f(x) \\quad \\text{subject to constraints} \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Control Systems\n",
            "- Artificial Intelligence\n",
            "- Decision Making\n",
            "Generated Text:\n",
            "Introduction to Rings in Abstract Algebra. Rings in Abstract Algebra is a topic that involves deep mathematical concepts. Applications. Control Systems. - Artificial Intelligence. - Decision Making.\n",
            "Target Text:\n",
            "Introduction to Rings in Abstract Algebra. Rings in Abstract Algebra is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Control Systems. Artificial Intelligence. Decision Making.\n",
            "BERTScore F1: 0.9000\n",
            "--------------------------------------------------\n",
            "Sample 55:\n",
            "Input Text:\n",
            "# Introduction to Heisenberg Uncertainty Principle\n",
            "\n",
            "Heisenberg Uncertainty Principle is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\lambda(G) = \\text{eigenvalue of the adjacency matrix} \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Cryptography\n",
            "- Polynomial Algebra\n",
            "- Number Theory\n",
            "Generated Text:\n",
            "Introduction to Heisenberg Uncertainty Principle. Heisenberg Uncertainty Principle is a topic that involves deep mathematical concepts. Applications. - Cryptography. - Polynomial Algebra. - Number Theory.\n",
            "Target Text:\n",
            "Introduction to Heisenberg Uncertainty Principle. Heisenberg Uncertainty Principle is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Cryptography. Polynomial Algebra. Number Theory.\n",
            "BERTScore F1: 0.8945\n",
            "--------------------------------------------------\n",
            "Sample 56:\n",
            "Input Text:\n",
            "# Introduction to Variational Calculus\n",
            "\n",
            "Variational Calculus is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\min_{x \\in X} f(x) \\quad \\text{subject to constraints} \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Classical Mechanics\n",
            "- Quantum Mechanics\n",
            "- Control Systems\n",
            "Generated Text:\n",
            "Introduction to Variational Calculus. Variational Calculus is a topic that involves deep mathematical concepts. Applications. Classical Mechanics. - Quantum Mechanics. - Control Systems.\n",
            "Target Text:\n",
            "Introduction to Variational Calculus. Variational Calculus is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Classical Mechanics. Quantum Mechanics. Control Systems.\n",
            "BERTScore F1: 0.8953\n",
            "--------------------------------------------------\n",
            "Sample 57:\n",
            "Input Text:\n",
            "# Introduction to Game Theory\n",
            "\n",
            "Game Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ (R, +, \\cdot) \\quad \\text{such that} \\quad a + b = b + a, \\ a\\cdot(b + c) = a\\cdot b + a\\cdot c \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Physics\n",
            "- Differential Geometry\n",
            "- Algebraic Geometry\n",
            "Generated Text:\n",
            "Introduction to Game Theory. Game Theory is a topic that involves deep mathematical concepts. Applications. - Physics. Differential Geometry. - Algebraic Geometry.\n",
            "Target Text:\n",
            "Introduction to Game Theory. Game Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Physics. Differential Geometry. Algebraic Geometry.\n",
            "BERTScore F1: 0.8840\n",
            "--------------------------------------------------\n",
            "Sample 58:\n",
            "Input Text:\n",
            "# Introduction to Galois Theory\n",
            "\n",
            "Galois Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ (G, \\cdot) \\quad \\text{such that } a\\cdot(b\\cdot c) = (a\\cdot b)\\cdot c \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Control Systems\n",
            "- Artificial Intelligence\n",
            "- Decision Making\n",
            "Generated Text:\n",
            "Introduction to Galois Theory. Galois Theory is a topic that involves deep mathematical concepts. Applications. - Control Systems. - Artificial Intelligence. - Decision Making.\n",
            "Target Text:\n",
            "Introduction to Galois Theory. Galois Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Control Systems. Artificial Intelligence. Decision Making.\n",
            "BERTScore F1: 0.8822\n",
            "--------------------------------------------------\n",
            "Sample 59:\n",
            "Input Text:\n",
            "# Introduction to Covariant Derivatives\n",
            "\n",
            "Covariant Derivatives is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ (G, \\cdot) \\quad \\text{such that } a\\cdot(b\\cdot c) = (a\\cdot b)\\cdot c \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Classical Mechanics\n",
            "- Quantum Mechanics\n",
            "- Control Systems\n",
            "Generated Text:\n",
            "Introduction to Covariant Derivatives. Covariant Derivatives is a topic that involves deep mathematical concepts. Applications. - Classical Mechanics. - Quantum Mechanics. - Control Systems.\n",
            "Target Text:\n",
            "Introduction to Covariant Derivatives. Covariant Derivatives is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Classical Mechanics. Quantum Mechanics. Control Systems.\n",
            "BERTScore F1: 0.8931\n",
            "--------------------------------------------------\n",
            "Sample 60:\n",
            "Input Text:\n",
            "# Introduction to Galois Theory\n",
            "\n",
            "Galois Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ (R, +, \\cdot) \\quad \\text{such that} \\quad a + b = b + a, \\ a\\cdot(b + c) = a\\cdot b + a\\cdot c \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Classical Mechanics\n",
            "- Quantum Mechanics\n",
            "- Control Systems\n",
            "Generated Text:\n",
            "Introduction to Galois Theory. Galois Theory is a topic that involves deep mathematical concepts. Applications. - Classical Mechanics. - Quantum Mechanics. - Control Systems.\n",
            "Target Text:\n",
            "Introduction to Galois Theory. Galois Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Classical Mechanics. Quantum Mechanics. Control Systems.\n",
            "BERTScore F1: 0.8823\n",
            "--------------------------------------------------\n",
            "Sample 61:\n",
            "Input Text:\n",
            "# Introduction to Green's Theorem\n",
            "\n",
            "Green's Theorem is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\omega = f(x,y) dx + g(x,y) dy \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Signal Processing\n",
            "- Image Processing\n",
            "- Quantum Mechanics\n",
            "Generated Text:\n",
            "Introduction to Green's Theorem. Green's Theorem is a topic that involves deep mathematical concepts. Applications. - Signal Processing. - Image Processing. - Quantum Mechanics.\n",
            "Target Text:\n",
            "Introduction to Green's Theorem. Green's Theorem is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Signal Processing. Image Processing. Quantum Mechanics.\n",
            "BERTScore F1: 0.8903\n",
            "--------------------------------------------------\n",
            "Sample 62:\n",
            "Input Text:\n",
            "# Introduction to Tensor Calculus\n",
            "\n",
            "Tensor Calculus is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ T = T^i_j \\partial_i dx^j \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Classical Mechanics\n",
            "- Quantum Mechanics\n",
            "- Control Systems\n",
            "Generated Text:\n",
            "Introduction to Tensor Calculus. Tensor Calculus is a topic that involves deep mathematical concepts. Applications. Classical Mechanics. - Quantum Mechanics. - Control Systems.\n",
            "Target Text:\n",
            "Introduction to Tensor Calculus. Tensor Calculus is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Classical Mechanics. Quantum Mechanics. Control Systems.\n",
            "BERTScore F1: 0.8922\n",
            "--------------------------------------------------\n",
            "Sample 63:\n",
            "Input Text:\n",
            "# Introduction to Measure Theory\n",
            "\n",
            "Measure Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ V(I) = \\{x \\in \\mathbb{A}^n \\mid f(x) = 0, \\forall f \\in I\\} \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Quantum Mechanics\n",
            "- Particle Physics\n",
            "- Quantum Electrodynamics\n",
            "Generated Text:\n",
            "Introduction to Measure Theory. Measure Theory is a topic that involves deep mathematical concepts. Applications. - Quantum Mechanics. - Particle Physics. Quantum Electrodynamics.\n",
            "Target Text:\n",
            "Introduction to Measure Theory. Measure Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Quantum Mechanics. Particle Physics. Quantum Electrodynamics.\n",
            "BERTScore F1: 0.9005\n",
            "--------------------------------------------------\n",
            "Sample 64:\n",
            "Input Text:\n",
            "# Introduction to Knot Theory\n",
            "\n",
            "Knot Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ (G, \\cdot) \\quad \\text{such that } a\\cdot(b\\cdot c) = (a\\cdot b)\\cdot c \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Quantum Mechanics\n",
            "- Particle Physics\n",
            "- Quantum Electrodynamics\n",
            "Generated Text:\n",
            "Introduction to Knot Theory. Knot Theory is a topic that involves deep mathematical concepts. Applications. - Quantum Mechanics. - Particle Physics. Quantum Electrodynamics.\n",
            "Target Text:\n",
            "Introduction to Knot Theory. Knot Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Quantum Mechanics. Particle Physics. Quantum Electrodynamics.\n",
            "BERTScore F1: 0.9003\n",
            "--------------------------------------------------\n",
            "Sample 65:\n",
            "Input Text:\n",
            "# Introduction to Measure Theory\n",
            "\n",
            "Measure Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\hat{\\phi}(x,t) = \\int \\frac{d^3k}{(2\\pi)^3} \\frac{1}{\\sqrt{2E_k}} \\left( a_k e^{-ikx} + a_k^\\dagger e^{ikx} \\right) \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Quantum Mechanics\n",
            "- Particle Physics\n",
            "- Quantum Electrodynamics\n",
            "Generated Text:\n",
            "Introduction to Measure Theory. Measure Theory is a topic that involves deep mathematical concepts. Applications. - Quantum Mechanics. - Particle Physics. Quantum Electrodynamics.\n",
            "Target Text:\n",
            "Introduction to Measure Theory. Measure Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Quantum Mechanics. Particle Physics. Quantum Electrodynamics.\n",
            "BERTScore F1: 0.9005\n",
            "--------------------------------------------------\n",
            "Sample 66:\n",
            "Input Text:\n",
            "# Introduction to Nonlinear Dynamics\n",
            "\n",
            "Nonlinear Dynamics is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ V(I) = \\{x \\in \\mathbb{A}^n \\mid f(x) = 0, \\forall f \\in I\\} \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Classical Mechanics\n",
            "- Quantum Mechanics\n",
            "- Control Systems\n",
            "Generated Text:\n",
            "Introduction to Nonlinear Dynamics. Nonlinear Dynamics is a topic that involves deep mathematical concepts. Applications. Classical Mechanics. - Quantum Mechanics. - Control Systems.\n",
            "Target Text:\n",
            "Introduction to Nonlinear Dynamics. Nonlinear Dynamics is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Classical Mechanics. Quantum Mechanics. Control Systems.\n",
            "BERTScore F1: 0.8866\n",
            "--------------------------------------------------\n",
            "Sample 67:\n",
            "Input Text:\n",
            "# Introduction to Differential Forms\n",
            "\n",
            "Differential Forms is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\mu(\\cup A_i) = \\sum \\mu(A_i) \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Cryptography\n",
            "- Polynomial Algebra\n",
            "- Number Theory\n",
            "Generated Text:\n",
            "Introduction to Differential Forms. Differential Forms is a topic that involves deep mathematical concepts. Applications. - Cryptography. - Polynomial Algebra. - Number Theory.\n",
            "Target Text:\n",
            "Introduction to Differential Forms. Differential Forms is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Cryptography. Polynomial Algebra. Number Theory.\n",
            "BERTScore F1: 0.8778\n",
            "--------------------------------------------------\n",
            "Sample 68:\n",
            "Input Text:\n",
            "# Introduction to Green's Theorem\n",
            "\n",
            "Green's Theorem is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\omega = f(x,y) dx + g(x,y) dy \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Classical Mechanics\n",
            "- Quantum Mechanics\n",
            "- Control Systems\n",
            "Generated Text:\n",
            "Introduction to Green's Theorem. Green's Theorem is a topic that involves deep mathematical concepts. Applications. Classical Mechanics. - Quantum Mechanics. - Control Systems.\n",
            "Target Text:\n",
            "Introduction to Green's Theorem. Green's Theorem is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Classical Mechanics. Quantum Mechanics. Control Systems.\n",
            "BERTScore F1: 0.9002\n",
            "--------------------------------------------------\n",
            "Sample 69:\n",
            "Input Text:\n",
            "# Introduction to Rings in Abstract Algebra\n",
            "\n",
            "Rings in Abstract Algebra is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\mu_A(x) \\in [0,1] \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Machine Learning\n",
            "- Operations Research\n",
            "- Economics\n",
            "Generated Text:\n",
            "Introduction to Rings in Abstract Algebra. Rings in Abstract Algebra is a topic that involves deep mathematical concepts. Applications. Machine Learning. Operations Research. - Economics.\n",
            "Target Text:\n",
            "Introduction to Rings in Abstract Algebra. Rings in Abstract Algebra is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Machine Learning. Operations Research. Economics.\n",
            "BERTScore F1: 0.9042\n",
            "--------------------------------------------------\n",
            "Sample 70:\n",
            "Input Text:\n",
            "# Introduction to Nonlinear Dynamics\n",
            "\n",
            "Nonlinear Dynamics is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ V(I) = \\{x \\in \\mathbb{A}^n \\mid f(x) = 0, \\forall f \\in I\\} \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Classical Mechanics\n",
            "- Quantum Mechanics\n",
            "- Control Systems\n",
            "Generated Text:\n",
            "Introduction to Nonlinear Dynamics. Nonlinear Dynamics is a topic that involves deep mathematical concepts. Applications. Classical Mechanics. - Quantum Mechanics. - Control Systems.\n",
            "Target Text:\n",
            "Introduction to Nonlinear Dynamics. Nonlinear Dynamics is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Classical Mechanics. Quantum Mechanics. Control Systems.\n",
            "BERTScore F1: 0.8866\n",
            "--------------------------------------------------\n",
            "Sample 71:\n",
            "Input Text:\n",
            "# Introduction to Variational Calculus\n",
            "\n",
            "Variational Calculus is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\Delta x \\Delta p \\geq \\frac{h}{4\\pi} \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Machine Learning\n",
            "- Operations Research\n",
            "- Economics\n",
            "Generated Text:\n",
            "Introduction to Variational Calculus. Variational Calculus is a topic that involves deep mathematical concepts. Applications. Machine Learning. Operations Research. Economics.\n",
            "Target Text:\n",
            "Introduction to Variational Calculus. Variational Calculus is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Machine Learning. Operations Research. Economics.\n",
            "BERTScore F1: 0.9261\n",
            "--------------------------------------------------\n",
            "Sample 72:\n",
            "Input Text:\n",
            "# Introduction to Topology\n",
            "\n",
            "Topology is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\frac{\\partial u}{\\partial x} = \\frac{\\partial v}{\\partial y}, \\quad \\frac{\\partial u}{\\partial y} = -\\frac{\\partial v}{\\partial x} \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Classical Mechanics\n",
            "- Quantum Mechanics\n",
            "- Control Systems\n",
            "Generated Text:\n",
            "Introduction to Topology. Topology is a topic that involves deep mathematical concepts. Applications. - Classical Mechanics. - Quantum Mechanics. - Control Systems.\n",
            "Target Text:\n",
            "Introduction to Topology. Topology is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Classical Mechanics. Quantum Mechanics. Control Systems.\n",
            "BERTScore F1: 0.8668\n",
            "--------------------------------------------------\n",
            "Sample 73:\n",
            "Input Text:\n",
            "# Introduction to Quantum Field Theory\n",
            "\n",
            "Quantum Field Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ T = T^i_j \\partial_i dx^j \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Cryptography\n",
            "- Polynomial Algebra\n",
            "- Number Theory\n",
            "Generated Text:\n",
            "Introduction to Quantum Field Theory. Quantum Field Theory is a topic that involves deep mathematical concepts. Applications. - Cryptography. - Polynomial Algebra. - Number Theory.\n",
            "Target Text:\n",
            "Introduction to Quantum Field Theory. Quantum Field Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Cryptography. Polynomial Algebra. Number Theory.\n",
            "BERTScore F1: 0.8829\n",
            "--------------------------------------------------\n",
            "Sample 74:\n",
            "Input Text:\n",
            "# Introduction to Quantum Field Theory\n",
            "\n",
            "Quantum Field Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ (G, \\cdot) \\quad \\text{such that } a\\cdot(b\\cdot c) = (a\\cdot b)\\cdot c \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Physics\n",
            "- Differential Geometry\n",
            "- Algebraic Geometry\n",
            "Generated Text:\n",
            "Introduction to Quantum Field Theory. Quantum Field Theory is a topic that involves deep mathematical concepts. Applications. - Physics. - Differential Geometry. - Algebraic Geometry.\n",
            "Target Text:\n",
            "Introduction to Quantum Field Theory. Quantum Field Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Physics. Differential Geometry. Algebraic Geometry.\n",
            "BERTScore F1: 0.8774\n",
            "--------------------------------------------------\n",
            "Sample 75:\n",
            "Input Text:\n",
            "# Introduction to Fourier Transform\n",
            "\n",
            "Fourier Transform is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\frac{\\partial u}{\\partial x} = \\frac{\\partial v}{\\partial y}, \\quad \\frac{\\partial u}{\\partial y} = -\\frac{\\partial v}{\\partial x} \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Quantum Mechanics\n",
            "- Particle Physics\n",
            "- Quantum Electrodynamics\n",
            "Generated Text:\n",
            "Introduction to Fourier Transform. Fourier Transform is a topic that involves deep mathematical concepts. Applications. - Quantum Mechanics. - Particle Physics. Quantum Electrodynamics.\n",
            "Target Text:\n",
            "Introduction to Fourier Transform. Fourier Transform is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Quantum Mechanics. Particle Physics. Quantum Electrodynamics.\n",
            "BERTScore F1: 0.9020\n",
            "--------------------------------------------------\n",
            "Sample 76:\n",
            "Input Text:\n",
            "# Introduction to Topology\n",
            "\n",
            "Topology is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\frac{\\partial u}{\\partial x} = \\frac{\\partial v}{\\partial y}, \\quad \\frac{\\partial u}{\\partial y} = -\\frac{\\partial v}{\\partial x} \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Signal Processing\n",
            "- Image Processing\n",
            "- Quantum Mechanics\n",
            "Generated Text:\n",
            "Introduction to Topology. Topology is a topic that involves deep mathematical concepts. Applications. - Signal Processing. - Image Processing. Quantum Mechanics.\n",
            "Target Text:\n",
            "Introduction to Topology. Topology is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Signal Processing. Image Processing. Quantum Mechanics.\n",
            "BERTScore F1: 0.8845\n",
            "--------------------------------------------------\n",
            "Sample 77:\n",
            "Input Text:\n",
            "# Introduction to Game Theory\n",
            "\n",
            "Game Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\min_{x \\in X} f(x) \\quad \\text{subject to constraints} \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Signal Processing\n",
            "- Image Processing\n",
            "- Quantum Mechanics\n",
            "Generated Text:\n",
            "Introduction to Game Theory. Game Theory is a topic that involves deep mathematical concepts. Applications. - Signal Processing. - Image Processing. - Quantum Mechanics.\n",
            "Target Text:\n",
            "Introduction to Game Theory. Game Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Signal Processing. Image Processing. Quantum Mechanics.\n",
            "BERTScore F1: 0.8774\n",
            "--------------------------------------------------\n",
            "Sample 78:\n",
            "Input Text:\n",
            "# Introduction to Optimization Theory\n",
            "\n",
            "Optimization Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\nabla_v X = v^i \\left( \\frac{\\partial X^j}{\\partial x^i} + \\Gamma^j_{ik} X^k \\right) e_j \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Physics\n",
            "- Differential Geometry\n",
            "- Algebraic Geometry\n",
            "Generated Text:\n",
            "Introduction to Optimization Theory. Optimization Theory is a topic that involves deep mathematical concepts. Applications. - Physics. - Differential Geometry. - Algebraic Geometry.\n",
            "Target Text:\n",
            "Introduction to Optimization Theory. Optimization Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Physics. Differential Geometry. Algebraic Geometry.\n",
            "BERTScore F1: 0.8702\n",
            "--------------------------------------------------\n",
            "Sample 79:\n",
            "Input Text:\n",
            "# Introduction to Functional Analysis\n",
            "\n",
            "Functional Analysis is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ (\\Omega, \\mathcal{F}, P) \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Signal Processing\n",
            "- Image Processing\n",
            "- Quantum Mechanics\n",
            "Generated Text:\n",
            "Introduction to Functional Analysis. Functional Analysis is a topic that involves deep mathematical concepts. Applications. - Signal Processing. - Image Processing. - Quantum Mechanics.\n",
            "Target Text:\n",
            "Introduction to Functional Analysis. Functional Analysis is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Signal Processing. Image Processing. Quantum Mechanics.\n",
            "BERTScore F1: 0.8793\n",
            "--------------------------------------------------\n",
            "Sample 80:\n",
            "Input Text:\n",
            "# Introduction to Topology\n",
            "\n",
            "Topology is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ G = \\text{exp}(\\mathfrak{g}) \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Cryptography\n",
            "- Polynomial Algebra\n",
            "- Number Theory\n",
            "Generated Text:\n",
            "Introduction to Topology. Topology is a topic that involves deep mathematical concepts. Applications. - Cryptography. - Polynomial Algebra. - Number Theory.\n",
            "Target Text:\n",
            "Introduction to Topology. Topology is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Cryptography. Polynomial Algebra. Number Theory.\n",
            "BERTScore F1: 0.8694\n",
            "--------------------------------------------------\n",
            "Sample 81:\n",
            "Input Text:\n",
            "# Introduction to Euler-Lagrange Equation\n",
            "\n",
            "Euler-Lagrange Equation is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ (R, +, \\cdot) \\quad \\text{such that} \\quad a + b = b + a, \\ a\\cdot(b + c) = a\\cdot b + a\\cdot c \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Machine Learning\n",
            "- Operations Research\n",
            "- Economics\n",
            "Generated Text:\n",
            "Introduction to Euler-Lagrange Equation. Euler-Lagrange Equation is a topic that involves deep mathematical concepts. Applications. Machine Learning. Operations Research. Economics.\n",
            "Target Text:\n",
            "Introduction to Euler-Lagrange Equation. Euler-Lagrange Equation is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Machine Learning. Operations Research. Economics.\n",
            "BERTScore F1: 0.9407\n",
            "--------------------------------------------------\n",
            "Sample 82:\n",
            "Input Text:\n",
            "# Introduction to Cauchy-Riemann Equations\n",
            "\n",
            "Cauchy-Riemann Equations is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\nabla_v X = v^i \\left( \\frac{\\partial X^j}{\\partial x^i} + \\Gamma^j_{ik} X^k \\right) e_j \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Physics\n",
            "- Differential Geometry\n",
            "- Algebraic Geometry\n",
            "Generated Text:\n",
            "Introduction to Cauchy-Riemann Equations. Cauchy-Riemann Equations is a topic that involves deep mathematical concepts. Applications. - Physics. Differential Geometry. - Algebraic Geometry.\n",
            "Target Text:\n",
            "Introduction to Cauchy-Riemann Equations. Cauchy-Riemann Equations is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Physics. Differential Geometry. Algebraic Geometry.\n",
            "BERTScore F1: 0.9102\n",
            "--------------------------------------------------\n",
            "Sample 83:\n",
            "Input Text:\n",
            "# Introduction to Cauchy-Riemann Equations\n",
            "\n",
            "Cauchy-Riemann Equations is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ ||T|| = \\sup_{||x||=1} ||T(x)|| \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Classical Mechanics\n",
            "- Quantum Mechanics\n",
            "- Control Systems\n",
            "Generated Text:\n",
            "Introduction to Cauchy-Riemann Equations. Cauchy-Riemann Equations is a topic that involves deep mathematical concepts. Applications. Classical Mechanics. - Quantum Mechanics. - Control Systems.\n",
            "Target Text:\n",
            "Introduction to Cauchy-Riemann Equations. Cauchy-Riemann Equations is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Classical Mechanics. Quantum Mechanics. Control Systems.\n",
            "BERTScore F1: 0.9128\n",
            "--------------------------------------------------\n",
            "Sample 84:\n",
            "Input Text:\n",
            "# Introduction to Measure Theory\n",
            "\n",
            "Measure Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ ||T|| = \\sup_{||x||=1} ||T(x)|| \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Control Systems\n",
            "- Artificial Intelligence\n",
            "- Decision Making\n",
            "Generated Text:\n",
            "Introduction to Measure Theory. Measure Theory is a topic that involves deep mathematical concepts. Applications. - Control Systems. - Artificial Intelligence. - Decision Making.\n",
            "Target Text:\n",
            "Introduction to Measure Theory. Measure Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Control Systems. Artificial Intelligence. Decision Making.\n",
            "BERTScore F1: 0.8773\n",
            "--------------------------------------------------\n",
            "Sample 85:\n",
            "Input Text:\n",
            "# Introduction to Measure Theory\n",
            "\n",
            "Measure Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\frac{\\partial u}{\\partial x} = \\frac{\\partial v}{\\partial y}, \\quad \\frac{\\partial u}{\\partial y} = -\\frac{\\partial v}{\\partial x} \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Machine Learning\n",
            "- Operations Research\n",
            "- Economics\n",
            "Generated Text:\n",
            "Introduction to Measure Theory. Measure Theory is a topic that involves deep mathematical concepts. Applications. Machine Learning. Operations Research. Economics.\n",
            "Target Text:\n",
            "Introduction to Measure Theory. Measure Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Machine Learning. Operations Research. Economics.\n",
            "BERTScore F1: 0.9196\n",
            "--------------------------------------------------\n",
            "Sample 86:\n",
            "Input Text:\n",
            "# Introduction to Variational Calculus\n",
            "\n",
            "Variational Calculus is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ V(I) = \\{x \\in \\mathbb{A}^n \\mid f(x) = 0, \\forall f \\in I\\} \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Control Systems\n",
            "- Artificial Intelligence\n",
            "- Decision Making\n",
            "Generated Text:\n",
            "Introduction to Variational Calculus. Variational Calculus is a topic that involves deep mathematical concepts. Applications. - Control Systems. - Artificial Intelligence. - Decision Making.\n",
            "Target Text:\n",
            "Introduction to Variational Calculus. Variational Calculus is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Control Systems. Artificial Intelligence. Decision Making.\n",
            "BERTScore F1: 0.8820\n",
            "--------------------------------------------------\n",
            "Sample 87:\n",
            "Input Text:\n",
            "# Introduction to Knot Theory\n",
            "\n",
            "Knot Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\mu_A(x) \\in [0,1] \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Physics\n",
            "- Differential Geometry\n",
            "- Algebraic Geometry\n",
            "Generated Text:\n",
            "Introduction to Knot Theory. Knot Theory is a topic that involves deep mathematical concepts. Applications. - Physics. - Differential Geometry. - Algebraic Geometry.\n",
            "Target Text:\n",
            "Introduction to Knot Theory. Knot Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Physics. Differential Geometry. Algebraic Geometry.\n",
            "BERTScore F1: 0.8712\n",
            "--------------------------------------------------\n",
            "Sample 88:\n",
            "Input Text:\n",
            "# Introduction to Tensor Calculus\n",
            "\n",
            "Tensor Calculus is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\mu_A(x) \\in [0,1] \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Classical Mechanics\n",
            "- Quantum Mechanics\n",
            "- Control Systems\n",
            "Generated Text:\n",
            "Introduction to Tensor Calculus. Tensor Calculus is a topic that involves deep mathematical concepts. Applications. Classical Mechanics. - Quantum Mechanics. - Control Systems.\n",
            "Target Text:\n",
            "Introduction to Tensor Calculus. Tensor Calculus is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Classical Mechanics. Quantum Mechanics. Control Systems.\n",
            "BERTScore F1: 0.8922\n",
            "--------------------------------------------------\n",
            "Sample 89:\n",
            "Input Text:\n",
            "# Introduction to Lie Groups\n",
            "\n",
            "Lie Groups is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\frac{\\partial u}{\\partial x} = \\frac{\\partial v}{\\partial y}, \\quad \\frac{\\partial u}{\\partial y} = -\\frac{\\partial v}{\\partial x} \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Machine Learning\n",
            "- Operations Research\n",
            "- Economics\n",
            "Generated Text:\n",
            "Introduction to Lie Groups. Lie Groups is a topic that involves deep mathematical concepts. Applications. Machine Learning. Operations Research. Economics.\n",
            "Target Text:\n",
            "Introduction to Lie Groups. Lie Groups is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Machine Learning. Operations Research. Economics.\n",
            "BERTScore F1: 0.9183\n",
            "--------------------------------------------------\n",
            "Sample 90:\n",
            "Input Text:\n",
            "# Introduction to Covariant Derivatives\n",
            "\n",
            "Covariant Derivatives is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\min_{x \\in X} f(x) \\quad \\text{subject to constraints} \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Cryptography\n",
            "- Polynomial Algebra\n",
            "- Number Theory\n",
            "Generated Text:\n",
            "Introduction to Covariant Derivatives. Covariant Derivatives is a topic that involves deep mathematical concepts. Applications. - Cryptography. - Polynomial Algebra. - Number Theory.\n",
            "Target Text:\n",
            "Introduction to Covariant Derivatives. Covariant Derivatives is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Cryptography. Polynomial Algebra. Number Theory.\n",
            "BERTScore F1: 0.8940\n",
            "--------------------------------------------------\n",
            "Sample 91:\n",
            "Input Text:\n",
            "# Introduction to Topology\n",
            "\n",
            "Topology is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ (\\Omega, \\mathcal{F}, P) \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Control Systems\n",
            "- Artificial Intelligence\n",
            "- Decision Making\n",
            "Generated Text:\n",
            "Introduction to Topology. Topology is a topic that involves deep mathematical concepts. Applications. - Control Systems. - Artificial Intelligence. - Decision Making.\n",
            "Target Text:\n",
            "Introduction to Topology. Topology is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Control Systems. Artificial Intelligence. Decision Making.\n",
            "BERTScore F1: 0.8658\n",
            "--------------------------------------------------\n",
            "Sample 92:\n",
            "Input Text:\n",
            "# Introduction to Quantum Field Theory\n",
            "\n",
            "Quantum Field Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\mu_A(x) \\in [0,1] \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Quantum Mechanics\n",
            "- Particle Physics\n",
            "- Quantum Electrodynamics\n",
            "Generated Text:\n",
            "Introduction to Quantum Field Theory. Quantum Field Theory is a topic that involves deep mathematical concepts. Applications. - Quantum Mechanics. - Particle Physics. Quantum Electrodynamics.\n",
            "Target Text:\n",
            "Introduction to Quantum Field Theory. Quantum Field Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Quantum Mechanics. Particle Physics. Quantum Electrodynamics.\n",
            "BERTScore F1: 0.9030\n",
            "--------------------------------------------------\n",
            "Sample 93:\n",
            "Input Text:\n",
            "# Introduction to Spectral Graph Theory\n",
            "\n",
            "Spectral Graph Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ (G, \\cdot) \\quad \\text{such that } a\\cdot(b\\cdot c) = (a\\cdot b)\\cdot c \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Control Systems\n",
            "- Artificial Intelligence\n",
            "- Decision Making\n",
            "Generated Text:\n",
            "Introduction to Spectral Graph Theory. Spectral Graph Theory is a topic that involves deep mathematical concepts. Applications. - Control Systems. - Artificial Intelligence. - Decision Making.\n",
            "Target Text:\n",
            "Introduction to Spectral Graph Theory. Spectral Graph Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Control Systems. Artificial Intelligence. Decision Making.\n",
            "BERTScore F1: 0.8818\n",
            "--------------------------------------------------\n",
            "Sample 94:\n",
            "Input Text:\n",
            "# Introduction to Topology\n",
            "\n",
            "Topology is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\mu(\\cup A_i) = \\sum \\mu(A_i) \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Control Systems\n",
            "- Artificial Intelligence\n",
            "- Decision Making\n",
            "Generated Text:\n",
            "Introduction to Topology. Topology is a topic that involves deep mathematical concepts. Applications. - Control Systems. - Artificial Intelligence. - Decision Making.\n",
            "Target Text:\n",
            "Introduction to Topology. Topology is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Control Systems. Artificial Intelligence. Decision Making.\n",
            "BERTScore F1: 0.8658\n",
            "--------------------------------------------------\n",
            "Sample 95:\n",
            "Input Text:\n",
            "# Introduction to Chaos Theory\n",
            "\n",
            "Chaos Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\hat{\\phi}(x,t) = \\int \\frac{d^3k}{(2\\pi)^3} \\frac{1}{\\sqrt{2E_k}} \\left( a_k e^{-ikx} + a_k^\\dagger e^{ikx} \\right) \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Quantum Mechanics\n",
            "- Particle Physics\n",
            "- Quantum Electrodynamics\n",
            "Generated Text:\n",
            "Introduction to Chaos Theory. Chaos Theory is a topic that involves deep mathematical concepts. Applications. - Quantum Mechanics. - Particle Physics. Quantum Electrodynamics.\n",
            "Target Text:\n",
            "Introduction to Chaos Theory. Chaos Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Quantum Mechanics. Particle Physics. Quantum Electrodynamics.\n",
            "BERTScore F1: 0.8983\n",
            "--------------------------------------------------\n",
            "Sample 96:\n",
            "Input Text:\n",
            "# Introduction to Nonlinear Dynamics\n",
            "\n",
            "Nonlinear Dynamics is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ (R, +, \\cdot) \\quad \\text{such that} \\quad a + b = b + a, \\ a\\cdot(b + c) = a\\cdot b + a\\cdot c \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Quantum Mechanics\n",
            "- Particle Physics\n",
            "- Quantum Electrodynamics\n",
            "Generated Text:\n",
            "Introduction to Nonlinear Dynamics. Nonlinear Dynamics is a topic that involves deep mathematical concepts. Applications. - Quantum Mechanics. - Particle Physics. Quantum Electrodynamics.\n",
            "Target Text:\n",
            "Introduction to Nonlinear Dynamics. Nonlinear Dynamics is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Quantum Mechanics. Particle Physics. Quantum Electrodynamics.\n",
            "BERTScore F1: 0.8974\n",
            "--------------------------------------------------\n",
            "Sample 97:\n",
            "Input Text:\n",
            "# Introduction to Group Theory\n",
            "\n",
            "Group Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\Delta x \\Delta p \\geq \\frac{h}{4\\pi} \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Physics\n",
            "- Differential Geometry\n",
            "- Algebraic Geometry\n",
            "Generated Text:\n",
            "Introduction to Group Theory. Group Theory is a topic that involves deep mathematical concepts. Applications. - Physics. - Differential Geometry. - Algebraic Geometry.\n",
            "Target Text:\n",
            "Introduction to Group Theory. Group Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Physics. Differential Geometry. Algebraic Geometry.\n",
            "BERTScore F1: 0.8709\n",
            "--------------------------------------------------\n",
            "Sample 98:\n",
            "Input Text:\n",
            "# Introduction to Covariant Derivatives\n",
            "\n",
            "Covariant Derivatives is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\hat{\\phi}(x,t) = \\int \\frac{d^3k}{(2\\pi)^3} \\frac{1}{\\sqrt{2E_k}} \\left( a_k e^{-ikx} + a_k^\\dagger e^{ikx} \\right) \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Machine Learning\n",
            "- Operations Research\n",
            "- Economics\n",
            "Generated Text:\n",
            "Introduction to Covariant Derivatives. Covariant Derivatives is a topic that involves deep mathematical concepts. Applications. Machine Learning. Operations Research. Economics.\n",
            "Target Text:\n",
            "Introduction to Covariant Derivatives. Covariant Derivatives is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Machine Learning. Operations Research. Economics.\n",
            "BERTScore F1: 0.9306\n",
            "--------------------------------------------------\n",
            "Sample 99:\n",
            "Input Text:\n",
            "# Introduction to Heisenberg Uncertainty Principle\n",
            "\n",
            "Heisenberg Uncertainty Principle is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\mu_A(x) \\in [0,1] \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Quantum Mechanics\n",
            "- Particle Physics\n",
            "- Quantum Electrodynamics\n",
            "Generated Text:\n",
            "Introduction to Heisenberg Uncertainty Principle. Heisenberg Uncertainty Principle is a topic that involves deep mathematical concepts. Applications. - Quantum Mechanics. - Particle Physics. Quantum Electrodynamics.\n",
            "Target Text:\n",
            "Introduction to Heisenberg Uncertainty Principle. Heisenberg Uncertainty Principle is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Quantum Mechanics. Particle Physics. Quantum Electrodynamics.\n",
            "BERTScore F1: 0.9133\n",
            "--------------------------------------------------\n",
            "Sample 100:\n",
            "Input Text:\n",
            "# Introduction to Spectral Graph Theory\n",
            "\n",
            "Spectral Graph Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\frac{\\partial u}{\\partial x} = \\frac{\\partial v}{\\partial y}, \\quad \\frac{\\partial u}{\\partial y} = -\\frac{\\partial v}{\\partial x} \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Classical Mechanics\n",
            "- Quantum Mechanics\n",
            "- Control Systems\n",
            "Generated Text:\n",
            "Introduction to Spectral Graph Theory. Spectral Graph Theory is a topic that involves deep mathematical concepts. Applications. - Classical Mechanics. - Quantum Mechanics. - Control Systems.\n",
            "Target Text:\n",
            "Introduction to Spectral Graph Theory. Spectral Graph Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Classical Mechanics. Quantum Mechanics. Control Systems.\n",
            "BERTScore F1: 0.8831\n",
            "--------------------------------------------------\n",
            "Sample 101:\n",
            "Input Text:\n",
            "# Introduction to Differential Forms\n",
            "\n",
            "Differential Forms is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\min_{x \\in X} f(x) \\quad \\text{subject to constraints} \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Control Systems\n",
            "- Artificial Intelligence\n",
            "- Decision Making\n",
            "Generated Text:\n",
            "Introduction to Differential Forms. Differential Forms is a topic that involves deep mathematical concepts. Applications. Control Systems. - Artificial Intelligence. - Decision Making.\n",
            "Target Text:\n",
            "Introduction to Differential Forms. Differential Forms is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Control Systems. Artificial Intelligence. Decision Making.\n",
            "BERTScore F1: 0.8920\n",
            "--------------------------------------------------\n",
            "Sample 102:\n",
            "Input Text:\n",
            "# Introduction to Tensor Calculus\n",
            "\n",
            "Tensor Calculus is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\lambda(G) = \\text{eigenvalue of the adjacency matrix} \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Cryptography\n",
            "- Polynomial Algebra\n",
            "- Number Theory\n",
            "Generated Text:\n",
            "Introduction to Tensor Calculus. Tensor Calculus is a topic that involves deep mathematical concepts. Applications. - Cryptography. - Polynomial Algebra. - Number Theory.\n",
            "Target Text:\n",
            "Introduction to Tensor Calculus. Tensor Calculus is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Cryptography. Polynomial Algebra. Number Theory.\n",
            "BERTScore F1: 0.8790\n",
            "--------------------------------------------------\n",
            "Sample 103:\n",
            "Input Text:\n",
            "# Introduction to Functional Analysis\n",
            "\n",
            "Functional Analysis is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\min_{x \\in X} f(x) \\quad \\text{subject to constraints} \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Classical Mechanics\n",
            "- Quantum Mechanics\n",
            "- Control Systems\n",
            "Generated Text:\n",
            "Introduction to Functional Analysis. Functional Analysis is a topic that involves deep mathematical concepts. Applications. Classical Mechanics. - Quantum Mechanics. - Control Systems.\n",
            "Target Text:\n",
            "Introduction to Functional Analysis. Functional Analysis is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Classical Mechanics. Quantum Mechanics. Control Systems.\n",
            "BERTScore F1: 0.8924\n",
            "--------------------------------------------------\n",
            "Sample 104:\n",
            "Input Text:\n",
            "# Introduction to Cauchy-Riemann Equations\n",
            "\n",
            "Cauchy-Riemann Equations is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ ||T|| = \\sup_{||x||=1} ||T(x)|| \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Signal Processing\n",
            "- Image Processing\n",
            "- Quantum Mechanics\n",
            "Generated Text:\n",
            "Introduction to Cauchy-Riemann Equations. Cauchy-Riemann Equations is a topic that involves deep mathematical concepts.\n",
            "Target Text:\n",
            "Introduction to Cauchy-Riemann Equations. Cauchy-Riemann Equations is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Signal Processing. Image Processing. Quantum Mechanics.\n",
            "BERTScore F1: 0.8496\n",
            "--------------------------------------------------\n",
            "Sample 105:\n",
            "Input Text:\n",
            "# Introduction to Chaos Theory\n",
            "\n",
            "Chaos Theory is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ (R, +, \\cdot) \\quad \\text{such that} \\quad a + b = b + a, \\ a\\cdot(b + c) = a\\cdot b + a\\cdot c \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Signal Processing\n",
            "- Image Processing\n",
            "- Quantum Mechanics\n",
            "Generated Text:\n",
            "Introduction to Chaos Theory. Chaos Theory is a topic that involves deep mathematical concepts. Applications. - Signal Processing. - Image Processing. Quantum Mechanics.\n",
            "Target Text:\n",
            "Introduction to Chaos Theory. Chaos Theory is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Signal Processing. Image Processing. Quantum Mechanics.\n",
            "BERTScore F1: 0.8925\n",
            "--------------------------------------------------\n",
            "Sample 106:\n",
            "Input Text:\n",
            "# Introduction to Algebraic Geometry\n",
            "\n",
            "Algebraic Geometry is a topic that involves deep mathematical concepts. \n",
            "\n",
            "\\[ \\text{Gal}(E/F) \\quad \\text{where } E \\text{ is an extension field of } F \\]\n",
            "\n",
            "## Applications\n",
            "\n",
            "- Classical Mechanics\n",
            "- Quantum Mechanics\n",
            "- Control Systems\n",
            "Generated Text:\n",
            "Introduction to Algebraic Geometry. Algebraic Geometry is a topic that involves deep mathematical concepts. Applications. Classical Mechanics. - Quantum Mechanics. - Control Systems.\n",
            "Target Text:\n",
            "Introduction to Algebraic Geometry. Algebraic Geometry is a topic that involves deep mathematical concepts. The formula is given for reference. Applications. Classical Mechanics. Quantum Mechanics. Control Systems.\n",
            "BERTScore F1: 0.8915\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-62bdc2ab8c7e>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Tokenize the input text and generate output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mgenerated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2062\u001b[0m             \u001b[0;31m# 14. run beam sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2063\u001b[0;31m             result = self._beam_search(\n\u001b[0m\u001b[1;32m   2064\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2065\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   3328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3329\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"past_key_values\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3330\u001b[0;31m                 model_kwargs[\"past_key_values\"] = self._temporary_reorder_cache(\n\u001b[0m\u001b[1;32m   3331\u001b[0m                     \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"past_key_values\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3332\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_temporary_reorder_cache\u001b[0;34m(self, past_key_values, beam_idx)\u001b[0m\n\u001b[1;32m   3082\u001b[0m         \u001b[0;31m# Exception 1: code path for models using the legacy cache format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3083\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3084\u001b[0;31m             \u001b[0mpast_key_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reorder_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3085\u001b[0m         \u001b[0;31m# Exception 2: models with different cache formats. These are limited to `DynamicCache` until their\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3086\u001b[0m         \u001b[0;31m# cache format is standardized, to avoid adding complexity to the codebase.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36m_reorder_cache\u001b[0;34m(self, past_key_values, beam_idx)\u001b[0m\n\u001b[1;32m   1847\u001b[0m                 \u001b[0;31m# need to set correct `past` for each of the four key / value states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m                 reordered_layer_past_states = reordered_layer_past_states + (\n\u001b[0;32m-> 1849\u001b[0;31m                     \u001b[0mlayer_past_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_past_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m                 )\n\u001b[1;32m   1851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jLFwMDbwKT4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from bert_score import score\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"./t5-text-to-text-models\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"./t5-text-to-text-models\")\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/combined_data.csv')\n",
        "\n",
        "# Preprocess function to ignore formulas\n",
        "def preprocess_text(text):\n",
        "    # Remove LaTeX math expressions and symbols\n",
        "    text = re.sub(r'\\\\\\w+', '', text)                 # Remove LaTeX commands like \\frac, \\sqrt, etc.\n",
        "    text = re.sub(r'\\$[^\\$]*\\$', '', text)            # Remove inline LaTeX math\n",
        "    text = re.sub(r'\\([^\\)]*\\)', '', text)            # Remove math in parentheses\n",
        "    text = re.sub(r'\\[[^\\]]*\\]', '', text)            # Remove content in square brackets\n",
        "    text = re.sub(r'_[a-zA-Z0-9]+', '', text)         # Remove subscripts\n",
        "    text = re.sub(r'\\^[a-zA-Z0-9]+', '', text)        # Remove superscripts\n",
        "\n",
        "    # Keep specific punctuation and remove extraneous symbols\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s.,;:\\'-]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()          # Clean up extra whitespace\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to input and target text\n",
        "data['input_text'] = data['input_text'].apply(preprocess_text)\n",
        "data['target_text'] = data['target_text'].apply(preprocess_text)\n",
        "\n",
        "# Store the BERTScore F1 results\n",
        "bert_scores = []\n",
        "\n",
        "# Generate predictions and calculate BERTScore for each row in the dataset\n",
        "for index, row in data.iterrows():\n",
        "    # Tokenize the input text and generate output\n",
        "    input_ids = tokenizer(row['input_text'], return_tensors=\"pt\", max_length=512, truncation=True).input_ids\n",
        "    outputs = model.generate(input_ids, max_length=150, num_beams=5, do_sample=True)\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Calculate BERTScore between generated text and target text\n",
        "    P, R, F1 = score([generated_text], [row['target_text']], lang=\"en\", model_type=\"bert-base-uncased\")\n",
        "    bert_score_f1 = F1.mean().item()\n",
        "    bert_scores.append(bert_score_f1)  # Store the F1 score from BERTScore\n",
        "\n",
        "    # Print each sample's details\n",
        "    print(f\"Sample {index + 1}:\")\n",
        "    print(f\"Input Text:\\n{row['input_text']}\")\n",
        "    print(f\"Generated Text:\\n{generated_text}\")\n",
        "    print(f\"Target Text:\\n{row['target_text']}\")\n",
        "    print(f\"BERTScore F1: {bert_score_f1:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Plot the BERTScore F1 scores for each sample\n",
        "plt.plot(range(1, len(bert_scores) + 1), bert_scores, marker=\"o\", linestyle=\"-\")\n",
        "plt.xlabel(\"Sample Number\")\n",
        "plt.ylabel(\"BERTScore F1 Score\")\n",
        "plt.title(\"BERTScore F1 Accuracy for Each Sample in the Dataset\")\n",
        "plt.xticks(range(1, len(bert_scores) + 1, max(1, len(bert_scores) // 10)))  # Adjust ticks for readability\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ar_idBWQEE12",
        "outputId": "f1098715-9599-47b0-9cd1-43d59be01fb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 1:\n",
            "Input Text:\n",
            "The limit of a function as is given by L .\n",
            "Generated Text:\n",
            "Die Limit der Funktion, wie L.\n",
            "Target Text:\n",
            "The limit of a function as is given by .\n",
            "BERTScore F1: 0.3229\n",
            "--------------------------------------------------\n",
            "Sample 2:\n",
            "Input Text:\n",
            "In calculus, the derivative of a function with respect to is denoted as .\n",
            "Generated Text:\n",
            "In calculus, the derivative of a function with respect to is denoted as.\n",
            "Target Text:\n",
            "In calculus, the derivative of a function with respect to is denoted as .\n",
            "BERTScore F1: 1.0000\n",
            "--------------------------------------------------\n",
            "Sample 3:\n",
            "Input Text:\n",
            "The integral of a function over an interval is represented by , dx .\n",
            "Generated Text:\n",
            "Die integrale einer Funktion über einen Zeitraum ist dargestellt durch, dx.\n",
            "Target Text:\n",
            "The integral of a function over an interval is represented by .\n",
            "BERTScore F1: 0.3162\n",
            "--------------------------------------------------\n",
            "Sample 4:\n",
            "Input Text:\n",
            "Euler's formula states that , a beautiful identity in mathematics.\n",
            "Generated Text:\n",
            "Euler's formula states that, a beautiful identity in mathematics.\n",
            "Target Text:\n",
            "Euler's formula states that , a beautiful identity in mathematics.\n",
            "BERTScore F1: 1.0000\n",
            "--------------------------------------------------\n",
            "Sample 5:\n",
            "Input Text:\n",
            "The Pythagorean theorem can be written as .\n",
            "Generated Text:\n",
            "Der Pythagorean theorem kann als.\n",
            "Target Text:\n",
            "The Pythagorean theorem can be written as .\n",
            "BERTScore F1: 0.6423\n",
            "--------------------------------------------------\n",
            "Sample 6:\n",
            "Input Text:\n",
            "The probability of an event is given by number of favorable outcomestotal number of outcomes .\n",
            "Generated Text:\n",
            "Die Wahrscheinlichkeit eines event wird gegeben durch Anzahl of positive outcomesTotal number of outcomes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
            "Target Text:\n",
            "The probability of an event is given by .\n",
            "BERTScore F1: 0.4066\n",
            "--------------------------------------------------\n",
            "Sample 7:\n",
            "Input Text:\n",
            "Newton's second law is formulated as , where is force, is mass, and is acceleration.\n",
            "Generated Text:\n",
            "'s second law is formulated as, where is force, is mass, and is acceleration. Newton's second law is formulated as, where is force, is mass, and is acceleration.\n",
            "Target Text:\n",
            "Newton's second law is formulated as , where is force, is mass, and is acceleration.\n",
            "BERTScore F1: 0.9058\n",
            "--------------------------------------------------\n",
            "Sample 8:\n",
            "Input Text:\n",
            "The quadratic formula is .\n",
            "Generated Text:\n",
            "Die quadratische Formulierung ist.\n",
            "Target Text:\n",
            "The quadratic formula is .\n",
            "BERTScore F1: 0.4182\n",
            "--------------------------------------------------\n",
            "Sample 9:\n",
            "Input Text:\n",
            "The area of a circle is given by , where is the radius.\n",
            "Generated Text:\n",
            "The area of a circle is given by, where is the radius.\n",
            "Target Text:\n",
            "The area of a circle is given by , where is the radius.\n",
            "BERTScore F1: 1.0000\n",
            "--------------------------------------------------\n",
            "Sample 10:\n",
            "Input Text:\n",
            "The binomial theorem states that k0n nk an-k b .\n",
            "Generated Text:\n",
            "k0n nk an-k b.\n",
            "Target Text:\n",
            "The binomial theorem states that .\n",
            "BERTScore F1: 0.2778\n",
            "--------------------------------------------------\n",
            "Sample 11:\n",
            "Input Text:\n",
            "Introduction to Heapsort Heapsort is a comparison-based sorting algorithm that uses a binary heap data structure to sort elements. Key Steps 1. Build a max heap 2. Extract elements from the heap 3. Heapify the remaining elements\n",
            "Generated Text:\n",
            "Heapsort Heapsort is a comparison-based sorting algorithm that uses a binary heap data structure to sort elements. Key Steps 1. Build a max heap 2. Extract elements from the heap 3. Heapify the remaining elements.\n",
            "Target Text:\n",
            "Introduction to Heapsort. Heapsort is a comparison-based sorting algorithm that uses a binary heap data structure to sort elements. Key Steps. Build a max heap. Extract elements from the heap. Heapify the remaining elements.\n",
            "BERTScore F1: 0.9066\n",
            "--------------------------------------------------\n",
            "Sample 12:\n",
            "Input Text:\n",
            "Basics of Computational Geometry Computational geometry studies algorithms for solving geometric problems involving points, lines, polygons, and more. Applications - Computer Graphics - Robotics - Geographic Information Systems\n",
            "Generated Text:\n",
            ", and more. Applications - Computer Graphics - Robotics - Geographic Information Systems Basics of Computational Geometry Computational geometry studies algorithms for solving geometric problems involving points, lines, polygons, and more. Applications - Computer Graphics - Robotics - Geographic Information Systems Basics of Computational Geometry Computational geometry studies algorithms for solving geometric problems involving points, lines, polygons, and more. Applications - Computer Graphics - Robotics - Geographic\n",
            "Target Text:\n",
            "Basics of Computational Geometry. Computational geometry studies algorithms for solving geometric problems involving points, lines, and polygons. Applications. Computer Graphics. Robotics. Geographic Information Systems.\n",
            "BERTScore F1: 0.7484\n",
            "--------------------------------------------------\n",
            "Sample 13:\n",
            "Input Text:\n",
            "Introduction to Merge Sort Merge sort is a divide-and-conquer sorting algorithm that splits the input array into halves, recursively sorts them, and merges the sorted halves. Applications 1. Sorting Large Datasets 2. External Sorting 3. Parallel Sorting\n",
            "Generated Text:\n",
            "Sort Merge Sort Merge sort is a divide-and-conquer sorting algorithm that splits the input array into halves, recursively sorts them, and merges the sorted halves. Applications 1. Sorting Large Datasets 2. External Sorting 3. Parallel Sorting 3. Parallel Sorting 3.\n",
            "Target Text:\n",
            "Introduction to Merge Sort. Merge sort is a divide-and-conquer sorting algorithm that splits an array, sorts the halves, and merges them. Applications. Sorting Large Datasets. External Sorting. Parallel Sorting.\n",
            "BERTScore F1: 0.8264\n",
            "--------------------------------------------------\n",
            "Sample 14:\n",
            "Input Text:\n",
            "Basics of Bit Manipulation Bit manipulation involves performing operations on binary data at the bit level. Key Operations - AND - OR - XOR - Left Shift - Right Shift\n",
            "Generated Text:\n",
            "Bit Manipulation Basics of Bit Manipulation Bit Manipulation Basics of Bit Manipulation Bit manipulation involves performing operations on binary data at bit level. Key Operations - AND - OR - XOR - Left Shift - Right Shift - Right Shift - Right Shift - Right Shift - Left Shift - Right Shift - Right Shift - Right Shift - Right Shift\n",
            "Target Text:\n",
            "Basics of Bit Manipulation. Bit manipulation involves performing operations on binary data at the bit level. Key Operations. AND. OR. XOR. Left Shift. Right Shift.\n",
            "BERTScore F1: 0.8017\n",
            "--------------------------------------------------\n",
            "Sample 15:\n",
            "Input Text:\n",
            "Introduction to Randomized Algorithms Randomized algorithms use random numbers to influence their behavior, often providing faster average performance or simplicity. Common Types 1. Monte Carlo Algorithms 2. Las Vegas Algorithms\n",
            "Generated Text:\n",
            "s 1. Monte Carlo Algorithms 2. Las Vegas Algorithms 2. Las Vegas Algorithms 2. Las Vegas Algorithms 2. Las Vegas Algorithms 2. Las Vegas Algorithms 2. Las Vegas Algorithms 2. Las Vegas Algorithms 2. Las Vegas Algorithms 2. Las Vegas Algorithms 2. Las Vegas Algorithms 2. Las Vegas Algorithms 2. Las Vegas Algorithms 2. Las Vegas Algorith\n",
            "Target Text:\n",
            "Introduction to Randomized Algorithms. Randomized algorithms use random numbers to influence their behavior. Common Types. Monte Carlo Algorithms. Las Vegas Algorithms.\n",
            "BERTScore F1: 0.5614\n",
            "--------------------------------------------------\n",
            "Sample 16:\n",
            "Input Text:\n",
            "Basics of Matrix Multiplication Matrix multiplication is the process of multiplying two matrices by taking the dot product of rows and columns. Applications - Computer Graphics - Physics Simulations - Machine Learning\n",
            "Generated Text:\n",
            "Multiplication Matrix Multiplication Matrix multiplication is a process of multiplying two matrices by taking the dot product of rows and columns. Applications - Computer Graphics - Physics Simulations - Machine Learning Matrix multiplication Matrix multiplication is a process of multiplying two matrices by taking the dot product of rows and columns.\n",
            "Target Text:\n",
            "Basics of Matrix Multiplication. Matrix multiplication is the process of multiplying two matrices by taking the dot product of rows and columns. The formula is given for reference. Applications. Computer Graphics. Physics Simulations. Machine Learning.\n",
            "BERTScore F1: 0.8026\n",
            "--------------------------------------------------\n",
            "Sample 17:\n",
            "Input Text:\n",
            "Introduction to Big-O Notation Big-O notation describes the upper bound of an algorithm's time complexity, giving an idea of its worst-case performance. Common Big-O Values 1. O 2. O 3. O 4. O\n",
            "Generated Text:\n",
            "Notation Big-O Notation Big-O notation describes the upper bound of an algorithm's time complexity, giving insight of its worst-case performance. Common Big-O Values 1. O 2. O 3. O 4. O 4. O 2. O 2. O 3. O 4. O 4. O 4. O 4. O 2. O 2. O 2. O 3. O 3. O 4. O 3. O 4. O 2. O 2. O 2. O 2. O 2. O 2. O 2. O 2. O 3. O 4. O 3. O 4. O 2. O 2. O 2. O 2. O 2.\n",
            "Target Text:\n",
            "Introduction to Big-O Notation. Big-O notation describes the upper bound of an algorithm's time complexity. Common Big-O Values. O. O. O. O.\n",
            "BERTScore F1: 0.6390\n",
            "--------------------------------------------------\n",
            "Sample 18:\n",
            "Input Text:\n",
            "Basics of Cryptanalysis Cryptanalysis is the study of analyzing and breaking cryptographic systems to decipher encrypted data without the key. Techniques - Brute Force Attack - Frequency Analysis - Known-Plaintext Attack\n",
            "Generated Text:\n",
            "- Brute Force Attack - Frequency Analysis - Known-Plaintext Attack - Known-Plaintext Attack - Known-Plaintext Attack - Known-Plaintext Attack - Known-Plaintext Attack - Known-Plaintext Attack - Known-Plaintext Attack - Known-Plaintext Attack - Known-Plaintext Attack - Known-Plaintext Attack\n",
            "Target Text:\n",
            "Basics of Cryptanalysis. Cryptanalysis is the study of analyzing and breaking cryptographic systems to decipher data without the key. Techniques. Brute Force Attack. Frequency Analysis. Known-Plaintext Attack.\n",
            "BERTScore F1: 0.6141\n",
            "--------------------------------------------------\n",
            "Sample 19:\n",
            "Input Text:\n",
            "Introduction to Depth-First Search Depth-First Search is an algorithm for traversing or searching tree or graph structures by exploring as far as possible along a branch before backtracking. Applications 1. Maze Solving 2. Pathfinding 3. Topological Sorting\n",
            "Generated Text:\n",
            "Depth-First Search Depth-First Search is an algorithm for traversing or searching tree or graph structures by exploring as far as possible along a branch before backtracking.\n",
            "Target Text:\n",
            "Introduction to Depth-First Search . DFS is an algorithm for traversing or searching trees and graphs by exploring as far as possible before backtracking. Applications. Maze Solving. Pathfinding. Topological Sorting.\n",
            "BERTScore F1: 0.7658\n",
            "--------------------------------------------------\n",
            "Sample 20:\n",
            "Input Text:\n",
            "Basics of Public Key Infrastructure Public Key Infrastructure provides the tools and processes for securely managing digital certificates and public keys. Key Components - Certificate Authorities - Digital Certificates - Public and Private Keys\n",
            "Generated Text:\n",
            "Public Key Infrastructure Public Key Infrastructure provides the tools and processes for securely managing digital certificates and public keys. Key Components - Certificate Authorities - Digital Certificates - Public and Private Keys - Public and Private Keys Keys Public Key Infrastructure provides the tools and processes for securely managing digital certificates and public keys. Key Components - Certificate Authorities - Digital Certificates - Public and Private Keys - Public and Private Keys Keys Public Key Infrastructure provides the tools and processes for securely managing digital certificates and public keys.\n",
            "Target Text:\n",
            "Basics of Public Key Infrastructure . PKI provides tools for managing digital certificates and public keys securely. Key Components. Certificate Authorities . Digital Certificates. Public and Private Keys.\n",
            "BERTScore F1: 0.7474\n",
            "--------------------------------------------------\n",
            "Sample 21:\n",
            "Input Text:\n",
            "Introduction to Linear Programming Linear programming is an optimization technique used to find the best outcome in a mathematical model whose requirements are represented by linear relationships. Key Concepts 1. Objective Function 2. Constraints 3. Feasible Region\n",
            "Generated Text:\n",
            "Linear Programming Linear programming is an optimization technique used to find the best outcome in a mathematical model whose requirements are represented by linear relationships. Key Concepts 1. Objective Function 2. Constraints 3. Feasible Region\n",
            "Target Text:\n",
            "Introduction to Linear Programming. Linear programming is an optimization technique for finding the best outcome in a model with linear relationships. Key Concepts. Objective Function. Constraints. Feasible Region.\n",
            "BERTScore F1: 0.7883\n",
            "--------------------------------------------------\n",
            "Sample 22:\n",
            "Input Text:\n",
            "Basics of Markov Chain Monte Carlo MCMC methods generate samples from a probability distribution based on constructing a Markov chain with the desired distribution as its equilibrium. Applications - Bayesian Inference - Machine Learning - Statistical Physics\n",
            "Generated Text:\n",
            "Monte Carlo MCMC methods generate samples from a probability distribution based on constructing a Markov chain with the desired distribution as its equilibrium. Applications - Bayesian Inference - Machine Learning - Statistical Physics - Statistical Physics - Markov Chain Basics of Markov Chain Monte Carlo MCMC methods generate samples from a probability distribution based on constructing a Markov chain with the desired distribution as its equilibrium. Applications - Bayesian Inference - Machine Learning -\n",
            "Target Text:\n",
            "Basics of Markov Chain Monte Carlo . MCMC methods generate samples from a probability distribution using a Markov chain. Applications. Bayesian Inference. Machine Learning. Statistical Physics.\n",
            "BERTScore F1: 0.7390\n",
            "--------------------------------------------------\n",
            "Sample 23:\n",
            "Input Text:\n",
            "Introduction to AVL Trees AVL trees are self-balancing binary search trees where the difference in heights of left and right subtrees cannot be more than one for any node. Operations 1. Insertion 2. Deletion 3. Rotation\n",
            "Generated Text:\n",
            ". Introduction to AVL Trees AVL Trees AVL trees are self-balancing binary search trees where the difference in heights of left and right subtrees cannot be more than one for any node. Operations 1. Insertion 2. Deletion 3. Rotation 3. Rotation 3. Rotation 3. Rotation 3. Rotation 3. Rotation 3. Rotation 1. Insertion 2. Deletion 3. Rotation 3. Rotation 3. Rotation 3. Rotation 3. Rotation 3. Rotation 3. Rotation 3. Rotation 3.\n",
            "Target Text:\n",
            "Introduction to AVL Trees. AVL trees are self-balancing binary search trees with height differences limited to one. Operations. Insertion. Deletion. Rotation.\n",
            "BERTScore F1: 0.6869\n",
            "--------------------------------------------------\n",
            "Sample 24:\n",
            "Input Text:\n",
            "Basics of Secure Multiparty Computation SMPC allows multiple parties to jointly compute a function over their inputs while keeping those inputs private. Applications - Secure Voting - Private Machine Learning - Collaborative Data Analysis\n",
            "Generated Text:\n",
            "- Secure Voting - Private Machine Learning - Collaborative Data Analysis SMPC Basics of Secure Multiparty Computation SMPC allows multiple parties to jointly compute a function over their inputs while keeping those inputs private. Applications - Secure Voting - Private Machine Learning - Collaborative Data Analysis - Collaborative Data Analysis SMPC allows multiple parties to jointly compute a function over their inputs while keeping those inputs private. Applications - Secure Voting - Private Machine Learning -\n",
            "Target Text:\n",
            "Basics of Secure Multiparty Computation . SMPC allows multiple parties to compute a function while keeping inputs private. Applications. Secure Voting. Private Machine Learning. Collaborative Data Analysis.\n",
            "BERTScore F1: 0.7681\n",
            "--------------------------------------------------\n",
            "Sample 25:\n",
            "Input Text:\n",
            "Introduction to Bellman-Ford Algorithm The Bellman-Ford algorithm calculates shortest paths from a single source to all other nodes in a weighted graph, even when edges have negative weights. Applications 1. Shortest Path Problems 2. Network Routing 3. Detecting Negative Cycles\n",
            "Generated Text:\n",
            "The Bellman-Ford algorithm calculates shortest paths from a single source to all other nodes in a weighted graph, even when edges have negative weights. Applications 1. Shortest Path Problems 2. Network Routing 3. Detecting Negative Cycles.\n",
            "Target Text:\n",
            "Introduction to Bellman-Ford Algorithm. Bellman-Ford calculates shortest paths in a weighted graph, even with negative edges. Applications. Shortest Path Problems. Network Routing. Detecting Negative Cycles.\n",
            "BERTScore F1: 0.7982\n",
            "--------------------------------------------------\n",
            "Sample 26:\n",
            "Input Text:\n",
            "Basics of Digital Signatures A digital signature is a mathematical technique used to validate the authenticity and integrity of a message, software, or digital document. Key Properties - Authentication - Non-repudiation - Integrity\n",
            "Generated Text:\n",
            "Key Properties - Authentication - Non-repudiation - Integrity of a message, software, or digital document. Key Properties - Authentication - Non-repudiation - Integrity of a message, software, or digital document. Key Properties - Authentication - Non-repudiation - Integrity of a message, software, or digital document. Key Properties - Authentication - Non-repudiation - Integrity of a\n",
            "Target Text:\n",
            "Basics of Digital Signatures. A digital signature validates the authenticity and integrity of a message or document. Key Properties. Authentication. Non-repudiation. Integrity.\n",
            "BERTScore F1: 0.6717\n",
            "--------------------------------------------------\n",
            "Sample 27:\n",
            "Input Text:\n",
            "Introduction to Graph Traversal Graph traversal refers to the process of visiting all nodes or vertices in a graph. The two most common methods are depth-first search and breadth-first search . Applications 1. Web Crawling 2. Pathfinding 3. Social Network Analysis\n",
            "Generated Text:\n",
            "Graph Traversal Graph traversal refers to the process of visiting all nodes or vertices in a graph. The two most common methods are depth-first search and breadth-first search. Applications 1. Web Crawling 2. Pathfinding 3. Social Network Analysis\n",
            "Target Text:\n",
            "Introduction to Graph Traversal. Graph traversal is the process of visiting all nodes or vertices in a graph. The two most common methods are DFS and BFS. Applications. Web Crawling. Pathfinding. Social Network Analysis.\n",
            "BERTScore F1: 0.8097\n",
            "--------------------------------------------------\n",
            "Sample 28:\n",
            "Input Text:\n",
            "Basics of Symmetric Encryption Symmetric encryption uses the same key for both encryption and decryption. Algorithms - Advanced Encryption Standard - Data Encryption Standard - Blowfish\n",
            "Generated Text:\n",
            "Symmetric Encryption Symmetric Encryption Symmetric encryption uses the same key for both encryption and decryption. Algorithms - Advanced Encryption Standard - Data Encryption Standard - Blowfish.\n",
            "Target Text:\n",
            "Basics of Symmetric Encryption. Symmetric encryption uses the same key for both encryption and decryption. Algorithms. Advanced Encryption Standard . Data Encryption Standard . Blowfish.\n",
            "BERTScore F1: 0.8828\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-b5e8df6278d7>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Tokenize the input text and generate output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mgenerated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2062\u001b[0m             \u001b[0;31m# 14. run beam sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2063\u001b[0;31m             result = self._beam_search(\n\u001b[0m\u001b[1;32m   2064\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2065\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   3236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3237\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Unchanged original behavior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3238\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3240\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msynced_gpus\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthis_peer_finished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1765\u001b[0m             \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_output\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dim\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1767\u001b[0;31m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1769\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"./t5-text-to-text-models\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"./t5-text-to-text-models\")\n",
        "\n",
        "# Example input text\n",
        "input_text = \"\"\"\n",
        "\n",
        "# Principles of Reinforcement Learning\n",
        "\n",
        "\n",
        "\n",
        "* Agent and Environment: The agent is the learner or decision-maker, while the environment is the external system that the agent interacts with.\n",
        "* •\tState: The state represents the current situation or configuration of the environment. It encapsulates all the relevant information that the agent needs to make a decision.\n",
        "\n",
        "* •\tAction: An action is any decision or move the agent makes in response to the current state. The set of all possible actions is known as the action space.\n",
        "* •Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time.\n",
        "\n",
        "* •\tPolicy: The policy is a strategy or mapping from states to actions. It defines the agent's behavior at any given state, determining which action to take.\n",
        "* •\tValue Function: The value function estimates the expected cumulative reward that can be obtained from a particular state (or state-action pair). It helps the agent to evaluate which states or actions are more promising in the long run.\n",
        "  .\n",
        "\n",
        "# Applications of Reinforcement Learning\n",
        "Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include:\n",
        "\n",
        "\n",
        "1. •\tGaming: RL has been widely used in the development of game-playing AI, from classic board games like Go and Chess to complex video games like StarCraft and Dota 2.\n",
        "2. •\tRobotics: In robotics, RL is used to teach robots how to perform tasks through trial and error, such as grasping objects, walking, or navigating environments.\n",
        "\n",
        "1. •\tAutonomous Vehicles: RL plays a significant role in the development of self-driving cars. These vehicles need to make real-time decisions based on their surroundings.\n",
        "2. •\tEnergy Management: RL is used in optimizing energy consumption in smart grids, reducing operational costs in power plants, and improving the efficiency of renewable energy systems.\n",
        "\n",
        "1. •\tHealthcare: In healthcare, RL is used for personalized treatment planning, drug discovery, and optimizing medical procedures.\n",
        "\n",
        "# Key Algorithms in Reinforcement Learning\n",
        "\n",
        "Several algorithms have been developed to address different challenges in reinforcement learning. Two of the most fundamental and widely used algorithms are Q-learning and Deep Q-Networks (DQN).\n",
        "Update Rule:\n",
        "\n",
        "The Q-value for a state-action pair is updated using the following rule:\n",
        "\n",
        "\n",
        "Q(s, a) ← Q(s, a) + α [r + γ max_a' Q(s', a') − Q(s, a)]\n",
        "\n",
        "Here, s and a are the current state and action, r is the reward received, s' is the next state, γ is the discount factor, and α is the learning rate.\n",
        "\n",
        "\n",
        "\n",
        "# Deep Q-Networks (DQN)\n",
        "\n",
        "DQN is an extension of Q-learning that uses deep neural networks to approximate the Q-function, making it scalable to environments with large or continuous state spaces.\n",
        "\n",
        "Network Architecture:\n",
        "\n",
        "In DQN, a neural network is used to approximate the Q-values for all possible actions given a state. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted.\n",
        "\n",
        "Experience Replay:\n",
        "\n",
        "To improve stability and efficiency, DQN uses experience replay, where the agent's experiences (state, action, reward, next state) are stored in a replay buffer. During training, random samples from this buffer are used to update the network, breaking the correlation between consecutive updates.\n",
        "\n",
        "Target Network:\n",
        "\n",
        "DQN also uses a target network, a copy of the Q-network that is updated less frequently. This helps in stabilizing the training by providing a more consistent target for the Q-value updates.\n",
        "\n",
        "Success Stories:\n",
        "\n",
        "DQN gained significant attention when it was used by DeepMind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs, showcasing the power of combining RL with deep learning.\n",
        "\n",
        "\n",
        "# conclusion\n",
        "\n",
        "Reinforcement learning represents a powerful approach to solving complex decision-making problems by enabling agents to learn through interaction with their environment.\n",
        "With applications ranging from gaming and robotics to healthcare and finance, RL continues to push the boundaries of what AI systems can achieve. Algorithms like Q-learning\n",
        "and Deep Q-Networks have laid the foundation for modern RL, and ongoing research promises to unlock even greater potential in the years to come.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Split input text into chunks if too long\n",
        "def chunk_text(text, max_chunk_size=400):\n",
        "    words = text.split()\n",
        "    return [' '.join(words[i:i + max_chunk_size]) for i in range(0, len(words), max_chunk_size)]\n",
        "\n",
        "# Split the input text into chunks\n",
        "text_chunks = chunk_text(input_text, max_chunk_size=100)\n",
        "\n",
        "# Generate output for each chunk and combine\n",
        "output_text = \"\"\n",
        "for chunk in text_chunks:\n",
        "    input_ids = tokenizer(chunk, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(input_ids['input_ids'], max_length=300, num_beams=5, do_sample=False)\n",
        "    output_text += tokenizer.decode(outputs[0], skip_special_tokens=True) + \" \"\n",
        "\n",
        "# Display final output\n",
        "print(\"Model Output:\\n\", output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zu2TKEb2JhKP",
        "outputId": "c6f9ede3-0499-4dc8-f0fa-8c1c2093f68b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Output:\n",
            " • State: The state represents the current situation or configuration of the environment. The set of possible actions is known as the action space. * •Reward. After taking an action. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time. * • Policy: The policy is a strategy or mapping from states to actions. The value function estimates the expected cumulative reward.  Key Algorithms in Reinforcement Learning. Several algorithms have been developed to address different challenges in reinforcement learning. Two of the most fundamental and widely used algorithms are Q-learning. Deep Q-Networks (DQN) uses deep neural networks to approximate the Q-function, making it scalable to environments with large or continuous state spaces. Network Architecture: In DQN, a neural network is used to approximate the Q-values for all possible actions given a state. Q(s, a)  Q(s, a) +  [r +  max_a' Q(s', a')  Q(s, a)]. Here, Experience Replay: To improve stability and efficiency, DQN uses experience replay, where the agent's experiences are stored in a replay buffer. During training, random samples from this buffer are used to update the network. This helps in stabilizing the training by providing a more consistent target for the Q-value updates. Success Stories: DQN gained significant attention when it was used by DeepMind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs, showcasing the power of combining RL with deep learning. # conclusion. Reinforcement learning represents a powerful approach to solving complex decision-making problems. Modern RL, and ongoing research promises to unlock even greater potential in the future. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"./t5-text-to-text-models\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"./t5-text-to-text-models\")\n",
        "\n",
        "# Example input text\n",
        "input_text = \"\"\"\n",
        "\n",
        "# Principles of Reinforcement Learning\n",
        "* Agent and Environment: The agent is the learner or decision-maker, while the environment is the external system that the agent interacts with.\n",
        "* •\tState: The state represents the current situation or configuration of the environment. It encapsulates all the relevant information that the agent needs to make a decision.\n",
        "\n",
        "* •\tAction: An action is any decision or move the agent makes in response to the current state. The set of all possible actions is known as the action space.\n",
        "* •Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time.\n",
        "\n",
        "* •\tPolicy: The policy is a strategy or mapping from states to actions. It defines the agent's behavior at any given state, determining which action to take.\n",
        "* •\tValue Function: The value function estimates the expected cumulative reward that can be obtained from a particular state (or state-action pair). It helps the agent to evaluate which states or actions are more promising in the long run.\n",
        "  .\n",
        "\n",
        "# Applications of Reinforcement Learning\n",
        "Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include:\n",
        "\n",
        "\n",
        "1. •\tGaming: RL has been widely used in the development of game-playing AI, from classic board games like Go and Chess to complex video games like StarCraft and Dota 2.\n",
        "2. •\tRobotics: In robotics, RL is used to teach robots how to perform tasks through trial and error, such as grasping objects, walking, or navigating environments.\n",
        "\n",
        "1. •\tAutonomous Vehicles: RL plays a significant role in the development of self-driving cars. These vehicles need to make real-time decisions based on their surroundings.\n",
        "2. •\tEnergy Management: RL is used in optimizing energy consumption in smart grids, reducing operational costs in power plants, and improving the efficiency of renewable energy systems.\n",
        "\n",
        "1. •\tHealthcare: In healthcare, RL is used for personalized treatment planning, drug discovery, and optimizing medical procedures.\n",
        "\n",
        "# Key Algorithms in Reinforcement Learning\n",
        "\n",
        "Several algorithms have been developed to address different challenges in reinforcement learning. Two of the most fundamental and widely used algorithms are Q-learning and Deep Q-Networks (DQN).\n",
        "Update Rule:\n",
        "\n",
        "The Q-value for a state-action pair is updated using the following rule:\n",
        "\n",
        "\n",
        "Q(s, a) ← Q(s, a) + α [r + γ max_a' Q(s', a') − Q(s, a)]\n",
        "\n",
        "Here, s and a are the current state and action, r is the reward received, s' is the next state, γ is the discount factor, and α is the learning rate.\n",
        "\n",
        "\n",
        "\n",
        "# Deep Q-Networks (DQN)\n",
        "\n",
        "DQN is an extension of Q-learning that uses deep neural networks to approximate the Q-function, making it scalable to environments with large or continuous state spaces.\n",
        "\n",
        "Network Architecture:\n",
        "\n",
        "In DQN, a neural network is used to approximate the Q-values for all possible actions given a state. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted.\n",
        "\n",
        "Experience Replay:\n",
        "\n",
        "To improve stability and efficiency, DQN uses experience replay, where the agent's experiences (state, action, reward, next state) are stored in a replay buffer. During training, random samples from this buffer are used to update the network, breaking the correlation between consecutive updates.\n",
        "\n",
        "Target Network:\n",
        "\n",
        "DQN also uses a target network, a copy of the Q-network that is updated less frequently. This helps in stabilizing the training by providing a more consistent target for the Q-value updates.\n",
        "\n",
        "Success Stories:\n",
        "\n",
        "DQN gained significant attention when it was used by DeepMind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs, showcasing the power of combining RL with deep learning.\n",
        "\n",
        "\n",
        "# conclusion\n",
        "\n",
        "Reinforcement learning represents a powerful approach to solving complex decision-making problems by enabling agents to learn through interaction with their environment.\n",
        "With applications ranging from gaming and robotics to healthcare and finance, RL continues to push the boundaries of what AI systems can achieve. Algorithms like Q-learning\n",
        "and Deep Q-Networks have laid the foundation for modern RL, and ongoing research promises to unlock even greater potential in the years to come.\n",
        "\"\"\"\n",
        "\n",
        "# Split input text into chunks with overlap\n",
        "def chunk_text_with_overlap(text, max_chunk_size=400, overlap_size=15):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), max_chunk_size - overlap_size):\n",
        "        chunk = words[i:i + max_chunk_size]\n",
        "        chunks.append(' '.join(chunk))\n",
        "    return chunks\n",
        "\n",
        "# Split the input text into chunks with overlap\n",
        "text_chunks = chunk_text_with_overlap(input_text, max_chunk_size=100, overlap_size=15)\n",
        "\n",
        "# Generate output for each chunk, adjusting max_length, and combine\n",
        "output_text = \"\"\n",
        "for chunk in text_chunks:\n",
        "    input_ids = tokenizer(chunk, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(input_ids['input_ids'], max_length=350, num_beams=5, do_sample=False)\n",
        "    chunk_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    output_text += chunk_output.strip() + \" \"\n",
        "\n",
        "# Post-process to clean up the output text\n",
        "output_text = ' '.join(output_text.split())  # Removes extra whitespace\n",
        "\n",
        "# Display final output\n",
        "print(\"Model Output:\\n\", output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8200UePDc_iF",
        "outputId": "8c0253ab-4f3c-4928-9f66-6b340dba545f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Output:\n",
            " • State: The state represents the current situation or configuration of the environment. The set of possible actions is known as the action space. * •Reward. After taking an action. * •Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time. * • Policy: The policy is a strategy or mapping from states to actions. Applications of Reinforcement Learning. Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include: 1. • Gaming. RL is used to teach robots how to perform. 2. Robotics: In robotics, RL is used to teach robots how to perform tasks through trial and error, such as grasping objects, walking, or navigating environments. 1. Autonomous Vehicles: RL plays a significant role in the development of self-driving cars. These vehicles need to make real-time decisions based on their surroundings. 2. Energy Management. RL is used in optimizing energy consumption in smart grids. reducing operational costs. Key Algorithms in Reinforcement Learning. Reinforcement Learning. Two of the most fundamental and widely used algorithms are Q-learning. Deep Q-Networks. (DQN). Update Rule: The Q-value for a state-action pair is updated using the following rule: Q(s, a) Q(s, a) + [r + max_a' Q(s', a') Q(s, a)] Here, Deep Q-Networks (DQN) uses deep neural networks to approximate the Q-function. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted. Experience Replay. To improve performance. Experience Replay: To improve stability and efficiency, DQN uses experience replay, where the agent's experiences (state, action, reward, next state) are stored in a replay buffer. random samples from this buffer are used to update the network. This helps in stabilizing the training by providing a more consistent target for the Q-value updates. Success Stories: DQN gained significant attention when it was used. Success Stories: DQN gained significant attention when it was used by DeepMind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs. showcasing the power of combining RL with deep learning. Reinforcement learning represents a powerful approach to solving complex decision-making problems. Algorithms like Q-learning and Deep Q-Networks have laid the foundation for modern RL, and ongoing research promises to unlock even greater potential in the years to come.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"./t5-text-to-text-models\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"./t5-text-to-text-models\")\n",
        "\n",
        "# Example input text\n",
        "input_text = \"\"\"\n",
        "# Principles of Reinforcement Learning\n",
        "* Agent and Environment: The agent is the learner or decision-maker, while the environment is the external system that the agent interacts with.\n",
        "* •\tState: The state represents the current situation or configuration of the environment. It encapsulates all the relevant information that the agent needs to make a decision.\n",
        "\n",
        "* •\tAction: An action is any decision or move the agent makes in response to the current state. The set of all possible actions is known as the action space.\n",
        "* •Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time.\n",
        "\n",
        "* •\tPolicy: The policy is a strategy or mapping from states to actions. It defines the agent's behavior at any given state, determining which action to take.\n",
        "* •\tValue Function: The value function estimates the expected cumulative reward that can be obtained from a particular state (or state-action pair). It helps the agent to evaluate which states or actions are more promising in the long run.\n",
        "  .\n",
        "\n",
        "# Applications of Reinforcement Learning\n",
        "Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include:\n",
        "\n",
        "\n",
        "1. •\tGaming: RL has been widely used in the development of game-playing AI, from classic board games like Go and Chess to complex video games like StarCraft and Dota 2.\n",
        "2. •\tRobotics: In robotics, RL is used to teach robots how to perform tasks through trial and error, such as grasping objects, walking, or navigating environments.\n",
        "\n",
        "1. •\tAutonomous Vehicles: RL plays a significant role in the development of self-driving cars. These vehicles need to make real-time decisions based on their surroundings.\n",
        "2. •\tEnergy Management: RL is used in optimizing energy consumption in smart grids, reducing operational costs in power plants, and improving the efficiency of renewable energy systems.\n",
        "\n",
        "1. •\tHealthcare: In healthcare, RL is used for personalized treatment planning, drug discovery, and optimizing medical procedures.\n",
        "\n",
        "# Key Algorithms in Reinforcement Learning\n",
        "\n",
        "Several algorithms have been developed to address different challenges in reinforcement learning. Two of the most fundamental and widely used algorithms are Q-learning and Deep Q-Networks (DQN).\n",
        "Update Rule:\n",
        "\n",
        "The Q-value for a state-action pair is updated using the following rule:\n",
        "\n",
        "\n",
        "\\[Q(s, a) ← Q(s, a) + α [r + γ max_a' Q(s', a') − Q(s, a)]\\\n",
        "\n",
        "Here, s and a are the current state and action, r is the reward received, s' is the next state, γ is the discount factor, and α is the learning rate.\n",
        "\n",
        "\n",
        "\n",
        "# Deep Q-Networks (DQN)\n",
        "\n",
        "DQN is an extension of Q-learning that uses deep neural networks to approximate the Q-function, making it scalable to environments with large or continuous state spaces.\n",
        "\n",
        "Network Architecture:\n",
        "\n",
        "In DQN, a neural network is used to approximate the Q-values for all possible actions given a state. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted.\n",
        "\n",
        "Experience Replay:\n",
        "\n",
        "To improve stability and efficiency, DQN uses experience replay, where the agent's experiences (state, action, reward, next state) are stored in a replay buffer. During training, random samples from this buffer are used to update the network, breaking the correlation between consecutive updates.\n",
        "\n",
        "Target Network:\n",
        "\n",
        "DQN also uses a target network, a copy of the Q-network that is updated less frequently. This helps in stabilizing the training by providing a more consistent target for the Q-value updates.\n",
        "\n",
        "Success Stories:\n",
        "\n",
        "DQN gained significant attention when it was used by DeepMind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs, showcasing the power of combining RL with deep learning.\n",
        "\n",
        "\n",
        "# conclusion\n",
        "\n",
        "Reinforcement learning represents a powerful approach to solving complex decision-making problems by enabling agents to learn through interaction with their environment.\n",
        "With applications ranging from gaming and robotics to healthcare and finance, RL continues to push the boundaries of what AI systems can achieve. Algorithms like Q-learning\n",
        "and Deep Q-Networks have laid the foundation for modern RL, and ongoing research promises to unlock even greater potential in the years to come.\n",
        "\"\"\"\n",
        "\n",
        "# Split input text into chunks with overlap\n",
        "def chunk_text_with_overlap(text, max_chunk_size=400, overlap_size=15):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), max_chunk_size - overlap_size):\n",
        "        chunk = words[i:i + max_chunk_size]\n",
        "        chunks.append(' '.join(chunk))\n",
        "    return chunks\n",
        "\n",
        "# Split the input text into chunks with overlap\n",
        "text_chunks = chunk_text_with_overlap(input_text, max_chunk_size=100, overlap_size=15)\n",
        "\n",
        "# Helper function to remove redundant phrases\n",
        "def remove_redundant_phrases(text):\n",
        "    sentences = text.split(\". \")\n",
        "    seen = set()\n",
        "    unique_sentences = []\n",
        "    for sentence in sentences:\n",
        "        if sentence not in seen:\n",
        "            unique_sentences.append(sentence)\n",
        "            seen.add(sentence)\n",
        "    return \". \".join(unique_sentences)\n",
        "\n",
        "# Generate output for each chunk, adjusting max_length, and combine\n",
        "output_text = \"\"\n",
        "for chunk in text_chunks:\n",
        "    input_ids = tokenizer(chunk, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(input_ids['input_ids'], max_length=350, num_beams=5, do_sample=False)\n",
        "    chunk_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    output_text += chunk_output.strip() + \" \"\n",
        "\n",
        "# Post-process to clean up and format the output text\n",
        "output_text = remove_redundant_phrases(output_text)\n",
        "output_text = ' '.join(output_text.split())  # Removes extra whitespace\n",
        "\n",
        "# Display final output\n",
        "print(\"Model Output:\\n\", output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHG1fCvaev9-",
        "outputId": "d960e5eb-7d5d-4391-9a6c-0487cc702ce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Output:\n",
            " • State: The state represents the current situation or configuration of the environment. The set of possible actions is known as the action space. * •Reward. After taking an action. * •Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time. * • Policy: The policy is a strategy or mapping from states to actions. Applications of Reinforcement Learning. Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include: 1. • Gaming. RL is used to teach robots how to perform. 2. Robotics: In robotics, RL is used to teach robots how to perform tasks through trial and error, such as grasping objects, walking, or navigating environments. 1. Autonomous Vehicles: RL plays a significant role in the development of self-driving cars. These vehicles need to make real-time decisions based on their surroundings. Energy Management. RL is used in optimizing energy consumption in smart grids. reducing operational costs. Key Algorithms in Reinforcement Learning. Reinforcement Learning. Two of the most fundamental and widely used algorithms are Q-learning. Deep Q-Networks. (DQN). Update Rule: The Q-value for a state-action pair is updated using the following rule: [Q(s, a) Q(s, a) + [r + max_a' Q(s', a') Q(s, a)] Deep Q-Networks (DQN) uses deep neural networks to approximate the Q-function. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted. Experience Replay. To improve performance. Experience Replay: To improve stability and efficiency, DQN uses experience replay, where the agent's experiences (state, action, reward, next state) are stored in a replay buffer. random samples from this buffer are used to update the network. This helps in stabilizing the training by providing a more consistent target for the Q-value updates. Success Stories: DQN gained significant attention when it was used. Success Stories: DQN gained significant attention when it was used by DeepMind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs. showcasing the power of combining RL with deep learning. Reinforcement learning represents a powerful approach to solving complex decision-making problems. Algorithms like Q-learning and Deep Q-Networks have laid the foundation for modern RL, and ongoing research promises to unlock even greater potential in the years to come.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk rouge-score\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAokpTecxLO7",
        "outputId": "04469c01-f8cf-4fb8-8c14-3e320a3a09d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=d4c79dce182cdc7f16937b5060b25643737a8e66e4fb101a13f40a61ceeceee1\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"./t5-text-to-text-models\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"./t5-text-to-text-models\")\n",
        "\n",
        "# Example input text\n",
        "input_text = \"\"\"\n",
        "# Principles of Reinforcement Learning\n",
        "* Agent and Environment: The agent is the learner or decision-maker, while the environment is the external system that the agent interacts with.\n",
        "* •\tState: The state represents the current situation or configuration of the environment. It encapsulates all the relevant information that the agent needs to make a decision.\n",
        "\n",
        "* •\tAction: An action is any decision or move the agent makes in response to the current state. The set of all possible actions is known as the action space.\n",
        "* •Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time.\n",
        "\n",
        "* •\tPolicy: The policy is a strategy or mapping from states to actions. It defines the agent's behavior at any given state, determining which action to take.\n",
        "* •\tValue Function: The value function estimates the expected cumulative reward that can be obtained from a particular state (or state-action pair). It helps the agent to evaluate which states or actions are more promising in the long run.\n",
        "  .\n",
        "\n",
        "# Applications of Reinforcement Learning\n",
        "Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include:\n",
        "\n",
        "\n",
        "1. •\tGaming: RL has been widely used in the development of game-playing AI, from classic board games like Go and Chess to complex video games like StarCraft and Dota 2.\n",
        "2. •\tRobotics: In robotics, RL is used to teach robots how to perform tasks through trial and error, such as grasping objects, walking, or navigating environments.\n",
        "\n",
        "1. •\tAutonomous Vehicles: RL plays a significant role in the development of self-driving cars. These vehicles need to make real-time decisions based on their surroundings.\n",
        "2. •\tEnergy Management: RL is used in optimizing energy consumption in smart grids, reducing operational costs in power plants, and improving the efficiency of renewable energy systems.\n",
        "\n",
        "1. •\tHealthcare: In healthcare, RL is used for personalized treatment planning, drug discovery, and optimizing medical procedures.\n",
        "\n",
        "# Key Algorithms in Reinforcement Learning\n",
        "\n",
        "Several algorithms have been developed to address different challenges in reinforcement learning. Two of the most fundamental and widely used algorithms are Q-learning and Deep Q-Networks (DQN).\n",
        "Update Rule:\n",
        "\n",
        "The Q-value for a state-action pair is updated using the following rule:\n",
        "\n",
        "\n",
        "\\[Q(s, a) ← Q(s, a) + α [r + γ max_a' Q(s', a') − Q(s, a)]\\\n",
        "\n",
        "Here, s and a are the current state and action, r is the reward received, s' is the next state, γ is the discount factor, and α is the learning rate.\n",
        "\n",
        "\n",
        "\n",
        "# Deep Q-Networks (DQN)\n",
        "\n",
        "DQN is an extension of Q-learning that uses deep neural networks to approximate the Q-function, making it scalable to environments with large or continuous state spaces.\n",
        "\n",
        "Network Architecture:\n",
        "\n",
        "In DQN, a neural network is used to approximate the Q-values for all possible actions given a state. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted.\n",
        "\n",
        "Experience Replay:\n",
        "\n",
        "To improve stability and efficiency, DQN uses experience replay, where the agent's experiences (state, action, reward, next state) are stored in a replay buffer. During training, random samples from this buffer are used to update the network, breaking the correlation between consecutive updates.\n",
        "\n",
        "Target Network:\n",
        "\n",
        "DQN also uses a target network, a copy of the Q-network that is updated less frequently. This helps in stabilizing the training by providing a more consistent target for the Q-value updates.\n",
        "\n",
        "Success Stories:\n",
        "\n",
        "DQN gained significant attention when it was used by DeepMind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs, showcasing the power of combining RL with deep learning.\n",
        "\n",
        "\n",
        "# conclusion\n",
        "\n",
        "Reinforcement learning represents a powerful approach to solving complex decision-making problems by enabling agents to learn through interaction with their environment.\n",
        "With applications ranging from gaming and robotics to healthcare and finance, RL continues to push the boundaries of what AI systems can achieve. Algorithms like Q-learning\n",
        "and Deep Q-Networks have laid the foundation for modern RL, and ongoing research promises to unlock even greater potential in the years to come.\n",
        ".\"\"\"\n",
        "\n",
        "# Split input text into chunks with overlap\n",
        "def chunk_text_with_overlap(text, max_chunk_size=400, overlap_size=15):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), max_chunk_size - overlap_size):\n",
        "        chunk = words[i:i + max_chunk_size]\n",
        "        chunks.append(' '.join(chunk))\n",
        "    return chunks\n",
        "\n",
        "# Split the input text into chunks with overlap\n",
        "text_chunks = chunk_text_with_overlap(input_text, max_chunk_size=100, overlap_size=15)\n",
        "\n",
        "# Helper function to remove redundant phrases\n",
        "def remove_redundant_phrases(text):\n",
        "    sentences = text.split(\". \")\n",
        "    seen = set()\n",
        "    unique_sentences = []\n",
        "    for sentence in sentences:\n",
        "        if sentence not in seen:\n",
        "            unique_sentences.append(sentence)\n",
        "            seen.add(sentence)\n",
        "    return \". \".join(unique_sentences)\n",
        "\n",
        "# Generate output for each chunk, adjusting max_length, and combine\n",
        "output_text = \"\"\n",
        "for chunk in text_chunks:\n",
        "    input_ids = tokenizer(chunk, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(input_ids['input_ids'], max_length=350, num_beams=5, do_sample=False)\n",
        "    chunk_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    output_text += chunk_output.strip() + \" \"\n",
        "\n",
        "# Post-process to clean up and format the output text\n",
        "output_text = remove_redundant_phrases(output_text)\n",
        "output_text = ' '.join(output_text.split())  # Removes extra whitespace\n",
        "\n",
        "# Function to calculate BLEU score\n",
        "def calculate_bleu(reference: str, generated: str) -> float:\n",
        "    reference_tokens = reference.split()\n",
        "    generated_tokens = generated.split()\n",
        "    return sentence_bleu([reference_tokens], generated_tokens)\n",
        "\n",
        "# Function to calculate ROUGE score\n",
        "def calculate_rouge(reference: str, generated: str) -> dict:\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    scores = scorer.score(reference, generated)\n",
        "    return {key: value.fmeasure for key, value in scores.items()}\n",
        "\n",
        "# Calculate and display accuracy metrics\n",
        "bleu_score = calculate_bleu(input_text, output_text)\n",
        "rouge_scores = calculate_rouge(input_text, output_text)\n",
        "\n",
        "print(\"Model Output:\\n\", output_text)\n",
        "print(f\"\\nBLEU Score: {bleu_score:.4f}\")\n",
        "print(f\"ROUGE Scores: {rouge_scores}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVbpkqVmxcMT",
        "outputId": "dffdb708-33d5-49f0-a940-ea4139b11212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Output:\n",
            " • State: The state represents the current situation or configuration of the environment. The set of possible actions is known as the action space. * •Reward. After taking an action. * •Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time. * • Policy: The policy is a strategy or mapping from states to actions. Applications of Reinforcement Learning. Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include: 1. • Gaming. RL is used to teach robots how to perform. 2. Robotics: In robotics, RL is used to teach robots how to perform tasks through trial and error, such as grasping objects, walking, or navigating environments. 1. Autonomous Vehicles: RL plays a significant role in the development of self-driving cars. These vehicles need to make real-time decisions based on their surroundings. Energy Management. RL is used in optimizing energy consumption in smart grids. reducing operational costs. Key Algorithms in Reinforcement Learning. Reinforcement Learning. Two of the most fundamental and widely used algorithms are Q-learning. Deep Q-Networks. (DQN). Update Rule: The Q-value for a state-action pair is updated using the following rule: [Q(s, a) Q(s, a) + [r + max_a' Q(s', a') Q(s, a)] Deep Q-Networks (DQN) uses deep neural networks to approximate the Q-function. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted. Experience Replay. To improve performance. Experience Replay: To improve stability and efficiency, DQN uses experience replay, where the agent's experiences (state, action, reward, next state) are stored in a replay buffer. random samples from this buffer are used to update the network. This helps in stabilizing the training by providing a more consistent target for the Q-value updates. Success Stories: DQN gained significant attention when it was used. Success Stories: DQN gained significant attention when it was used by DeepMind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs. showcasing the power of combining RL with deep learning. Reinforcement learning represents a powerful approach to solving complex decision-making problems. Algorithms like Q-learning and Deep Q-Networks have laid the foundation for modern RL. ongoing research promises to unlock even greater potential in the years to come.\n",
            "\n",
            "BLEU Score: 0.3908\n",
            "ROUGE Scores: {'rouge1': 0.7286956521739132, 'rouge2': 0.686411149825784, 'rougeL': 0.7078260869565216}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"./t5-text-to-text-models\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"./t5-text-to-text-models\")\n",
        "\n",
        "# Example input text\n",
        "input_text = \"\"\"\n",
        "# Principles of Reinforcement Learning\n",
        "* Agent and Environment: The agent is the learner or decision-maker, while the environment is the external system that the agent interacts with.\n",
        "* •\tState: The state represents the current situation or configuration of the environment. It encapsulates all the relevant information that the agent needs to make a decision.\n",
        "\n",
        "* •\tAction: An action is any decision or move the agent makes in response to the current state. The set of all possible actions is known as the action space.\n",
        "* •Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time.\n",
        "\n",
        "* •\tPolicy: The policy is a strategy or mapping from states to actions. It defines the agent's behavior at any given state, determining which action to take.\n",
        "* •\tValue Function: The value function estimates the expected cumulative reward that can be obtained from a particular state (or state-action pair). It helps the agent to evaluate which states or actions are more promising in the long run.\n",
        "  .\n",
        "\n",
        "# Applications of Reinforcement Learning\n",
        "Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include:\n",
        "\n",
        "\n",
        "1. •\tGaming: RL has been widely used in the development of game-playing AI, from classic board games like Go and Chess to complex video games like StarCraft and Dota 2.\n",
        "2. •\tRobotics: In robotics, RL is used to teach robots how to perform tasks through trial and error, such as grasping objects, walking, or navigating environments.\n",
        "\n",
        "1. •\tAutonomous Vehicles: RL plays a significant role in the development of self-driving cars. These vehicles need to make real-time decisions based on their surroundings.\n",
        "2. •\tEnergy Management: RL is used in optimizing energy consumption in smart grids, reducing operational costs in power plants, and improving the efficiency of renewable energy systems.\n",
        "\n",
        "1. •\tHealthcare: In healthcare, RL is used for personalized treatment planning, drug discovery, and optimizing medical procedures.\n",
        "\n",
        "# Key Algorithms in Reinforcement Learning\n",
        "\n",
        "Several algorithms have been developed to address different challenges in reinforcement learning. Two of the most fundamental and widely used algorithms are Q-learning and Deep Q-Networks (DQN).\n",
        "Update Rule:\n",
        "\n",
        "The Q-value for a state-action pair is updated using the following rule:\n",
        "\n",
        "\n",
        "\\[Q(s, a) ← Q(s, a) + α [r + γ max_a' Q(s', a') − Q(s, a)]\\\n",
        "\n",
        "Here, s and a are the current state and action, r is the reward received, s' is the next state, γ is the discount factor, and α is the learning rate.\n",
        "\n",
        "\n",
        "\n",
        "# Deep Q-Networks (DQN)\n",
        "\n",
        "DQN is an extension of Q-learning that uses deep neural networks to approximate the Q-function, making it scalable to environments with large or continuous state spaces.\n",
        "\n",
        "Network Architecture:\n",
        "\n",
        "In DQN, a neural network is used to approximate the Q-values for all possible actions given a state. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted.\n",
        "\n",
        "Experience Replay:\n",
        "\n",
        "To improve stability and efficiency, DQN uses experience replay, where the agent's experiences (state, action, reward, next state) are stored in a replay buffer. During training, random samples from this buffer are used to update the network, breaking the correlation between consecutive updates.\n",
        "\n",
        "Target Network:\n",
        "\n",
        "DQN also uses a target network, a copy of the Q-network that is updated less frequently. This helps in stabilizing the training by providing a more consistent target for the Q-value updates.\n",
        "\n",
        "Success Stories:\n",
        "\n",
        "DQN gained significant attention when it was used by DeepMind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs, showcasing the power of combining RL with deep learning.\n",
        "\n",
        "\n",
        "# conclusion\n",
        "\n",
        "Reinforcement learning represents a powerful approach to solving complex decision-making problems by enabling agents to learn through interaction with their environment.\n",
        "With applications ranging from gaming and robotics to healthcare and finance, RL continues to push the boundaries of what AI systems can achieve. Algorithms like Q-learning\n",
        "and Deep Q-Networks have laid the foundation for modern RL, and ongoing research promises to unlock even greater potential in the years to come.\n",
        "\"\"\"\n",
        "\n",
        "# Split input text into chunks with overlap\n",
        "def chunk_text_with_overlap(text, max_chunk_size=400, overlap_size=15):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), max_chunk_size - overlap_size):\n",
        "        chunk = words[i:i + max_chunk_size]\n",
        "        chunks.append(' '.join(chunk))\n",
        "    return chunks\n",
        "\n",
        "# Helper function to remove redundant phrases\n",
        "def remove_redundant_phrases(text):\n",
        "    sentences = text.split(\". \")\n",
        "    seen = set()\n",
        "    unique_sentences = []\n",
        "    for sentence in sentences:\n",
        "        if sentence not in seen:\n",
        "            unique_sentences.append(sentence)\n",
        "            seen.add(sentence)\n",
        "    return \". \".join(unique_sentences)\n",
        "\n",
        "# Parameters to experiment with\n",
        "num_beam_values = [1, 3, 5]  # 1 for greedy decoding, more for beam search\n",
        "do_sample_values = [True, False]\n",
        "temperature_values = [0.5, 0.7, 1.0]\n",
        "\n",
        "# Collect results\n",
        "results = []\n",
        "\n",
        "# Iterate over all combinations of parameters\n",
        "for num_beams in num_beam_values:\n",
        "    for do_sample in do_sample_values:\n",
        "        for temperature in temperature_values:\n",
        "            print(f\"Running with num_beams={num_beams}, do_sample={do_sample}, temperature={temperature}...\")\n",
        "\n",
        "            # Split the input text into chunks with overlap\n",
        "            text_chunks = chunk_text_with_overlap(input_text, max_chunk_size=100, overlap_size=15)\n",
        "\n",
        "            # Generate output for each chunk\n",
        "            output_text = \"\"\n",
        "            for chunk in text_chunks:\n",
        "                input_ids = tokenizer(chunk, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "                outputs = model.generate(\n",
        "                    input_ids['input_ids'],\n",
        "                    max_length=350,\n",
        "                    num_beams=num_beams,\n",
        "                    do_sample=do_sample,\n",
        "                    temperature=temperature\n",
        "                )\n",
        "                chunk_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                output_text += chunk_output.strip() + \" \"\n",
        "\n",
        "            # Post-process to clean up and format the output text\n",
        "            output_text = remove_redundant_phrases(output_text)\n",
        "            output_text = ' '.join(output_text.split())  # Removes extra whitespace\n",
        "\n",
        "            # Store the results\n",
        "            results.append((num_beams, do_sample, temperature, output_text))\n",
        "\n",
        "# Display results\n",
        "for (num_beams, do_sample, temperature, output_text) in results:\n",
        "    print(f\"Results for num_beams={num_beams}, do_sample={do_sample}, temperature={temperature}:\\n{output_text}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdroVOoezZVk",
        "outputId": "d43a7e4b-778e-4700-aa18-516d404552f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running with num_beams=1, do_sample=True, temperature=0.5...\n",
            "Running with num_beams=1, do_sample=True, temperature=0.7...\n",
            "Running with num_beams=1, do_sample=True, temperature=1.0...\n",
            "Running with num_beams=1, do_sample=False, temperature=0.5...\n",
            "Running with num_beams=1, do_sample=False, temperature=0.7...\n",
            "Running with num_beams=1, do_sample=False, temperature=1.0...\n",
            "Running with num_beams=3, do_sample=True, temperature=0.5...\n",
            "Running with num_beams=3, do_sample=True, temperature=0.7...\n",
            "Running with num_beams=3, do_sample=True, temperature=1.0...\n",
            "Running with num_beams=3, do_sample=False, temperature=0.5...\n",
            "Running with num_beams=3, do_sample=False, temperature=0.7...\n",
            "Running with num_beams=3, do_sample=False, temperature=1.0...\n",
            "Running with num_beams=5, do_sample=True, temperature=0.5...\n",
            "Running with num_beams=5, do_sample=True, temperature=0.7...\n",
            "Running with num_beams=5, do_sample=True, temperature=1.0...\n",
            "Running with num_beams=5, do_sample=False, temperature=0.5...\n",
            "Running with num_beams=5, do_sample=False, temperature=0.7...\n",
            "Running with num_beams=5, do_sample=False, temperature=1.0...\n",
            "Results for num_beams=1, do_sample=True, temperature=0.5:\n",
            "• State: The state represents the current situation or configuration of the environment. The set of possible actions is known as the action space. * •Reward. After taking an action. * •Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time. * • Policy: The policy is a strategy or mapping. Applications of Reinforcement Learning. Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include: 1. • Gaming. RL is used to teach robots how to perform. Lessons from Robotics: In robotics, RL is used to teach robots how to perform tasks through trial and error. Robotics: RL plays a significant role in the development of self-driving cars. Autonomous Vehicles. RL is used in optimizing energy consumption. smart grids. reducing operational costs. in power plants. and improving the efficiency of renewable energy systems. 1. Healthcare. RL is used for personalized treatment planning. drug discovery. and optimizing medical procedures. Key Algorithms in Reinforcement Learning. Reinforcement Learning. Two of the most fundamental and widely used algorithms are Q-learning. Deep Q-Networks. (DQN). Update Rule: The Q-value for a state-action pair is updated using the following rule: [Q(s, a) Q(s, a) + [r + max_a' Q(s', a) Q(s, a)] [ Deep Q-Networks (DQN) uses deep neural networks to approximate the Q-function, making it scalable to environments with large or continuous state spaces. Network Architecture: In DQN, a neural network is used to approximate the Q-values for all possible actions given a state. The network is trained using the Q-learning update rule. but instead of updating a table of Q-values, the network's weights are adjusted. Experience Replay. To improve performance. To improve stability and efficiency, DQN uses experience replay. Experience replay. The agent's experiences (state, action, reward. next state) are stored in a replay buffer. During training, random samples. from this buffer. Break the correlation between consecutive updates. DQN. Success Stories. RL gained significant attention when it was used to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs. # conclusion Reinforcement learning represents a powerful approach to solving complex decision-making problems. Algorithms like Q-learning and Deep Q-Networks have laid the foundation for modern RL. Modern RL. and ongoing research promises to unlock even greater potential in the years to come.\n",
            "\n",
            "Results for num_beams=1, do_sample=True, temperature=0.7:\n",
            "Les Principes of Reinforcement Learning. Reinforcement Learning. * Agent and Environment. The agent is the learner or decision-maker. The environment is the external system that the agent interacts. * • State. The state represents the current situation. The set of possible actions. action. An action. The set of all possible actions. * •Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time. * • Policy: The policy is a strategy or mapping. Applications of Reinforcement Learning. Reinforcement learning has found applications in various domains. Applications. Reinforcement learning. has found applications in various domains. The application is used to teach robots how to perform. Use of RL. In robotics. Robotics: RL is used to teach robots how to perform tasks through trial and error, such as grasping objects, walking, or navigating environments. Autonomous Vehicles: RL plays a significant role in the development of self-driving cars. These vehicles need to make real-time decisions based on their surroundings. 2. Energy Management: RL is used in optimizing energy consumption. Key Algorithms in Reinforcement Learning. Two of the most fundamental and widely used algorithms are Q-learning. Deep Q-Networks. (DQN.) Update Rule: The Q-value for a state-action pair is updated using the following rule: [Q(s, a) Q(s, a) + [r + max_a' Q(s', a') Q(s, a) Deep Q-Networks. (DQN) uses deep neural networks to approximate the Q-function. (DQN) is an extension of Q-learning. Q-learning. It uses deep neural networks to approximate the Q-values. Experience Replay, where the agent's experiences (state, action, reward, next state) are stored in a replay buffer. replay buffer breaks the correlation between consecutive updates. Target Network: DQN gained significant attention when it was used. Success Stories: DQN gained significant attention when it was used by DeepMind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs. showcasing the power of combining RL with deep learning. Algorithms like Q-learning and Deep Q-Networks have laid the foundation for modern RL. Further research promises to unlock even greater potential in the years to come.\n",
            "\n",
            "Results for num_beams=1, do_sample=True, temperature=1.0:\n",
            "Ledger.net. You can be a guest speaker about the changes. Tips. Programs. Reinforcement Learning. Human Resources. The Human Resources. Agent and Environment. The agent is the learner or decision-maker. The environment is the external system that the agent interacts with. * •Reward. After taking an action. * •The Policy: The policy defines the agent's behavior at any given state, determining which action to take. The value function estimates the expected cumulative reward that can be obtained from a particular state. The value function allows the agent to evaluate the expected cumulative reward. Applications of Reinforcement Learning. Reinforcement learning has found applications in various domains. Applications. Reinforcement learning. has found applications in diverse areas. particularly where decision-making has been crucial. 2. Robotics: RL helps robots learn how to perform tasks through trial and error. repulsion of computers and microprocessors. Health. RL is used in personalized treatment planning. drug discovery. and optimizing medical procedures. RL is used for personalized treatment planning, drug discovery, and optimizing medical procedures. Key Algorithms in Reinforcement Learning. Deep Q-Networks (DQN), a neural network is used to approximate the Q-values for all possible actions given a state. This application comes with deep neural networks. Deep Q-Networks. (DQN). DQN scalable to environments with large or continuous state spaces. Experience Replay. For improved stability and efficiency, DQN uses experience replay. Experience replay. where the agent's experiences are stored in a replay buffer. During training. random samples from this buffer. success stories: RL gained significant attention when it was used by Deepmind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs. # conclusion Reinforcement learning is a powerful approach to solving complex decision-making problems. Algorithms like Q-learning and Deep Q-Networks have laid the foundation for modern RL. In the past, research promises to unlock even more potential.\n",
            "\n",
            "Results for num_beams=1, do_sample=False, temperature=0.5:\n",
            "• State: The state represents the current situation or configuration of the environment. The set of possible actions is known as the action space. * •Reward. After taking an action. * •Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time. * • Policy: The policy is a strategy or mapping from states to actions. Applications of Reinforcement Learning. Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include: 1. • Gaming. RL is used to teach robots how to perform. RL is used to teach robots how to perform tasks through trial and error. Robotics: RL plays a significant role in the development of self-driving cars. Autonomous Vehicles. RL is used to optimize energy consumption. smart grids. reducing operational costs. in power plants. and improving the efficiency of renewable energy systems. 1. Healthcare. RL is used for personalized treatment planning. drug discovery. and optimizing medical procedures. Key Algorithms in Reinforcement Learning. Reinforcement Learning. Two of the most fundamental and widely used algorithms are Q-learning. Deep Q-Networks. (DQN). Update Rule: The Q-value for a state-action pair is updated using the following rule: [Q(s, a) Q(s, a) + [r + max_a' Q(s', a') Q(s, a)] Deep Q-Networks (DQN) uses deep neural networks to approximate the Q-function. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted. Experience Replay. To improve the performance of the network. To improve stability and efficiency, DQN uses experience replay. Experience replay. The agent's experiences (state, action, reward, next state) are stored in a replay buffer. During training. random samples. from this buffer. Success Stories: DQN gained significant attention when it was used by DeepMind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs. showcasing the power of combining RL with deep learning. Reinforcement learning represents a powerful approach to solving complex decision-making problems. Algorithms like Q-learning and Deep Q-Networks have laid the foundation for modern RL. Modern RL. and ongoing research promises to unlock even greater potential.\n",
            "\n",
            "Results for num_beams=1, do_sample=False, temperature=0.7:\n",
            "• State: The state represents the current situation or configuration of the environment. The set of possible actions is known as the action space. * •Reward. After taking an action. * •Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time. * • Policy: The policy is a strategy or mapping from states to actions. Applications of Reinforcement Learning. Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include: 1. • Gaming. RL is used to teach robots how to perform. RL is used to teach robots how to perform tasks through trial and error. Robotics: RL plays a significant role in the development of self-driving cars. Autonomous Vehicles. RL is used to optimize energy consumption. smart grids. reducing operational costs. in power plants. and improving the efficiency of renewable energy systems. 1. Healthcare. RL is used for personalized treatment planning. drug discovery. and optimizing medical procedures. Key Algorithms in Reinforcement Learning. Reinforcement Learning. Two of the most fundamental and widely used algorithms are Q-learning. Deep Q-Networks. (DQN). Update Rule: The Q-value for a state-action pair is updated using the following rule: [Q(s, a) Q(s, a) + [r + max_a' Q(s', a') Q(s, a)] Deep Q-Networks (DQN) uses deep neural networks to approximate the Q-function. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted. Experience Replay. To improve the performance of the network. To improve stability and efficiency, DQN uses experience replay. Experience replay. The agent's experiences (state, action, reward, next state) are stored in a replay buffer. During training. random samples. from this buffer. Success Stories: DQN gained significant attention when it was used by DeepMind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs. showcasing the power of combining RL with deep learning. Reinforcement learning represents a powerful approach to solving complex decision-making problems. Algorithms like Q-learning and Deep Q-Networks have laid the foundation for modern RL. Modern RL. and ongoing research promises to unlock even greater potential.\n",
            "\n",
            "Results for num_beams=1, do_sample=False, temperature=1.0:\n",
            "• State: The state represents the current situation or configuration of the environment. The set of possible actions is known as the action space. * •Reward. After taking an action. * •Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time. * • Policy: The policy is a strategy or mapping from states to actions. Applications of Reinforcement Learning. Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include: 1. • Gaming. RL is used to teach robots how to perform. RL is used to teach robots how to perform tasks through trial and error. Robotics: RL plays a significant role in the development of self-driving cars. Autonomous Vehicles. RL is used to optimize energy consumption. smart grids. reducing operational costs. in power plants. and improving the efficiency of renewable energy systems. 1. Healthcare. RL is used for personalized treatment planning. drug discovery. and optimizing medical procedures. Key Algorithms in Reinforcement Learning. Reinforcement Learning. Two of the most fundamental and widely used algorithms are Q-learning. Deep Q-Networks. (DQN). Update Rule: The Q-value for a state-action pair is updated using the following rule: [Q(s, a) Q(s, a) + [r + max_a' Q(s', a') Q(s, a)] Deep Q-Networks (DQN) uses deep neural networks to approximate the Q-function. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted. Experience Replay. To improve the performance of the network. To improve stability and efficiency, DQN uses experience replay. Experience replay. The agent's experiences (state, action, reward, next state) are stored in a replay buffer. During training. random samples. from this buffer. Success Stories: DQN gained significant attention when it was used by DeepMind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs. showcasing the power of combining RL with deep learning. Reinforcement learning represents a powerful approach to solving complex decision-making problems. Algorithms like Q-learning and Deep Q-Networks have laid the foundation for modern RL. Modern RL. and ongoing research promises to unlock even greater potential.\n",
            "\n",
            "Results for num_beams=3, do_sample=True, temperature=0.5:\n",
            "• State: The state represents the current situation or configuration of the environment. The set of possible actions is known as the action space. * •Reward. After taking an action. * •Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time. * • Policy: The policy is a strategy or mapping from states to actions. Applications of Reinforcement Learning. Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include: 1. • Gaming. RL is used to teach robots how to perform. 2. Robotics: In robotics, RL is used to teach robots how to perform tasks through trial and error, such as grasping objects, walking, or navigating environments. 1. Autonomous Vehicles: RL plays a significant role in the development of self-driving cars. These vehicles need to make real-time decisions based on their surroundings. Energy Management. RL is used in optimizing energy consumption in smart grids. reducing operational costs. Key Algorithms in Reinforcement Learning. Reinforcement Learning. Two of the most fundamental and widely used algorithms are Q-learning. Deep Q-Networks. (DQN). Update Rule: The Q-value for a state-action pair is updated using the following rule: [Q(s, a) Q(s, a) + [r + max_a' Q(s', a') Q(s, a)] Deep Q-Networks (DQN) uses deep neural networks to approximate the Q-function. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted. Experience Replay. To improve performance. Experience Replay helps in stabilizing the training by providing a more consistent target for the Q-value updates. Success Stories: DQN gained significant attention when it was used. Target Network: DQN uses a target network. a copy of the Q-network. Success Stories: DQN gained significant attention when it was used by DeepMind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs. showcasing the power of combining RL with deep learning. Reinforcement learning represents a powerful approach to solving complex decision-making problems. Algorithms like Q-learning and Deep Q-Networks have laid the foundation for modern RL. Research promises to unlock even greater potential in the years to come.\n",
            "\n",
            "Results for num_beams=3, do_sample=True, temperature=0.7:\n",
            "• State: The state represents the current situation or configuration of the environment. The set of possible actions is known as the action space. * •Reward. After taking an action. * •Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time. * • Policy: The policy is a strategy or mapping from states to actions. Applications of Reinforcement Learning. Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include: 1. • Gaming. RL is used to teach robots how to perform. 2. Robotics: In robotics, RL is used to teach robots how to perform tasks through trial and error, such as grasping objects, walking, or navigating environments. 1. Autonomous Vehicles: RL plays a significant role in the development of self-driving cars. These vehicles need to make real-time decisions based on their surroundings. Energy Management. RL is used in optimizing energy consumption in smart grids. reducing operational costs. Key Algorithms in Reinforcement Learning. Reinforcement Learning. Two of the most fundamental and widely used algorithms are Q-learning. Deep Q-Networks. (DQN). Update Rule: The Q-value for a state-action pair is updated using the following rule: [Q(s, a) Q(s, a) + [r + max_a' Q(s', a') Q(s, a)] Deep Q-Networks (DQN) uses deep neural networks to approximate the Q-function. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted. Experience Replay. To improve performance. Experience Replay helps in stabilizing the training by providing a more consistent target for the Q-value updates. Success Stories: DQN gained significant attention when it was used. Target Network: DQN uses a target network. a copy of the Q-network. Success Stories: DQN gained significant attention when it was used by DeepMind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs. showcasing the power of combining RL with deep learning. Reinforcement learning represents a powerful approach to solving complex decision-making problems. Algorithms like Q-learning and Deep Q-Networks have laid the foundation for modern RL. Research promises to unlock even greater potential in the years to come.\n",
            "\n",
            "Results for num_beams=3, do_sample=True, temperature=1.0:\n",
            "• State: The state represents the current situation or configuration of the environment. The set of possible actions is known as the action space. * •Reward. After taking an action. * •Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time. * • Policy: The policy is a strategy or mapping from states to actions. Applications of Reinforcement Learning. Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include: 1. • Gaming. RL is used to teach robots how to perform. 2. Robotics: In robotics, RL is used to teach robots how to perform tasks through trial and error. RL plays a significant role in the development of self-driving cars. Autonomous Vehicles. RL is used in optimizing energy consumption. smart grids. reducing operational costs. in power plants. and improving the efficiency of renewable energy systems. 1. Healthcare. In healthcare. RL is used for personalized treatment planning. drug discovery. and optimizing medical procedures. Key Algorithms in Reinforcement Learning. Reinforcement Learning. Two of the most fundamental and widely used algorithms are Q-learning. Deep Q-Networks. (DQN). Update Rule: The Q-value for a state-action pair is updated using the following rule: [Q(s, a) Q(s, a) + [r + max_a' Q(s', a') Q(s, a)] Deep Q-Networks (DQN) uses deep neural networks to approximate the Q-function. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted. Experience Replay. To improve performance. Experience Replay helps in stabilizing the training by providing a more consistent target for the Q-value updates. Success Stories: DQN gained significant attention when it was used. Target Network: DQN uses a target network. a copy of the Q-network. Success Stories: DQN gained significant attention when it was used by DeepMind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs. showcasing the power of combining RL with deep learning. Reinforcement learning represents a powerful approach to solving complex decision-making problems. Algorithms like Q-learning and Deep Q-Networks have laid the foundation for modern RL. Modern RL. and ongoing research promises to unlock even greater potential in the years to come.\n",
            "\n",
            "Results for num_beams=3, do_sample=False, temperature=0.5:\n",
            "• State: The state represents the current situation or configuration of the environment. The set of possible actions is known as the action space. * •Reward. After taking an action. * •Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time. * • Policy: The policy is a strategy or mapping from states to actions. Applications of Reinforcement Learning. Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include: 1. • Gaming. RL is used to teach robots how to perform. 2. Robotics: In robotics, RL is used to teach robots how to perform tasks through trial and error, such as grasping objects, walking, or navigating environments. 1. Autonomous Vehicles: RL plays a significant role in the development of self-driving cars. These vehicles need to make real-time decisions based on their surroundings. Energy Management. RL is used in optimizing energy consumption in smart grids. reducing operational costs. Key Algorithms in Reinforcement Learning. Reinforcement Learning. Two of the most fundamental and widely used algorithms are Q-learning. Deep Q-Networks. (DQN). Update Rule: The Q-value for a state-action pair is updated using the following rule: [Q(s, a) Q(s, a) + [r + max_a' Q(s', a') Q(s, a)] Deep Q-Networks (DQN) uses deep neural networks to approximate the Q-function. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted. Experience Replay. To improve performance. Experience Replay helps in stabilizing the training by providing a more consistent target for the Q-value updates. Success Stories: DQN gained significant attention when it was used. Target Network: DQN uses a target network. a copy of the Q-network. Success Stories: DQN gained significant attention when it was used by DeepMind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs. showcasing the power of combining RL with deep learning. Reinforcement learning represents a powerful approach to solving complex decision-making problems. Algorithms like Q-learning and Deep Q-Networks have laid the foundation for modern RL. Research promises to unlock even greater potential in the years to come.\n",
            "\n",
            "Results for num_beams=3, do_sample=False, temperature=0.7:\n",
            "• State: The state represents the current situation or configuration of the environment. The set of possible actions is known as the action space. * •Reward. After taking an action. * •Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time. * • Policy: The policy is a strategy or mapping from states to actions. Applications of Reinforcement Learning. Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include: 1. • Gaming. RL is used to teach robots how to perform. 2. Robotics: In robotics, RL is used to teach robots how to perform tasks through trial and error, such as grasping objects, walking, or navigating environments. 1. Autonomous Vehicles: RL plays a significant role in the development of self-driving cars. These vehicles need to make real-time decisions based on their surroundings. Energy Management. RL is used in optimizing energy consumption in smart grids. reducing operational costs. Key Algorithms in Reinforcement Learning. Reinforcement Learning. Two of the most fundamental and widely used algorithms are Q-learning. Deep Q-Networks. (DQN). Update Rule: The Q-value for a state-action pair is updated using the following rule: [Q(s, a) Q(s, a) + [r + max_a' Q(s', a') Q(s, a)] Deep Q-Networks (DQN) uses deep neural networks to approximate the Q-function. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted. Experience Replay. To improve performance. Experience Replay helps in stabilizing the training by providing a more consistent target for the Q-value updates. Success Stories: DQN gained significant attention when it was used. Target Network: DQN uses a target network. a copy of the Q-network. Success Stories: DQN gained significant attention when it was used by DeepMind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs. showcasing the power of combining RL with deep learning. Reinforcement learning represents a powerful approach to solving complex decision-making problems. Algorithms like Q-learning and Deep Q-Networks have laid the foundation for modern RL. Research promises to unlock even greater potential in the years to come.\n",
            "\n",
            "Results for num_beams=3, do_sample=False, temperature=1.0:\n",
            "• State: The state represents the current situation or configuration of the environment. The set of possible actions is known as the action space. * •Reward. After taking an action. * •Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time. * • Policy: The policy is a strategy or mapping from states to actions. Applications of Reinforcement Learning. Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include: 1. • Gaming. RL is used to teach robots how to perform. 2. Robotics: In robotics, RL is used to teach robots how to perform tasks through trial and error, such as grasping objects, walking, or navigating environments. 1. Autonomous Vehicles: RL plays a significant role in the development of self-driving cars. These vehicles need to make real-time decisions based on their surroundings. Energy Management. RL is used in optimizing energy consumption in smart grids. reducing operational costs. Key Algorithms in Reinforcement Learning. Reinforcement Learning. Two of the most fundamental and widely used algorithms are Q-learning. Deep Q-Networks. (DQN). Update Rule: The Q-value for a state-action pair is updated using the following rule: [Q(s, a) Q(s, a) + [r + max_a' Q(s', a') Q(s, a)] Deep Q-Networks (DQN) uses deep neural networks to approximate the Q-function. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted. Experience Replay. To improve performance. Experience Replay helps in stabilizing the training by providing a more consistent target for the Q-value updates. Success Stories: DQN gained significant attention when it was used. Target Network: DQN uses a target network. a copy of the Q-network. Success Stories: DQN gained significant attention when it was used by DeepMind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs. showcasing the power of combining RL with deep learning. Reinforcement learning represents a powerful approach to solving complex decision-making problems. Algorithms like Q-learning and Deep Q-Networks have laid the foundation for modern RL. Research promises to unlock even greater potential in the years to come.\n",
            "\n",
            "Results for num_beams=5, do_sample=True, temperature=0.5:\n",
            "• State: The state represents the current situation or configuration of the environment. The set of possible actions is known as the action space. * •Reward. After taking an action. * •Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time. * • Policy: The policy is a strategy or mapping from states to actions. Applications of Reinforcement Learning. Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include: 1. • Gaming. RL is used to teach robots how to perform. 2. Robotics: In robotics, RL is used to teach robots how to perform tasks through trial and error, such as grasping objects, walking, or navigating environments. 1. Autonomous Vehicles: RL plays a significant role in the development of self-driving cars. These vehicles need to make real-time decisions based on their surroundings. Energy Management. RL is used in optimizing energy consumption in smart grids. reducing operational costs. Key Algorithms in Reinforcement Learning. Reinforcement Learning. Two of the most fundamental and widely used algorithms are Q-learning. Deep Q-Networks. (DQN). Update Rule: The Q-value for a state-action pair is updated using the following rule: [Q(s, a) Q(s, a) + [r + max_a' Q(s', a') Q(s, a)] Deep Q-Networks (DQN) uses deep neural networks to approximate the Q-function. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted. Experience Replay. To improve performance. Experience Replay: To improve stability and efficiency, DQN uses experience replay, where the agent's experiences (state, action, reward, next state) are stored in a replay buffer. random samples from this buffer are used to update the network. This helps in stabilizing the training by providing a more consistent target for the Q-value updates. Success Stories: DQN gained significant attention when it was used. Success Stories: DQN gained significant attention when it was used by DeepMind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs. showcasing the power of combining RL with deep learning. Reinforcement learning represents a powerful approach to solving complex decision-making problems. Algorithms like Q-learning and Deep Q-Networks have laid the foundation for modern RL, and ongoing research promises to unlock even greater potential in the years to come.\n",
            "\n",
            "Results for num_beams=5, do_sample=True, temperature=0.7:\n",
            "• State: The state represents the current situation or configuration of the environment. The set of possible actions is known as the action space. * •Reward. After taking an action. * •Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time. * • Policy: The policy is a strategy or mapping from states to actions. Applications of Reinforcement Learning. Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include: 1. • Gaming. RL is used to teach robots how to perform. 2. Robotics: In robotics, RL is used to teach robots how to perform tasks through trial and error, such as grasping objects, walking, or navigating environments. 1. Autonomous Vehicles: RL plays a significant role in the development of self-driving cars. These vehicles need to make real-time decisions based on their surroundings. Energy Management. RL is used in optimizing energy consumption in smart grids. reducing operational costs. Key Algorithms in Reinforcement Learning. Reinforcement Learning. Two of the most fundamental and widely used algorithms are Q-learning. Deep Q-Networks. (DQN). Update Rule: The Q-value for a state-action pair is updated using the following rule: [Q(s, a) Q(s, a) + [r + max_a' Q(s', a') Q(s, a)] Deep Q-Networks (DQN) uses deep neural networks to approximate the Q-function. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted. Experience Replay. To improve performance. Experience Replay: To improve stability and efficiency, DQN uses experience replay, where the agent's experiences (state, action, reward, next state) are stored in a replay buffer. random samples from this buffer are used to update the network. This helps in stabilizing the training by providing a more consistent target for the Q-value updates. Success Stories: DQN gained significant attention when it was used. Success Stories: DQN gained significant attention when it was used by DeepMind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs. showcasing the power of combining RL with deep learning. Reinforcement learning represents a powerful approach to solving complex decision-making problems. Algorithms like Q-learning and Deep Q-Networks have laid the foundation for modern RL, and ongoing research promises to unlock even greater potential in the years to come.\n",
            "\n",
            "Results for num_beams=5, do_sample=True, temperature=1.0:\n",
            "• State: The state represents the current situation or configuration of the environment. The set of possible actions is known as the action space. * •Reward. After taking an action. * •Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time. * • Policy: The policy is a strategy or mapping from states to actions. Applications of Reinforcement Learning. Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include: 1. • Gaming. RL is used to teach robots how to perform. 2. Robotics: In robotics, RL is used to teach robots how to perform tasks through trial and error, such as grasping objects, walking, or navigating environments. 1. Autonomous Vehicles: RL plays a significant role in the development of self-driving cars. These vehicles need to make real-time decisions based on their surroundings. Energy Management. RL is used in optimizing energy consumption in smart grids. reducing operational costs. Key Algorithms in Reinforcement Learning. Reinforcement Learning. Two of the most fundamental and widely used algorithms are Q-learning. Deep Q-Networks. (DQN). Update Rule: The Q-value for a state-action pair is updated using the following rule: [Q(s, a) Q(s, a) + [r + max_a' Q(s', a') Q(s, a)] Deep Q-Networks (DQN) uses deep neural networks to approximate the Q-function. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted. Experience Replay. To improve learning. Experience Replay: To improve stability and efficiency, DQN uses experience replay, where the agent's experiences (state, action, reward, next state) are stored in a replay buffer. random samples from this buffer are used to update the network. This helps in stabilizing the training by providing a more consistent target for the Q-value updates. Success Stories: DQN gained significant attention when it was used. Success Stories: DQN gained significant attention when it was used by DeepMind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs. showcasing the power of combining RL with deep learning. Reinforcement learning represents a powerful approach to solving complex decision-making problems. Algorithms like Q-learning and Deep Q-Networks have laid the foundation for modern RL. Research promises to unlock even greater potential in the years to come.\n",
            "\n",
            "Results for num_beams=5, do_sample=False, temperature=0.5:\n",
            "• State: The state represents the current situation or configuration of the environment. The set of possible actions is known as the action space. * •Reward. After taking an action. * •Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time. * • Policy: The policy is a strategy or mapping from states to actions. Applications of Reinforcement Learning. Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include: 1. • Gaming. RL is used to teach robots how to perform. 2. Robotics: In robotics, RL is used to teach robots how to perform tasks through trial and error, such as grasping objects, walking, or navigating environments. 1. Autonomous Vehicles: RL plays a significant role in the development of self-driving cars. These vehicles need to make real-time decisions based on their surroundings. Energy Management. RL is used in optimizing energy consumption in smart grids. reducing operational costs. Key Algorithms in Reinforcement Learning. Reinforcement Learning. Two of the most fundamental and widely used algorithms are Q-learning. Deep Q-Networks. (DQN). Update Rule: The Q-value for a state-action pair is updated using the following rule: [Q(s, a) Q(s, a) + [r + max_a' Q(s', a') Q(s, a)] Deep Q-Networks (DQN) uses deep neural networks to approximate the Q-function. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted. Experience Replay. To improve performance. Experience Replay: To improve stability and efficiency, DQN uses experience replay, where the agent's experiences (state, action, reward, next state) are stored in a replay buffer. random samples from this buffer are used to update the network. This helps in stabilizing the training by providing a more consistent target for the Q-value updates. Success Stories: DQN gained significant attention when it was used. Success Stories: DQN gained significant attention when it was used by DeepMind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs. showcasing the power of combining RL with deep learning. Reinforcement learning represents a powerful approach to solving complex decision-making problems. Algorithms like Q-learning and Deep Q-Networks have laid the foundation for modern RL, and ongoing research promises to unlock even greater potential in the years to come.\n",
            "\n",
            "Results for num_beams=5, do_sample=False, temperature=0.7:\n",
            "• State: The state represents the current situation or configuration of the environment. The set of possible actions is known as the action space. * •Reward. After taking an action. * •Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time. * • Policy: The policy is a strategy or mapping from states to actions. Applications of Reinforcement Learning. Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include: 1. • Gaming. RL is used to teach robots how to perform. 2. Robotics: In robotics, RL is used to teach robots how to perform tasks through trial and error, such as grasping objects, walking, or navigating environments. 1. Autonomous Vehicles: RL plays a significant role in the development of self-driving cars. These vehicles need to make real-time decisions based on their surroundings. Energy Management. RL is used in optimizing energy consumption in smart grids. reducing operational costs. Key Algorithms in Reinforcement Learning. Reinforcement Learning. Two of the most fundamental and widely used algorithms are Q-learning. Deep Q-Networks. (DQN). Update Rule: The Q-value for a state-action pair is updated using the following rule: [Q(s, a) Q(s, a) + [r + max_a' Q(s', a') Q(s, a)] Deep Q-Networks (DQN) uses deep neural networks to approximate the Q-function. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted. Experience Replay. To improve performance. Experience Replay: To improve stability and efficiency, DQN uses experience replay, where the agent's experiences (state, action, reward, next state) are stored in a replay buffer. random samples from this buffer are used to update the network. This helps in stabilizing the training by providing a more consistent target for the Q-value updates. Success Stories: DQN gained significant attention when it was used. Success Stories: DQN gained significant attention when it was used by DeepMind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs. showcasing the power of combining RL with deep learning. Reinforcement learning represents a powerful approach to solving complex decision-making problems. Algorithms like Q-learning and Deep Q-Networks have laid the foundation for modern RL, and ongoing research promises to unlock even greater potential in the years to come.\n",
            "\n",
            "Results for num_beams=5, do_sample=False, temperature=1.0:\n",
            "• State: The state represents the current situation or configuration of the environment. The set of possible actions is known as the action space. * •Reward. After taking an action. * •Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time. * • Policy: The policy is a strategy or mapping from states to actions. Applications of Reinforcement Learning. Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include: 1. • Gaming. RL is used to teach robots how to perform. 2. Robotics: In robotics, RL is used to teach robots how to perform tasks through trial and error, such as grasping objects, walking, or navigating environments. 1. Autonomous Vehicles: RL plays a significant role in the development of self-driving cars. These vehicles need to make real-time decisions based on their surroundings. Energy Management. RL is used in optimizing energy consumption in smart grids. reducing operational costs. Key Algorithms in Reinforcement Learning. Reinforcement Learning. Two of the most fundamental and widely used algorithms are Q-learning. Deep Q-Networks. (DQN). Update Rule: The Q-value for a state-action pair is updated using the following rule: [Q(s, a) Q(s, a) + [r + max_a' Q(s', a') Q(s, a)] Deep Q-Networks (DQN) uses deep neural networks to approximate the Q-function. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted. Experience Replay. To improve performance. Experience Replay: To improve stability and efficiency, DQN uses experience replay, where the agent's experiences (state, action, reward, next state) are stored in a replay buffer. random samples from this buffer are used to update the network. This helps in stabilizing the training by providing a more consistent target for the Q-value updates. Success Stories: DQN gained significant attention when it was used. Success Stories: DQN gained significant attention when it was used by DeepMind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs. showcasing the power of combining RL with deep learning. Reinforcement learning represents a powerful approach to solving complex decision-making problems. Algorithms like Q-learning and Deep Q-Networks have laid the foundation for modern RL, and ongoing research promises to unlock even greater potential in the years to come.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"./t5-text-to-text-models\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"./t5-text-to-text-models\")\n",
        "\n",
        "# Example input text\n",
        "input_text = \"\"\"\n",
        "# Principles of Reinforcement Learning\n",
        "* Agent and Environment: The agent is the learner or decision-maker, while the environment is the external system that the agent interacts with.\n",
        "* State: The state represents the current situation or configuration of the environment. It encapsulates all the relevant information that the agent needs to make a decision.\n",
        "* Action: An action is any decision or move the agent makes in response to the current state. The set of all possible actions is known as the action space.\n",
        "* Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time.\n",
        "* Policy: The policy is a strategy or mapping from states to actions. It defines the agent's behavior at any given state, determining which action to take.\n",
        "* Value Function: The value function estimates the expected cumulative reward that can be obtained from a particular state (or state-action pair). It helps the agent to evaluate which states or actions are more promising in the long run.\n",
        "\n",
        "# Applications of Reinforcement Learning\n",
        "Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include:\n",
        "1. Gaming: RL has been widely used in the development of game-playing AI, from classic board games like Go and Chess to complex video games like StarCraft and Dota 2.\n",
        "2. Robotics: In robotics, RL is used to teach robots how to perform tasks through trial and error, such as grasping objects, walking, or navigating environments.\n",
        "3. Autonomous Vehicles: RL plays a significant role in the development of self-driving cars. These vehicles need to make real-time decisions based on their surroundings.\n",
        "4. Energy Management: RL is used in optimizing energy consumption in smart grids, reducing operational costs in power plants, and improving the efficiency of renewable energy systems.\n",
        "5. Healthcare: In healthcare, RL is used for personalized treatment planning, drug discovery, and optimizing medical procedures.\n",
        "\n",
        "# Key Algorithms in Reinforcement Learning\n",
        "Several algorithms have been developed to address different challenges in reinforcement learning. Two of the most fundamental and widely used algorithms are Q-learning and Deep Q-Networks (DQN).\n",
        "Update Rule:\n",
        "The Q-value for a state-action pair is updated using the following rule:\n",
        "\\[Q(s, a) ← Q(s, a) + α [r + γ max_a' Q(s', a') − Q(s, a)]\\\n",
        "\n",
        "# Deep Q-Networks (DQN)\n",
        "DQN is an extension of Q-learning that uses deep neural networks to approximate the Q-function, making it scalable to environments with large or continuous state spaces.\n",
        "Network Architecture:\n",
        "In DQN, a neural network is used to approximate the Q-values for all possible actions given a state. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted.\n",
        "Experience Replay:\n",
        "To improve stability and efficiency, DQN uses experience replay, where the agent's experiences (state, action, reward, next state) are stored in a replay buffer. During training, random samples from this buffer are used to update the network, breaking the correlation between consecutive updates.\n",
        "Target Network:\n",
        "DQN also uses a target network, a copy of the Q-network that is updated less frequently. This helps in stabilizing the training by providing a more consistent target for the Q-value updates.\n",
        "Success Stories:\n",
        "DQN gained significant attention when it was used by DeepMind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs, showcasing the power of combining RL with deep learning.\n",
        "\n",
        "# Conclusion\n",
        "Reinforcement learning represents a powerful approach to solving complex decision-making problems by enabling agents to learn through interaction with their environment.\n",
        "With applications ranging from gaming and robotics to healthcare and finance, RL continues to push the boundaries of what AI systems can achieve. Algorithms like Q-learning\n",
        "and Deep Q-Networks have laid the foundation for modern RL, and ongoing research promises to unlock even greater potential in the years to come.\n",
        "\"\"\"\n",
        "\n",
        "# Split input text into chunks with overlap\n",
        "def chunk_text_with_overlap(text, max_chunk_size=400, overlap_size=15):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), max_chunk_size - overlap_size):\n",
        "        chunk = words[i:i + max_chunk_size]\n",
        "        chunks.append(' '.join(chunk))\n",
        "    return chunks\n",
        "\n",
        "# Split the input text into chunks with overlap\n",
        "text_chunks = chunk_text_with_overlap(input_text, max_chunk_size=400, overlap_size=15)\n",
        "\n",
        "# Helper function to remove redundant phrases\n",
        "def remove_redundant_phrases(text):\n",
        "    sentences = text.split(\". \")\n",
        "    seen = set()\n",
        "    unique_sentences = []\n",
        "    for sentence in sentences:\n",
        "        if sentence.strip() and sentence not in seen:  # Ensure not to add empty strings\n",
        "            unique_sentences.append(sentence.strip())\n",
        "            seen.add(sentence.strip())\n",
        "    return \". \".join(unique_sentences)\n",
        "\n",
        "# Generate output for each chunk, adjusting max_length, and combine\n",
        "output_text = \"\"\n",
        "for chunk in text_chunks:\n",
        "    input_ids = tokenizer(chunk, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(input_ids['input_ids'],\n",
        "                             max_length=350,\n",
        "                             num_beams=5,\n",
        "                             early_stopping=True,\n",
        "                             do_sample=False,\n",
        "                             temperature=0.7)  # Adjusted temperature\n",
        "    chunk_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    output_text += chunk_output.strip() + \" \"\n",
        "\n",
        "# Post-process to clean up and format the output text\n",
        "output_text = remove_redundant_phrases(output_text)\n",
        "output_text = ' '.join(output_text.split())  # Removes extra whitespace\n",
        "\n",
        "# Display final output\n",
        "print(\"Model Output:\\n\", output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bdi72yyPtZu2",
        "outputId": "190983b2-697c-42a0-9ac6-752d4bfbc3f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Output:\n",
            " Applications of Reinforcement Learning. Reinforcement learning is a strategy or mapping from states to actions. The value function estimates the expected cumulative reward. The reward is a numerical value that indicates the immediate benefit of the action. Key Algorithms. in Reinforcement Learning. Reinforcement learning. Deep Q-Networks (DQN) uses deep neural networks to approximate the Q-values. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted. Experience Replay: To improve stability and efficiency, DQN uses a target network\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"./t5-text-to-text-models\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"./t5-text-to-text-models\")\n",
        "\n",
        "# Example input text\n",
        "input_text = \"\"\"\n",
        "# Principles of Reinforcement Learning\n",
        "* Agent and Environment: The agent is the learner or decision-maker, while the environment is the external system that the agent interacts with.\n",
        "* State: The state represents the current situation or configuration of the environment. It encapsulates all the relevant information that the agent needs to make a decision.\n",
        "* Action: An action is any decision or move the agent makes in response to the current state. The set of all possible actions is known as the action space.\n",
        "* Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time.\n",
        "* Policy: The policy is a strategy or mapping from states to actions. It defines the agent's behavior at any given state, determining which action to take.\n",
        "* Value Function: The value function estimates the expected cumulative reward that can be obtained from a particular state (or state-action pair). It helps the agent to evaluate which states or actions are more promising in the long run.\n",
        "\n",
        "# Applications of Reinforcement Learning\n",
        "Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include:\n",
        "1. Gaming: RL has been widely used in the development of game-playing AI, from classic board games like Go and Chess to complex video games like StarCraft and Dota 2.\n",
        "2. Robotics: In robotics, RL is used to teach robots how to perform tasks through trial and error, such as grasping objects, walking, or navigating environments.\n",
        "3. Autonomous Vehicles: RL plays a significant role in the development of self-driving cars. These vehicles need to make real-time decisions based on their surroundings.\n",
        "4. Energy Management: RL is used in optimizing energy consumption in smart grids, reducing operational costs in power plants, and improving the efficiency of renewable energy systems.\n",
        "5. Healthcare: In healthcare, RL is used for personalized treatment planning, drug discovery, and optimizing medical procedures.\n",
        "\n",
        "# Key Algorithms in Reinforcement Learning\n",
        "Several algorithms have been developed to address different challenges in reinforcement learning. Two of the most fundamental and widely used algorithms are Q-learning and Deep Q-Networks (DQN).\n",
        "Update Rule:\n",
        "The Q-value for a state-action pair is updated using the following rule:\n",
        "\\[Q(s, a) ← Q(s, a) + α [r + γ max_a' Q(s', a') − Q(s, a)]\\\n",
        "\n",
        "# Deep Q-Networks (DQN)\n",
        "DQN is an extension of Q-learning that uses deep neural networks to approximate the Q-function, making it scalable to environments with large or continuous state spaces.\n",
        "Network Architecture:\n",
        "In DQN, a neural network is used to approximate the Q-values for all possible actions given a state. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted.\n",
        "Experience Replay:\n",
        "To improve stability and efficiency, DQN uses experience replay, where the agent's experiences (state, action, reward, next state) are stored in a replay buffer. During training, random samples from this buffer are used to update the network, breaking the correlation between consecutive updates.\n",
        "Target Network:\n",
        "DQN also uses a target network, a copy of the Q-network that is updated less frequently. This helps in stabilizing the training by providing a more consistent target for the Q-value updates.\n",
        "Success Stories:\n",
        "DQN gained significant attention when it was used by DeepMind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs, showcasing the power of combining RL with deep learning.\n",
        "\n",
        "# Conclusion\n",
        "Reinforcement learning represents a powerful approach to solving complex decision-making problems by enabling agents to learn through interaction with their environment.\n",
        "With applications ranging from gaming and robotics to healthcare and finance, RL continues to push the boundaries of what AI systems can achieve. Algorithms like Q-learning\n",
        "and Deep Q-Networks have laid the foundation for modern RL, and ongoing research promises to unlock even greater potential in the years to come.\n",
        "\"\"\"\n",
        "\n",
        "# Split input text into chunks with overlap\n",
        "def chunk_text_with_overlap(text, max_chunk_size=400, overlap_size=15):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), max_chunk_size - overlap_size):\n",
        "        chunk = words[i:i + max_chunk_size]\n",
        "        chunks.append(' '.join(chunk))\n",
        "    return chunks\n",
        "\n",
        "# Split the input text into chunks with overlap\n",
        "text_chunks = chunk_text_with_overlap(input_text, max_chunk_size=400, overlap_size=15)\n",
        "\n",
        "# Helper function to remove redundant phrases\n",
        "def remove_redundant_phrases(text):\n",
        "    sentences = text.split(\". \")\n",
        "    seen = set()\n",
        "    unique_sentences = []\n",
        "    for sentence in sentences:\n",
        "        if sentence.strip() and sentence not in seen:  # Ensure not to add empty strings\n",
        "            unique_sentences.append(sentence.strip())\n",
        "            seen.add(sentence.strip())\n",
        "    return \". \".join(unique_sentences)\n",
        "\n",
        "# Generate output for each chunk, adjusting max_length, and combine\n",
        "output_text = \"\"\n",
        "for chunk in text_chunks:\n",
        "    input_ids = tokenizer(chunk, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(input_ids['input_ids'],\n",
        "                             max_length=400,  # Increased max_length for output\n",
        "                             num_beams=5,\n",
        "                             num_return_sequences=3,  # Generate multiple sequences\n",
        "                             do_sample=True,  # Enable sampling\n",
        "                             temperature=0.7,\n",
        "                             early_stopping=True)\n",
        "\n",
        "    # Decode outputs and take the best one\n",
        "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "    best_output = max(generated_texts, key=lambda x: len(x.split()))  # Select the longest output\n",
        "    output_text += best_output.strip() + \" \"\n",
        "\n",
        "# Post-process to clean up and format the output text\n",
        "output_text = remove_redundant_phrases(output_text)\n",
        "output_text = ' '.join(output_text.split())  # Removes extra whitespace\n",
        "\n",
        "# Display final output\n",
        "print(\"Model Output:\\n\", output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo7J1YMAuK40",
        "outputId": "6c6c7acd-e623-4874-ae67-b2ee9186b02c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Output:\n",
            " Applications of Reinforcement Learning. Reinforcement learning is a strategy or mapping from states to actions. The value function estimates the expected cumulative reward. The reward is a numerical value that indicates the immediate benefit of the action. Key Algorithms. in Reinforcement Learning. Reinforcement learning. Deep Q-Networks (DQN) uses deep neural networks to approximate the Q-values. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted. Experience Replay: To improve stability and efficiency, DQN uses a target network\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import textwrap\n",
        "import logging\n",
        "from typing import List, Tuple\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def chunk_text_with_overlap(text: str, max_chunk_size: int = 400, overlap_size: int = 15) -> List[str]:\n",
        "    \"\"\"Split text into chunks with overlap.\"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), max_chunk_size - overlap_size):\n",
        "        chunk = words[i:i + max_chunk_size]\n",
        "        chunks.append(' '.join(chunk))\n",
        "    return chunks\n",
        "\n",
        "def remove_redundant_phrases(text: str) -> str:\n",
        "    \"\"\"Remove duplicate sentences and clean up text.\"\"\"\n",
        "    sentences = text.split(\". \")\n",
        "    seen = set()\n",
        "    unique_sentences = []\n",
        "    for sentence in sentences:\n",
        "        cleaned = sentence.strip()\n",
        "        if cleaned and cleaned not in seen:\n",
        "            unique_sentences.append(cleaned)\n",
        "            seen.add(cleaned)\n",
        "    return \". \".join(unique_sentences)\n",
        "\n",
        "def process_text(input_text: str, model_name: str = \"t5-base\",\n",
        "                 max_length: int = 400, chunk_size: int = 400,\n",
        "                 overlap_size: int = 15, num_return_sequences: int = 3) -> str:\n",
        "    \"\"\"Process input text through a T5 model.\"\"\"\n",
        "    try:\n",
        "        # Load model and tokenizer\n",
        "        logger.info(f\"Loading model: {model_name}\")\n",
        "        tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "        model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "        # Split text into manageable chunks\n",
        "        text_chunks = chunk_text_with_overlap(input_text, max_chunk_size=chunk_size, overlap_size=overlap_size)\n",
        "        logger.info(f\"Text split into {len(text_chunks)} chunks.\")\n",
        "\n",
        "        # Process each chunk\n",
        "        output_text = \"\"\n",
        "        for i, chunk in enumerate(text_chunks):\n",
        "            logger.info(f\"Processing chunk {i + 1}/{len(text_chunks)}\")\n",
        "            # Prepare input\n",
        "            inputs = tokenizer(chunk, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "            # Generate output\n",
        "            outputs = model.generate(\n",
        "                inputs['input_ids'],\n",
        "                max_length=max_length,\n",
        "                num_beams=5,\n",
        "                num_return_sequences=num_return_sequences,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "            # Decode and select best output\n",
        "            generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "            best_output = max(generated_texts, key=lambda x: len(x.split()))\n",
        "            output_text += best_output.strip() + \" \"\n",
        "\n",
        "        # Clean up final output\n",
        "        final_output = remove_redundant_phrases(output_text)\n",
        "\n",
        "        # Format output with proper line breaks\n",
        "        formatted_output = textwrap.fill(final_output, width=80)\n",
        "\n",
        "        return formatted_output\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error occurred: {str(e)}\")\n",
        "        return f\"Error occurred: {str(e)}\"\n",
        "\n",
        "# Input text (placeholder)\n",
        "input_text = \"\"\"\n",
        "# Principles of Reinforcement Learning\n",
        "* Agent and Environment: The agent is the learner or decision-maker, while the environment is the external system that the agent interacts with.\n",
        "* State: The state represents the current situation or configuration of the environment.\n",
        "* Action: An action is any decision or move the agent makes in response to the current state.\n",
        "* Reward: The reward is a numerical value that indicates the immediate benefit of the action.\n",
        "\"\"\"\n",
        "\n",
        "# Process and print output\n",
        "result = process_text(input_text)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229,
          "referenced_widgets": [
            "78085c31a83048a4921e9b5c3ec24944",
            "d3dd4c68b7ef449b966f18ac3fa66ea9",
            "bb1551dc2ccd4d099c18bd5cb1a66c6e",
            "5f0b24fc57b14089b1f1e21cd826c41b",
            "281ee20d4a454f90965f4ce7987737d0",
            "5a99fe19a64142ca8470f57b9f275805",
            "fabbf68ab67e445faa9ab9db2de8ca43",
            "f6912e0ac7f34aa8bf86a88755a39657",
            "daa262d542d742e0b73981b9b65499ed",
            "19f483a47f6d48d4b37f4940a2ca099b",
            "7972d8da99974d9584897d7a4242a4f0",
            "120a5391ac734214a9a6daac1fc3c176",
            "3059e99b914b4ee18eaadb2813901f38",
            "66c5f41624b9407b91d12461085de80c",
            "d82fda7ab884493fa8db5404f6c3616e",
            "c1f193a963c949cc952638a6a7a1bd2a",
            "608ff6b0d2e54b608ae8ec4c31b50e45",
            "261fcf6034f84b838096c9c56933c03e",
            "fb880be30710473f9ba6b3da5573759b",
            "091786de70b44567b637c6a253255d3e",
            "b2692dcb2da049329eb8f9fc44cab002",
            "c9fceff872b04bba8e6871d3e2df3c23",
            "664a28fe55b14a5a9ce87d78e1e0e975",
            "700a89df2c9c48a8abb4b8af68eb9753",
            "5f4e7c02c0d84e7d84ab7aea8ddf0c65",
            "0daa9a02c8c541fdae8f8fefa82b6236",
            "35dbeef556644c62a8f9fffe585350c6",
            "3605eb2aaf5f45f8b9cff2f6ad721913",
            "77c0827783b448328eea98a70bc48c1b",
            "abf6b7a2297141b783906a9144f79556",
            "fdc5785ec53f485c8836aaae0c08afec",
            "d103a146b3ed407196598133dcc94728",
            "d1dd141d7c5e45ad8efa1683fd68d449",
            "4db376fb82b4464db2124cf721331d75",
            "970b50291e6048e082430f38bdc78ba6",
            "bb819daea1374a28924d63b57c34d6ba",
            "c480612175b0400a9b4ee12f5dd59d4f",
            "7e5a8b7441854627bc8673e64d35a227",
            "e3fbbeac6a234e92bd101708f78d5947",
            "b932fdbef0c0452280321e20aa3b4088",
            "bb6c3022107445aaad2bf67a29281da8",
            "3dcd12b0b05f487490697d77a185cf9d",
            "c771e1a7bb6c4d72a886cd26127d92d7",
            "c623554889f44d5aa6861a67e2dc3bc8",
            "07a072b8dfa646d29bdde0d10d0bbc29",
            "136e9cf52693436e8bca0861880f2f1d",
            "8d7e3a469e4f4e3ab81723b869a15de2",
            "78436bdd9a244e41bd6d6dce32e86bf7",
            "ed5189f38f874dc68276fe829e803fb0",
            "c9771ab5a34e4a04b05b7c96466c5ee5",
            "fc603cc1cb7247dfba032009f2171fb3",
            "395c986e8424464db583c8a5d759716c",
            "cc3745357f1b4e2da1a3df7e2b1dc6a8",
            "783eb1a7aa4c495b8d24d76fb167b834",
            "27d74f3875b845918374fcdd9e5aef75"
          ]
        },
        "id": "q5_AmK8WvGen",
        "outputId": "b7a76961-74e5-41a5-9813-ca236c6a525b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78085c31a83048a4921e9b5c3ec24944"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "120a5391ac734214a9a6daac1fc3c176"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "664a28fe55b14a5a9ce87d78e1e0e975"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4db376fb82b4464db2124cf721331d75"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07a072b8dfa646d29bdde0d10d0bbc29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* Agent and Environment: The agent is the learner or decision-maker, while the\n",
            "environment is the external system that the agent interacts with. * State: The\n",
            "state represents the current state or configuration of the environment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"./t5-text-to-text-models\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"./t5-text-to-text-models\")\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "# Example input text\n",
        "input_text = \"\"\"\n",
        "# Introduction to Optimization Theory\n",
        "\n",
        "Optimization Theory is a topic that involves deep mathematical concepts.\n",
        "\n",
        "\\[ \\frac{\\partial u}{\\partial x} = \\frac{\\partial v}{\\partial y}, \\quad \\frac{\\partial u}{\\partial y} = -\\frac{\\partial v}{\\partial x} \\]\n",
        "\n",
        "## Applications\n",
        "\n",
        "- Cryptography\n",
        "- Polynomial Algebra\n",
        "- Number Theory\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the input text without chunking\n",
        "input_ids = tokenizer(\n",
        "    input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\"\n",
        ")\n",
        "\n",
        "# Generate output\n",
        "outputs = model.generate(\n",
        "    input_ids['input_ids'],\n",
        "    max_length=300,\n",
        "    num_beams=5,\n",
        "    do_sample=True,       # Enable sampling for more diverse outputs\n",
        "    temperature=0.7       # Add randomness to the generation\n",
        ")\n",
        "\n",
        "# Decode the generated output, setting `clean_up_tokenization_spaces` here\n",
        "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "\n",
        "# Display the final output\n",
        "print(\"Model Output:\\n\", output_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3-ZsSRki3NN",
        "outputId": "e59cb986-0e3c-4e3f-9e2b-44da99225447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Output:\n",
            " Introduction to Optimization Theory. Optimization Theory is a topic that involves deep mathematical concepts. Applications. - Cryptography. - Polynomial Algebra. - Number Theory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"./t5-text-to-text-models\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"./t5-text-to-text-models\")\n",
        "\n",
        "# Example input text\n",
        "input_text = \"\"\"\n",
        "# Introduction to Probability Spaces\n",
        "\n",
        "Probability Spaces is a topic that involves deep mathematical concepts.\n",
        "\n",
        "\\[ \\mu(\\cup A_i) = \\sum \\mu(A_i) \\]\n",
        "\"\"\"\n",
        "\n",
        "# Split input text into chunks if too long\n",
        "def chunk_text(text, max_chunk_size=400):\n",
        "    words = text.split()\n",
        "    return [' '.join(words[i:i + max_chunk_size]) for i in range(0, len(words), max_chunk_size)]\n",
        "\n",
        "# Split the input text into chunks\n",
        "text_chunks = chunk_text(input_text, max_chunk_size=100)\n",
        "\n",
        "# Generate output for each chunk and combine\n",
        "output_text = \"\"\n",
        "for chunk in text_chunks:\n",
        "    input_ids = tokenizer(chunk, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(input_ids['input_ids'], max_length=300, num_beams=5, do_sample=False)\n",
        "    output_text += tokenizer.decode(outputs[0], skip_special_tokens=True) + \" \"\n",
        "\n",
        "# Display final output\n",
        "print(\"Model Output:\\n\", output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwoNz-XofcZd",
        "outputId": "e2a00d86-b91b-4d44-c6cc-19e939fa2517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Output:\n",
            " Introduction to Probability Spaces. Probability Spaces is a topic that involves deep mathematical concepts. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load the fine-tuned tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"./t5-text-to-text-models\")  # Replace with your fine-tuned model path\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"./t5-text-to-text-models\")  # Replace with your fine-tuned model path\n",
        "\n",
        "# Sample input text (preprocessed and split into smaller sections for generation)\n",
        "input_text = \"\"\"\n",
        "Introduction to Quantum Field Theory\n",
        "\n",
        "Quantum field theory is the framework for constructing quantum mechanical models of subatomic particles in particle physics and quantum chemistry. It combines classical field theory, special relativity, and quantum mechanics. The creation and annihilation of particles are described by operators in quantum field theory. Mathematically, these operators act on states defined in a Hilbert space and follow commutation relations:\n",
        "\n",
        "\\[ [\\hat{\\phi}(x), \\hat{\\pi}(y)] = i\\delta^{(3)}(x - y) \\]\n",
        "\"\"\"\n",
        "\n",
        "# Define a function to split large text into smaller chunks\n",
        "def chunk_text(text, chunk_size=80):\n",
        "    words = text.split()\n",
        "    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "    return chunks\n",
        "\n",
        "# Split the input text into chunks\n",
        "text_chunks = chunk_text(input_text, chunk_size=80)\n",
        "\n",
        "# Generate output for each chunk and concatenate results\n",
        "output_text = \"\"\n",
        "for chunk in text_chunks:\n",
        "    input_ids = tokenizer(chunk, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(input_ids['input_ids'], max_length=100, num_beams=3, do_sample=True)\n",
        "    output_text += tokenizer.decode(outputs[0], skip_special_tokens=True) + \" \"\n",
        "\n",
        "# Print final concatenated output\n",
        "print(\"Final Model Output:\\n\", output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDADXnWqz8AZ",
        "outputId": "46881c03-8494-4cc5-bfa1-e0e13b6489a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Model Output:\n",
            " Introduction to Quantum Field Theory. Quantum field theory combines classical field theory, special relativity, and quantum mechanics. The creation and annihilation of particles are described by operators in quantum field theory. Mathematically, these operators act on states defined in a Hilbert space. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RNIZ5g4bTxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ced19b8e-6524-4afc-bc3e-27bb39a59103"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gtts\n",
            "  Downloading gTTS-2.5.3-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gtts) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gtts) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gtts) (2024.8.30)\n",
            "Downloading gTTS-2.5.3-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: gtts\n",
            "Successfully installed gtts-2.5.3\n"
          ]
        }
      ],
      "source": [
        "pip install gtts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6cioFbt1GUx",
        "outputId": "08c18d3f-d3b7-4f70-c61b-4d90ba29f368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text Output:\n",
            " Introduction to Group Theory. Group Theory is a topic that involves deep mathematical concepts. Applications. - Quantum Mechanics. - Particle Physics. Quantum Electrodynamics. \n",
            "Audio saved as output_audio.mp3\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from gtts import gTTS\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"./t5-text-to-text-models\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"./t5-text-to-text-models\")\n",
        "\n",
        "# Input text\n",
        "input_text = \"\"\"\n",
        "# Introduction to Group Theory\n",
        "\n",
        "Group Theory is a topic that involves deep mathematical concepts.\n",
        "\n",
        "\\\\[ T = T^i_j \\\\partial_i dx^j \\\\]\n",
        "\n",
        "## Applications\n",
        "\n",
        "- Quantum Mechanics\n",
        "- Particle Physics\n",
        "- Quantum Electrodynamics\n",
        "\"\"\"\n",
        "\n",
        "# Split input text into chunks if too long\n",
        "def chunk_text(text, max_chunk_size=400):\n",
        "    words = text.split()\n",
        "    return [' '.join(words[i:i + max_chunk_size]) for i in range(0, len(words), max_chunk_size)]\n",
        "\n",
        "# Split the input text into chunks\n",
        "text_chunks = chunk_text(input_text, max_chunk_size=100)\n",
        "\n",
        "# Generate output for each chunk and combine\n",
        "output_text = \"\"\n",
        "for chunk in text_chunks:\n",
        "    input_ids = tokenizer(chunk, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(input_ids['input_ids'], max_length=300, num_beams=5, do_sample=False)\n",
        "    output_text += tokenizer.decode(outputs[0], skip_special_tokens=True) + \" \"\n",
        "\n",
        "# Display final output\n",
        "print(\"Generated Text Output:\\n\", output_text)\n",
        "\n",
        "# Convert the generated text to audio using gTTS\n",
        "tts = gTTS(output_text, lang='en')\n",
        "tts.save(\"output_audio.mp3\")\n",
        "print(\"Audio saved as output_audio.mp3\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1neYFdmUm38",
        "outputId": "e2a68c15-7ee9-4715-b7b3-799783b46e74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gTTS\n",
            "  Downloading gTTS-2.5.3-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gTTS) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gTTS) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (2024.8.30)\n",
            "Downloading gTTS-2.5.3-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: gTTS\n",
            "Successfully installed gTTS-2.5.3\n"
          ]
        }
      ],
      "source": [
        "pip install gTTS\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bert_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZuZjpPlOVwH",
        "outputId": "6ccad87b-5b1a-4d5a-c083-8029c4fa9fd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bert_score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.5.0+cu121)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.2.2)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.44.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.66.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (24.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2024.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.24.7)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.19.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
            "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bert_score\n",
            "Successfully installed bert_score-0.3.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from bert_score import score\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"./t5-text-to-text-models\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"./t5-text-to-text-models\")\n",
        "\n",
        "def generate_and_evaluate(input_text):\n",
        "    # Generate output text from the model\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(input_ids['input_ids'], max_length=150, num_beams=5, do_sample=True)\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Display the generated text\n",
        "    print(\"Predicted Text:\\n\", generated_text)\n",
        "\n",
        "    # Calculate BERTScore similarity between input and generated text\n",
        "    P, R, F1 = score([generated_text], [input_text], lang=\"en\", model_type=\"bert-base-uncased\")\n",
        "\n",
        "    # Display BERTScore results\n",
        "    print(\"\\nBERTScore Results (Similarity between Input and Output):\")\n",
        "    print(f\"Precision: {P.mean().item():.4f}\")\n",
        "    print(f\"Recall: {R.mean().item():.4f}\")\n",
        "    print(f\"F1 Score: {F1.mean().item():.4f}\")\n",
        "\n",
        "# Example usage\n",
        "input_text = \"\"\"# Introduction to Group Theory\n",
        "\n",
        "Group Theory is a topic that involves deep mathematical concepts.\n",
        "\n",
        "\\\\[ T = T^i_j \\\\partial_i dx^j \\\\]\n",
        "\n",
        "## Applications\n",
        "\n",
        "- Quantum Mechanics\n",
        "- Particle Physics\n",
        "- Quantum Electrodynamics\"\"\"\n",
        "\n",
        "generate_and_evaluate(input_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EtkQKScQqlC",
        "outputId": "be76f732-7207-4b08-b801-ce5d463f4c0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Text:\n",
            " Introduction to Group Theory. Group Theory is a topic that involves deep mathematical concepts. Applications. - Quantum Mechanics. - Particle Physics. Quantum Electrodynamics.\n",
            "\n",
            "BERTScore Results (Similarity between Input and Output):\n",
            "Precision: 0.8374\n",
            "Recall: 0.6201\n",
            "F1 Score: 0.7125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from bert_score import score\n",
        "import re\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"./t5-text-to-text-models\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"./t5-text-to-text-models\")\n",
        "\n",
        "# Define the cleaning function for input text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\\\[a-zA-Z]+', ' the formula is given for reference ', text)  # LaTeX commands\n",
        "    text = re.sub(r'\\$\\$[^\\$]*\\$\\$', ' the formula is given for reference ', text)  # Double dollar signs\n",
        "    text = re.sub(r'\\$[^\\$]*\\$', ' the formula is given for reference ', text)  # Single dollar signs\n",
        "    text = re.sub(r'\\\\\\([^\\)]+\\\\\\)', ' the formula is given for reference ', text)  # \\( ... \\) notation\n",
        "    text = re.sub(r'\\\\\\[[^\\]]+\\\\\\]', ' the formula is given for reference ', text)  # \\[ ... \\] notation\n",
        "    text = re.sub(r'\\([^)]*\\)', '', text)  # Parentheses\n",
        "    text = re.sub(r'\\[[^\\]]*\\]', '', text)  # Square brackets\n",
        "    text = re.sub(r'\\{[^}]*\\}', '', text)  # Curly braces\n",
        "    text = re.sub(r'_[a-zA-Z0-9]+', '', text)  # Subscripts\n",
        "    text = re.sub(r'\\^[a-zA-Z0-9]+', '', text)  # Superscripts\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s.,;:\\'-]', '', text)  # Retain only standard punctuation\n",
        "    text = re.sub(r'[-•]\\s*', '', text)  # Remove bullet points\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Normalize whitespace\n",
        "    return text\n",
        "\n",
        "def generate_and_evaluate(input_text):\n",
        "    # Clean the input text\n",
        "    cleaned_input = clean_text(input_text)\n",
        "    print(\"Cleaned Input Text:\\n\", cleaned_input)\n",
        "\n",
        "    # Generate output text from the model\n",
        "    input_ids = tokenizer(cleaned_input, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(input_ids['input_ids'], max_length=150, num_beams=5, do_sample=True)\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Display the generated text\n",
        "    print(\"\\nPredicted Text:\\n\", generated_text)\n",
        "\n",
        "    # Calculate BERTScore similarity between cleaned input and generated text\n",
        "    P, R, F1 = score([generated_text], [cleaned_input], lang=\"en\", model_type=\"bert-base-uncased\")\n",
        "\n",
        "    # Display BERTScore results\n",
        "    print(\"\\nBERTScore Results (Similarity between Cleaned Input and Generated Output):\")\n",
        "    print(f\"Precision: {P.mean().item():.4f}\")\n",
        "    print(f\"Recall: {R.mean().item():.4f}\")\n",
        "    print(f\"F1 Score: {F1.mean().item():.4f}\")\n",
        "\n",
        "# Example usage\n",
        "input_text = \"\"\"# Introduction to Group Theory\n",
        "\n",
        "Group Theory is a topic that involves deep mathematical concepts.\n",
        "\n",
        "\\\\[ T = T^i_j \\\\partial_i dx^j \\\\]\n",
        "\n",
        "## Applications\n",
        "\n",
        "- Quantum Mechanics\n",
        "- Particle Physics\n",
        "- Quantum Electrodynamics\"\"\"\n",
        "\n",
        "generate_and_evaluate(input_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0JZjyULf0sD",
        "outputId": "825df6ee-a1f1-4d7e-8762-3865c3de34f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Input Text:\n",
            " Introduction to Group Theory Group Theory is a topic that involves deep mathematical concepts. the formula is given for reference Applications Quantum Mechanics Particle Physics Quantum Electrodynamics\n",
            "\n",
            "Predicted Text:\n",
            " Introduction to Group Theory. Group Theory is a topic that involves deep mathematical concepts. the formula is given for reference Applications. Quantum Mechanics. Particle Physics. Quantum Electrodynamics.\n",
            "\n",
            "BERTScore Results (Similarity between Cleaned Input and Generated Output):\n",
            "Precision: 0.8526\n",
            "Recall: 0.9146\n",
            "F1 Score: 0.8825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from bert_score import score\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"./t5-text-to-text-models\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"./t5-text-to-text-models\")\n",
        "\n",
        "def generate_and_evaluate(input_text):\n",
        "    # Generate output text from the model\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(input_ids['input_ids'], max_length=150, num_beams=5, do_sample=True)\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Display the generated text\n",
        "    print(\"Predicted Text:\\n\", generated_text)\n",
        "\n",
        "    # Calculate BERTScore similarity between input and generated text\n",
        "    P, R, F1 = score([generated_text], [input_text], lang=\"en\", model_type=\"bert-base-uncased\")\n",
        "\n",
        "    # Display BERTScore results\n",
        "    print(f\"\\nBERTScore Results (Similarity between Input and Output):\")\n",
        "    print(f\"Precision: {P.mean().item():.4f}\")\n",
        "    print(f\"Recall: {R.mean().item():.4f}\")\n",
        "    print(f\"F1 Score: {F1.mean().item():.4f}\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "input_text = \"To calculate the area under the curve f(x)=∫01x2 dxf(x)=∫01​x2dx, we can use the formula 13x331​x3 evaluated from 00 to 11. The result, given by a2+b2a2+b2 can be verified by substituting x=ax=a and y=by=b.\"\n",
        "generate_and_evaluate(input_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubgyNntFSgoM",
        "outputId": "62336371-69a8-4fb8-ec64-bc0ff059a8cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Text:\n",
            " To calculate the area under the curve f(x)=01x2 dxf(x)=01 x2dx, we can use the formula 13x331 x3 evaluated from 00 to 11. The result, given by a2+b2a2+b2 can be verified by substituting x=ax=a and y=by=b.\n",
            "\n",
            "BERTScore Results (Similarity between Input and Output):\n",
            "Precision: 0.9422\n",
            "Recall: 0.9609\n",
            "F1 Score: 0.9515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from bert_score import score\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"./t5-text-to-text-models\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"./t5-text-to-text-models\")\n",
        "\n",
        "def generate_and_evaluate(input_text):\n",
        "    # Generate output text from the model\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)  # Increased max_length\n",
        "    outputs = model.generate(input_ids['input_ids'], max_length=120, num_beams=7, do_sample=False)  # Updated settings\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Display the generated text\n",
        "    print(\"Predicted Text:\\n\", generated_text)\n",
        "\n",
        "    # Calculate BERTScore similarity between input and generated text\n",
        "    P, R, F1 = score([generated_text], [input_text], lang=\"en\", model_type=\"bert-base-uncased\")\n",
        "\n",
        "    # Display BERTScore results\n",
        "    print(f\"\\nBERTScore Results (Similarity between Input and Output):\")\n",
        "    print(f\"Precision: {P.mean().item():.4f}\")\n",
        "    print(f\"Recall: {R.mean().item():.4f}\")\n",
        "    print(f\"F1 Score: {F1.mean().item():.4f}\")\n",
        "\n",
        "# Example usage\n",
        "input_text = \"\"\"\n",
        "# Principles of Reinforcement Learning\n",
        "* Agent and Environment: The agent is the learner or decision-maker, while the environment is the external system that the agent interacts with.\n",
        "* State: The state represents the current situation or configuration of the environment. It encapsulates all the relevant information that the agent needs to make a decision.\n",
        "* Action: An action is any decision or move the agent makes in response to the current state. The set of all possible actions is known as the action space.\n",
        "* Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time.\n",
        "* Policy: The policy is a strategy or mapping from states to actions. It defines the agent's behavior at any given state, determining which action to take.\n",
        "* Value Function: The value function estimates the expected cumulative reward that can be obtained from a particular state (or state-action pair). It helps the agent to evaluate which states or actions are more promising in the long run.\n",
        "\n",
        "# Applications of Reinforcement Learning\n",
        "Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include:\n",
        "1. Gaming: RL has been widely used in the development of game-playing AI, from classic board games like Go and Chess to complex video games like StarCraft and Dota 2.\n",
        "2. Robotics: In robotics, RL is used to teach robots how to perform tasks through trial and error, such as grasping objects, walking, or navigating environments.\n",
        "3. Autonomous Vehicles: RL plays a significant role in the development of self-driving cars. These vehicles need to make real-time decisions based on their surroundings.\n",
        "4. Energy Management: RL is used in optimizing energy consumption in smart grids, reducing operational costs in power plants, and improving the efficiency of renewable energy systems.\n",
        "5. Healthcare: In healthcare, RL is used for personalized treatment planning, drug discovery, and optimizing medical procedures.\n",
        "\n",
        "# Key Algorithms in Reinforcement Learning\n",
        "Several algorithms have been developed to address different challenges in reinforcement learning. Two of the most fundamental and widely used algorithms are Q-learning and Deep Q-Networks (DQN).\n",
        "The Q-value for a state-action pair is updated using the following rule: Q(s, a) ← Q(s, a) + α [r + γ max_a' Q(s', a') − Q(s, a)]\n",
        "Here, s and a are the current state and action, r is the reward received, s' is the next state, γ is the discount factor, and α is the learning rate.\n",
        "\n",
        "# Deep Q-Networks (DQN)\n",
        "DQN is an extension of Q-learning that uses deep neural networks to approximate the Q-function, making it scalable to environments with large or continuous state spaces.\n",
        "In DQN, a neural network is used to approximate the Q-values for all possible actions given a state. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted.\n",
        "Experience Replay: To improve stability and efficiency, DQN uses experience replay, where the agent's experiences (state, action, reward, next state) are stored in a replay buffer. Random samples from this buffer are used to update the network, breaking the correlation between consecutive updates.\n",
        "Target Network: DQN also uses a target network, a copy of the Q-network that is updated less frequently. This helps in stabilizing the training by providing a more consistent target for the Q-value updates.\n",
        "\n",
        "# Conclusion\n",
        "Reinforcement learning represents a powerful approach to solving complex decision-making problems by enabling agents to learn through interaction with their environment. With applications ranging from gaming and robotics to healthcare and finance, RL continues to push the boundaries of what AI systems can achieve. Algorithms like Q-learning and Deep Q-Networks have laid the foundation for modern RL, and ongoing research promises to unlock even greater potential in the years to come.\n",
        "\"\"\"\n",
        "\n",
        "generate_and_evaluate(input_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pcY1-1ogVRn",
        "outputId": "61699f43-32ec-4040-c537-c7a3070e8373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Text:\n",
            " Applications of Reinforcement Learning. Reinforcement learning uses deep neural networks to approximate the Q-values. The agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward.\n",
            "\n",
            "BERTScore Results (Similarity between Input and Output):\n",
            "Precision: 0.7836\n",
            "Recall: 0.4859\n",
            "F1 Score: 0.5999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Principles of Reinforcement Learning\n",
        "* Agent and Environment: The agent is the learner or decision-maker, while the environment is the external system that the agent interacts with.\n",
        "* •\tState: The state represents the current situation or configuration of the environment. It encapsulates all the relevant information that the agent needs to make a decision.\n",
        "\n",
        "* •\tAction: An action is any decision or move the agent makes in response to the current state. The set of all possible actions is known as the action space.\n",
        "* •Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time.\n",
        "\n",
        "* •\tPolicy: The policy is a strategy or mapping from states to actions. It defines the agent's behavior at any given state, determining which action to take.\n",
        "* •\tValue Function: The value function estimates the expected cumulative reward that can be obtained from a particular state (or state-action pair). It helps the agent to evaluate which states or actions are more promising in the long run.\n",
        "  .\n",
        "\n",
        "# Applications of Reinforcement Learning\n",
        "Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include:\n",
        "\n",
        "\n",
        "1. •\tGaming: RL has been widely used in the development of game-playing AI, from classic board games like Go and Chess to complex video games like StarCraft and Dota 2.\n",
        "2. •\tRobotics: In robotics, RL is used to teach robots how to perform tasks through trial and error, such as grasping objects, walking, or navigating environments.\n",
        "\n",
        "1. •\tAutonomous Vehicles: RL plays a significant role in the development of self-driving cars. These vehicles need to make real-time decisions based on their surroundings.\n",
        "2. •\tEnergy Management: RL is used in optimizing energy consumption in smart grids, reducing operational costs in power plants, and improving the efficiency of renewable energy systems.\n",
        "\n",
        "1. •\tHealthcare: In healthcare, RL is used for personalized treatment planning, drug discovery, and optimizing medical procedures.\n",
        "\n",
        "# Key Algorithms in Reinforcement Learning\n",
        "\n",
        "Several algorithms have been developed to address different challenges in reinforcement learning. Two of the most fundamental and widely used algorithms are Q-learning and Deep Q-Networks (DQN).\n",
        "Update Rule:\n",
        "\n",
        "The Q-value for a state-action pair is updated using the following rule:\n",
        "\n",
        "\n",
        "Q(s, a) ← Q(s, a) + α [r + γ max_a' Q(s', a') − Q(s, a)]\n",
        "\n",
        "Here, s and a are the current state and action, r is the reward received, s' is the next state, γ is the discount factor, and α is the learning rate.\n",
        "\n",
        "\n",
        "\n",
        "# Deep Q-Networks (DQN)\n",
        "\n",
        "DQN is an extension of Q-learning that uses deep neural networks to approximate the Q-function, making it scalable to environments with large or continuous state spaces.\n",
        "\n",
        "Network Architecture:\n",
        "\n",
        "In DQN, a neural network is used to approximate the Q-values for all possible actions given a state. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted.\n",
        "\n",
        "Experience Replay:\n",
        "\n",
        "To improve stability and efficiency, DQN uses experience replay, where the agent's experiences (state, action, reward, next state) are stored in a replay buffer. During training, random samples from this buffer are used to update the network, breaking the correlation between consecutive updates.\n",
        "\n",
        "Target Network:\n",
        "\n",
        "DQN also uses a target network, a copy of the Q-network that is updated less frequently. This helps in stabilizing the training by providing a more consistent target for the Q-value updates.\n",
        "\n",
        "Success Stories:\n",
        "\n",
        "DQN gained significant attention when it was used by DeepMind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs, showcasing the power of combining RL with deep learning.\n",
        "\n",
        "\n",
        "# conclusion\n",
        "\n",
        "Reinforcement learning represents a powerful approach to solving complex decision-making problems by enabling agents to learn through interaction with their environment.\n",
        "With applications ranging from gaming and robotics to healthcare and finance, RL continues to push the boundaries of what AI systems can achieve. Algorithms like Q-learning\n",
        "and Deep Q-Networks have laid the foundation for modern RL, and ongoing research promises to unlock even greater potential in the years to come."
      ],
      "metadata": {
        "id": "Ckq0N4vXgSby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_evaluate(input_text):\n",
        "    # Enhanced generation parameters\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=1536, truncation=True)\n",
        "    outputs = model.generate(\n",
        "        input_ids['input_ids'],\n",
        "        max_length=300,\n",
        "        min_length=150,\n",
        "        num_beams=10,\n",
        "        temperature=0.5,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.2,\n",
        "        length_penalty=1.0,\n",
        "        do_sample=True\n",
        "    )\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Display the generated text\n",
        "    print(\"Predicted Text:\\n\", generated_text)\n",
        "\n",
        "    # Calculate BERTScore similarity between input and generated text\n",
        "    P, R, F1 = score([generated_text], [input_text], lang=\"en\", model_type=\"bert-base-uncased\")\n",
        "\n",
        "    # Display BERTScore results\n",
        "    print(f\"\\nBERTScore Results (Similarity between Input and Output):\")\n",
        "    print(f\"Precision: {P.mean().item():.4f}\")\n",
        "    print(f\"Recall: {R.mean().item():.4f}\")\n",
        "    print(f\"F1 Score: {F1.mean().item():.4f}\")\n",
        "\n",
        "# Example usage with a rephrased input for better mapping\n",
        "input_text = \"\"\"\n",
        "# Principles of Reinforcement Learning\n",
        "* Agent and Environment: The agent is the learner or decision-maker, while the environment is the external system that the agent interacts with.\n",
        "* •\tState: The state represents the current situation or configuration of the environment. It encapsulates all the relevant information that the agent needs to make a decision.\n",
        "\n",
        "* •\tAction: An action is any decision or move the agent makes in response to the current state. The set of all possible actions is known as the action space.\n",
        "* •Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time.\n",
        "\n",
        "* •\tPolicy: The policy is a strategy or mapping from states to actions. It defines the agent's behavior at any given state, determining which action to take.\n",
        "* •\tValue Function: The value function estimates the expected cumulative reward that can be obtained from a particular state (or state-action pair). It helps the agent to evaluate which states or actions are more promising in the long run.\n",
        "  .\n",
        "\n",
        "# Applications of Reinforcement Learning\n",
        "Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include:\n",
        "\n",
        "\n",
        "1. •\tGaming: RL has been widely used in the development of game-playing AI, from classic board games like Go and Chess to complex video games like StarCraft and Dota 2.\n",
        "2. •\tRobotics: In robotics, RL is used to teach robots how to perform tasks through trial and error, such as grasping objects, walking, or navigating environments.\n",
        "\n",
        "1. •\tAutonomous Vehicles: RL plays a significant role in the development of self-driving cars. These vehicles need to make real-time decisions based on their surroundings.\n",
        "2. •\tEnergy Management: RL is used in optimizing energy consumption in smart grids, reducing operational costs in power plants, and improving the efficiency of renewable energy systems.\n",
        "\n",
        "1. •\tHealthcare: In healthcare, RL is used for personalized treatment planning, drug discovery, and optimizing medical procedures.\n",
        "\n",
        "# Key Algorithms in Reinforcement Learning\n",
        "\n",
        "Several algorithms have been developed to address different challenges in reinforcement learning. Two of the most fundamental and widely used algorithms are Q-learning and Deep Q-Networks (DQN).\n",
        "Update Rule:\n",
        "\n",
        "The Q-value for a state-action pair is updated using the following rule:\n",
        "\n",
        "\n",
        "Q(s, a) ← Q(s, a) + α [r + γ max_a' Q(s', a') − Q(s, a)]\n",
        "\n",
        "Here, s and a are the current state and action, r is the reward received, s' is the next state, γ is the discount factor, and α is the learning rate.\n",
        "\n",
        "\n",
        "\n",
        "# Deep Q-Networks (DQN)\n",
        "\n",
        "DQN is an extension of Q-learning that uses deep neural networks to approximate the Q-function, making it scalable to environments with large or continuous state spaces.\n",
        "\n",
        "Network Architecture:\n",
        "\n",
        "In DQN, a neural network is used to approximate the Q-values for all possible actions given a state. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted.\n",
        "\n",
        "Experience Replay:\n",
        "\n",
        "To improve stability and efficiency, DQN uses experience replay, where the agent's experiences (state, action, reward, next state) are stored in a replay buffer. During training, random samples from this buffer are used to update the network, breaking the correlation between consecutive updates.\n",
        "\n",
        "Target Network:\n",
        "\n",
        "DQN also uses a target network, a copy of the Q-network that is updated less frequently. This helps in stabilizing the training by providing a more consistent target for the Q-value updates.\n",
        "\n",
        "Success Stories:\n",
        "\n",
        "DQN gained significant attention when it was used by DeepMind to play Atari games at a superhuman level. The algorithm demonstrated the ability to learn complex policies directly from raw pixel inputs, showcasing the power of combining RL with deep learning.\n",
        "\n",
        "\n",
        "# conclusion\n",
        "\n",
        "Reinforcement learning represents a powerful approach to solving complex decision-making problems by enabling agents to learn through interaction with their environment.\n",
        "With applications ranging from gaming and robotics to healthcare and finance, RL continues to push the boundaries of what AI systems can achieve. Algorithms like Q-learning\n",
        "and Deep Q-Networks have laid the foundation for modern RL, and ongoing research promises to unlock even greater potential in the years to come.\n",
        "\"\"\"\n",
        "\n",
        "generate_and_evaluate(input_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oe4eoOGyiCkI",
        "outputId": "f2291aba-3f8d-4144-dc2e-1267ed1fd933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Text:\n",
            " Applications of Reinforcement Learning. Reinforcement learning uses deep neural networks to approximate the Q-values. The agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward. The reward is a strategy. The value function. The value function. The value function. The value function. The value function. The value function. The reward is a strategy. The agent’s goal is to maximize the cumulative.. Reinforcement Learning. In e-learning.DQ-learning.I.E,'s (â€” and.\n",
            "\n",
            "BERTScore Results (Similarity between Input and Output):\n",
            "Precision: 0.7192\n",
            "Recall: 0.5108\n",
            "F1 Score: 0.5974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from bert_score import score\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"./t5-text-to-text-models\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"./t5-text-to-text-models\")\n",
        "\n",
        "def generate_and_evaluate_chunked(input_text):\n",
        "    # Split the input text into sections by double newlines or logical segments\n",
        "    sections = input_text.split(\"\\n\\n\")  # Basic split; adjust as needed for your text structure\n",
        "\n",
        "    # Store the full generated text\n",
        "    full_generated_text = \"\"\n",
        "\n",
        "    for section in sections:\n",
        "        # Generate output text for each section\n",
        "        input_ids = tokenizer(section, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "        outputs = model.generate(\n",
        "            input_ids['input_ids'],\n",
        "            max_length=300,  # Adjust max_length for each section\n",
        "            min_length=150,\n",
        "            num_beams=10,\n",
        "            temperature=0.3,\n",
        "            top_p=0.85,\n",
        "            repetition_penalty=1.2,\n",
        "            length_penalty=1.0,\n",
        "            do_sample=True\n",
        "        )\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        full_generated_text += generated_text + \"\\n\\n\"  # Append each section's output to the full text\n",
        "\n",
        "    # Display the full generated text\n",
        "    print(\"Predicted Text:\\n\", full_generated_text)\n",
        "\n",
        "    # Calculate BERTScore similarity between input and generated text\n",
        "    P, R, F1 = score([full_generated_text], [input_text], lang=\"en\", model_type=\"bert-base-uncased\")\n",
        "\n",
        "    # Display BERTScore results\n",
        "    print(f\"\\nBERTScore Results (Similarity between Input and Output):\")\n",
        "    print(f\"Precision: {P.mean().item():.4f}\")\n",
        "    print(f\"Recall: {R.mean().item():.4f}\")\n",
        "    print(f\"F1 Score: {F1.mean().item():.4f}\")\n",
        "\n",
        "# Example usage with your input text\n",
        "input_text = \"\"\"\n",
        "# Principles of Reinforcement Learning\n",
        "* Agent and Environment: The agent is the learner or decision-maker, while the environment is the external system that the agent interacts with.\n",
        "* State: The state represents the current situation or configuration of the environment. It encapsulates all the relevant information that the agent needs to make a decision.\n",
        "* Action: An action is any decision or move the agent makes in response to the current state. The set of all possible actions is known as the action space.\n",
        "* Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time.\n",
        "* Policy: The policy is a strategy or mapping from states to actions. It defines the agent's behavior at any given state, determining which action to take.\n",
        "* Value Function: The value function estimates the expected cumulative reward that can be obtained from a particular state (or state-action pair). It helps the agent to evaluate which states or actions are more promising in the long run.\n",
        "\n",
        "# Applications of Reinforcement Learning\n",
        "Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include:\n",
        "1. Gaming: RL has been widely used in the development of game-playing AI, from classic board games like Go and Chess to complex video games like StarCraft and Dota 2.\n",
        "2. Robotics: In robotics, RL is used to teach robots how to perform tasks through trial and error, such as grasping objects, walking, or navigating environments.\n",
        "3. Autonomous Vehicles: RL plays a significant role in the development of self-driving cars. These vehicles need to make real-time decisions based on their surroundings.\n",
        "4. Energy Management: RL is used in optimizing energy consumption in smart grids, reducing operational costs in power plants, and improving the efficiency of renewable energy systems.\n",
        "5. Healthcare: In healthcare, RL is used for personalized treatment planning, drug discovery, and optimizing medical procedures.\n",
        "\n",
        "# Key Algorithms in Reinforcement Learning\n",
        "Several algorithms have been developed to address different challenges in reinforcement learning. Two of the most fundamental and widely used algorithms are Q-learning and Deep Q-Networks (DQN).\n",
        "The Q-value for a state-action pair is updated using the following rule: Q(s, a) ← Q(s, a) + α [r + γ max_a' Q(s', a') − Q(s, a)]\n",
        "Here, s and a are the current state and action, r is the reward received, s' is the next state, γ is the discount factor, and α is the learning rate.\n",
        "\n",
        "# Deep Q-Networks (DQN)\n",
        "DQN is an extension of Q-learning that uses deep neural networks to approximate the Q-function, making it scalable to environments with large or continuous state spaces.\n",
        "In DQN, a neural network is used to approximate the Q-values for all possible actions given a state. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted.\n",
        "Experience Replay: To improve stability and efficiency, DQN uses experience replay, where the agent's experiences (state, action, reward, next state) are stored in a replay buffer. Random samples from this buffer are used to update the network, breaking the correlation between consecutive updates.\n",
        "Target Network: DQN also uses a target network, a copy of the Q-network that is updated less frequently. This helps in stabilizing the training by providing a more consistent target for the Q-value updates.\n",
        "\n",
        "# Conclusion\n",
        "Reinforcement learning represents a powerful approach to solving complex decision-making problems by enabling agents to learn through interaction with their environment. With applications ranging from gaming and robotics to healthcare and finance, RL continues to push the boundaries of what AI systems can achieve. Algorithms like Q-learning and Deep Q-Networks have laid the foundation for modern RL, and ongoing research promises to unlock even greater potential in the years to come.\n",
        "\"\"\"\n",
        "\n",
        "generate_and_evaluate_chunked(input_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfIkjTiHi7Vl",
        "outputId": "1c44c71d-3e74-4bd7-a0a6-488ec940862a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Text:\n",
            " Principles of Reinforcement Learning. Reinforcement Learning. * Agent and Environment. The agent is the learner or decision-maker. The environment is the external system that the agent interacts with. * Reward. After taking an action. the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. * Reward. After taking an action. the agent receives feedback in the form of a reward. The reward is a measure that indicates the immediate benefit?•Re-Enforcement Learning..... * Principles.. * Agent and English. * Ag.A. The agency's.com. ()\n",
            "\n",
            "Applications of Reinforcement Learning. Reinforcement learning has found applications in various domains. Some of the most prominent applications include: 1. Gaming. RL is widely used in the development of game-playing AI. From classic board games like Go and Chess. to complex video games like StarCraft. Dota. 2. Autonomous Vehicles. RL is used to teach robots how to perform tasks. These vehicles need to make real-time decisions. 4. Energy Management. RL is used in optimizing energy consumption.». Reinforcement learning\n",
            "\n",
            "Key Algorithms in Reinforcement Learning. Reinforcement Learning (DQN). Two of the most fundamental and widely used algorithms are Q-learning. Deep Q-Networks. (DQN). The Q-value for a state-action pair is updated using the following rule: Q(s, a)  Q(s, a) +  [r +  max_a' Q(s', a)  Q(s, a)]. Here,... Key Algorithms in Reinforcement Learning...)..........\n",
            "\n",
            "Deep Q-Networks (DQN). DQN uses deep neural networks to approximate the Q-function. The network is trained using the Q-learning update rule. but instead of updating a table of Q-values. The network's weights are adjusted. Experience Replay. To improve stability and efficiency. DQN. uses a target network. a copy of the Q-network. updated less frequently. This helps in stabilizing the training by providing a more consistent target for the Q-value updates.Quetworks (Q-Networks (DQN) to make it scalable to environments with large or continuous state, a.A. (Q-learning.\n",
            "\n",
            "RL continues to push the boundaries of what AI systems can achieve. Algorithms have laid the foundation for modern RL. modern RL. RL continues to push the boundaries of what AI systems can achieve. RL continues to push the boundaries. algorithms. such as Q-learning. Deep Q-Networks. have laid the foundation for modern RL. and ongoing research promises to unlock even greater potential in the years to come. RL offers applications ranging from gaming and robotics. Conclusion. Reinforcement learning. RL offers a platform for AI.com. e.a. can’t, and RL has a RL.\n",
            "\n",
            "\n",
            "\n",
            "BERTScore Results (Similarity between Input and Output):\n",
            "Precision: 0.7147\n",
            "Recall: 0.7098\n",
            "F1 Score: 0.7123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from bert_score import score\n",
        "import re\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"./t5-text-to-text-models\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"./t5-text-to-text-models\")\n",
        "\n",
        "def post_process_text(generated_text):\n",
        "    # Remove duplicate sentences\n",
        "    sentences = generated_text.split('. ')\n",
        "    unique_sentences = list(dict.fromkeys(sentences))  # Preserve order while removing duplicates\n",
        "\n",
        "    # Clean up formatting and punctuation\n",
        "    cleaned_text = '. '.join(unique_sentences).replace(\"  \", \" \")  # Replace double spaces\n",
        "    cleaned_text = re.sub(r'\\s*\\.\\s*', '. ', cleaned_text)  # Fix spaces around periods\n",
        "    return cleaned_text\n",
        "\n",
        "def generate_and_evaluate_chunked(input_text):\n",
        "    # Split the input text into sections by double newlines or logical segments\n",
        "    sections = input_text.split(\"\\n\\n\")  # Basic split; adjust as needed for your text structure\n",
        "\n",
        "    # Store the full generated text\n",
        "    full_generated_text = \"\"\n",
        "\n",
        "    for section in sections:\n",
        "        # Generate output text for each section\n",
        "        input_ids = tokenizer(section, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "        outputs = model.generate(\n",
        "            input_ids['input_ids'],\n",
        "            max_length=300,  # Adjust max_length for each section\n",
        "            min_length=150,\n",
        "            num_beams=10,\n",
        "            temperature=0.7,  # Adjusted for more diverse outputs\n",
        "            top_p=0.9,        # Adjusted to allow for more flexible sampling\n",
        "            repetition_penalty=1.2,\n",
        "            length_penalty=1.0,\n",
        "            do_sample=True\n",
        "        )\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        full_generated_text += generated_text + \"\\n\\n\"  # Append each section's output to the full text\n",
        "\n",
        "    # Post-process the generated text\n",
        "    cleaned_generated_text = post_process_text(full_generated_text)\n",
        "\n",
        "    # Display the cleaned generated text\n",
        "    print(\"Cleaned Predicted Text:\\n\", cleaned_generated_text)\n",
        "\n",
        "    # Calculate BERTScore similarity between input and generated text\n",
        "    P, R, F1 = score([cleaned_generated_text], [input_text], lang=\"en\", model_type=\"bert-base-uncased\")\n",
        "\n",
        "    # Display BERTScore results\n",
        "    print(f\"\\nBERTScore Results (Similarity between Input and Output):\")\n",
        "    print(f\"Precision: {P.mean().item():.4f}\")\n",
        "    print(f\"Recall: {R.mean().item():.4f}\")\n",
        "    print(f\"F1 Score: {F1.mean().item():.4f}\")\n",
        "\n",
        "# Example usage with your input text\n",
        "input_text = \"\"\"\n",
        "# Principles of Reinforcement Learning\n",
        "* Agent and Environment: The agent is the learner or decision-maker, while the environment is the external system that the agent interacts with.\n",
        "* State: The state represents the current situation or configuration of the environment. It encapsulates all the relevant information that the agent needs to make a decision.\n",
        "* Action: An action is any decision or move the agent makes in response to the current state. The set of all possible actions is known as the action space.\n",
        "* Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time.\n",
        "* Policy: The policy is a strategy or mapping from states to actions. It defines the agent's behavior at any given state, determining which action to take.\n",
        "* Value Function: The value function estimates the expected cumulative reward that can be obtained from a particular state (or state-action pair). It helps the agent to evaluate which states or actions are more promising in the long run.\n",
        "\n",
        "# Applications of Reinforcement Learning\n",
        "Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include:\n",
        "1. Gaming: RL has been widely used in the development of game-playing AI, from classic board games like Go and Chess to complex video games like StarCraft and Dota 2.\n",
        "2. Robotics: In robotics, RL is used to teach robots how to perform tasks through trial and error, such as grasping objects, walking, or navigating environments.\n",
        "3. Autonomous Vehicles: RL plays a significant role in the development of self-driving cars. These vehicles need to make real-time decisions based on their surroundings.\n",
        "4. Energy Management: RL is used in optimizing energy consumption in smart grids, reducing operational costs in power plants, and improving the efficiency of renewable energy systems.\n",
        "5. Healthcare: In healthcare, RL is used for personalized treatment planning, drug discovery, and optimizing medical procedures.\n",
        "\n",
        "# Key Algorithms in Reinforcement Learning\n",
        "Several algorithms have been developed to address different challenges in reinforcement learning. Two of the most fundamental and widely used algorithms are Q-learning and Deep Q-Networks (DQN).\n",
        "The Q-value for a state-action pair is updated using the following rule: Q(s, a) ← Q(s, a) + α [r + γ max_a' Q(s', a') − Q(s, a)].\n",
        "Here, s and a are the current state and action, r is the reward received, s' is the next state, γ is the discount factor, and α is the learning rate.\n",
        "\n",
        "# Deep Q-Networks (DQN)\n",
        "DQN is an extension of Q-learning that uses deep neural networks to approximate the Q-function, making it scalable to environments with large or continuous state spaces.\n",
        "In DQN, a neural network is used to approximate the Q-values for all possible actions given a state. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted.\n",
        "Experience Replay: To improve stability and efficiency, DQN uses experience replay, where the agent's experiences (state, action, reward, next state) are stored in a replay buffer. Random samples from this buffer are used to update the network, breaking the correlation between consecutive updates.\n",
        "Target Network: DQN also uses a target network, a copy of the Q-network that is updated less frequently. This helps in stabilizing the training by providing a more consistent target for the Q-value updates.\n",
        "\n",
        "# Conclusion\n",
        "Reinforcement learning represents a powerful approach to solving complex decision-making problems by enabling agents to learn through interaction with their environment. With applications ranging from gaming and robotics to healthcare and finance, RL continues to push the boundaries of what AI systems can achieve. Algorithms like Q-learning and Deep Q-Networks have laid the foundation for modern RL, and ongoing research promises to unlock even greater potential in the years to come.\n",
        "\"\"\"\n",
        "\n",
        "# Call the function\n",
        "generate_and_evaluate_chunked(input_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3uBnc8jmlzJ",
        "outputId": "72a5c3e7-e136-4728-b14e-28f7709d073b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Predicted Text:\n",
            " Principles of Reinforcement Learning. Reinforcement Learning. * Agent and Environment. The agent is the learner or decision-maker. The environment is the external system that the agent interacts with. * Reward. After taking an action. the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The reward is a measure that indicates the immediate benefit. »Enforcement Learning. [Trag. * Principles. English. * Agent. * Agent. com. * State. It's. Applications of Reinforcement Learning. Reinforcement learning has found applications in various domains. Some of the most prominent applications include: 1. Gaming. RL is widely used in the development of game-playing AI. From classic board games like Go and Chess. to complex video games like StarCraft. Dota. 2. Autonomous Vehicles. RL is used to teach robots how to perform tasks. These vehicles need to make real-time decisions. 4. Energy Management. RL is used in optimizing energy consumption. ». Reinforcement learning\n",
            "\n",
            "Key Algorithms in Reinforcement Learning. Two of the most fundamental and widely used algorithms are Q-learning. Deep Q-Networks. (DQN). The Q-value for a state-action pair is updated using the following rule: Q(s, a) Q(s, a) + [r + max_a' Q(s', a') Q(s, a)]. Here, s and. . . algorithms in Reinforcement Learning. . in. . . . and. . . . . . . . Deep Q-Networks (DQN). DQN uses deep neural networks to approximate the Q-function, making it scalable to environments with large or continuous state spaces. The network is trained using the Q-learning update rule. but instead of updating a table of Q-values. The network's weights are adjusted. Experience Replay. To improve stability and efficiency. DQN. uses a target network. a copy of the Q-network. updated less frequently. This helps in stabilizing the training. Rooi-Networks (DQN). Cadreel to use deep neural networks to approximate the Q-function, making it scalable to environments with large or continuous. RL continues to push the boundaries of what AI systems can achieve. Algorithms have laid the foundation for modern RL. modern RL. RL continues to push the boundaries of what AI systems can achieve. RL continues to push the boundaries. algorithms. like Q-learning. have laid the foundation for modern RL. research promises to unlock even greater potential in the years to come. “Reforcement learning represents a powerful approach to solving complex decision-making challenges. “. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
            "\n",
            "BERTScore Results (Similarity between Input and Output):\n",
            "Precision: 0.7001\n",
            "Recall: 0.7079\n",
            "F1 Score: 0.7040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from bert_score import score\n",
        "import re\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"./t5-text-to-text-models\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"./t5-text-to-text-models\")\n",
        "\n",
        "def post_process_text(generated_text):\n",
        "    # Remove duplicate sentences while preserving order\n",
        "    sentences = generated_text.split('. ')\n",
        "    unique_sentences = list(dict.fromkeys(sentences))\n",
        "\n",
        "    # Join the unique sentences into a single text block\n",
        "    cleaned_text = '. '.join(unique_sentences).replace(\"  \", \" \")\n",
        "\n",
        "    # Remove unwanted characters and fix common punctuation issues\n",
        "    cleaned_text = re.sub(r'[\\*\\[\\]\\\"\\'’]', '', cleaned_text)  # Remove unwanted symbols\n",
        "    cleaned_text = re.sub(r'\\s*\\.\\s*', '. ', cleaned_text)  # Fix spaces around periods\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Collapse multiple spaces\n",
        "    cleaned_text = cleaned_text.strip()  # Remove leading and trailing spaces\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "def generate_and_evaluate_chunked(input_text):\n",
        "    # Split the input text into sections by double newlines\n",
        "    sections = input_text.split(\"\\n\\n\")\n",
        "\n",
        "    # Store the full generated text\n",
        "    full_generated_text = \"\"\n",
        "\n",
        "    for section in sections:\n",
        "        # Generate output text for each section\n",
        "        input_ids = tokenizer(section, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "        outputs = model.generate(\n",
        "            input_ids['input_ids'],\n",
        "            max_length=300,\n",
        "            min_length=150,\n",
        "            num_beams=10,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2,\n",
        "            length_penalty=1.0,\n",
        "            do_sample=True\n",
        "        )\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        full_generated_text += generated_text + \"\\n\\n\"\n",
        "\n",
        "    # Post-process the generated text\n",
        "    cleaned_generated_text = post_process_text(full_generated_text)\n",
        "\n",
        "    # Display the cleaned generated text\n",
        "    print(\"Cleaned Predicted Text:\\n\", cleaned_generated_text)\n",
        "\n",
        "    # Calculate BERTScore similarity between input and generated text\n",
        "    P, R, F1 = score([cleaned_generated_text], [input_text], lang=\"en\", model_type=\"bert-base-uncased\")\n",
        "\n",
        "    # Display BERTScore results\n",
        "    print(f\"\\nBERTScore Results (Similarity between Input and Output):\")\n",
        "    print(f\"Precision: {P.mean().item():.4f}\")\n",
        "    print(f\"Recall: {R.mean().item():.4f}\")\n",
        "    print(f\"F1 Score: {F1.mean().item():.4f}\")\n",
        "\n",
        "# Example usage with your input text\n",
        "input_text = \"\"\"\n",
        "# Principles of Reinforcement Learning\n",
        "* Agent and Environment: The agent is the learner or decision-maker, while the environment is the external system that the agent interacts with.\n",
        "* State: The state represents the current situation or configuration of the environment. It encapsulates all the relevant information that the agent needs to make a decision.\n",
        "* Action: An action is any decision or move the agent makes in response to the current state. The set of all possible actions is known as the action space.\n",
        "* Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a numerical value that indicates the immediate benefit of the action. The agent’s goal is to maximize the cumulative reward over time.\n",
        "* Policy: The policy is a strategy or mapping from states to actions. It defines the agent's behavior at any given state, determining which action to take.\n",
        "* Value Function: The value function estimates the expected cumulative reward that can be obtained from a particular state (or state-action pair). It helps the agent to evaluate which states or actions are more promising in the long run.\n",
        "\n",
        "# Applications of Reinforcement Learning\n",
        "Reinforcement learning has found applications in various domains, particularly where decision-making in dynamic environments is crucial. Some of the most prominent applications include:\n",
        "1. Gaming: RL has been widely used in the development of game-playing AI, from classic board games like Go and Chess to complex video games like StarCraft and Dota 2.\n",
        "2. Robotics: In robotics, RL is used to teach robots how to perform tasks through trial and error, such as grasping objects, walking, or navigating environments.\n",
        "3. Autonomous Vehicles: RL plays a significant role in the development of self-driving cars. These vehicles need to make real-time decisions based on their surroundings.\n",
        "4. Energy Management: RL is used in optimizing energy consumption in smart grids, reducing operational costs in power plants, and improving the efficiency of renewable energy systems.\n",
        "5. Healthcare: In healthcare, RL is used for personalized treatment planning, drug discovery, and optimizing medical procedures.\n",
        "\n",
        "# Key Algorithms in Reinforcement Learning\n",
        "Several algorithms have been developed to address different challenges in reinforcement learning. Two of the most fundamental and widely used algorithms are Q-learning and Deep Q-Networks (DQN).\n",
        "The Q-value for a state-action pair is updated using the following rule: Q(s, a) ← Q(s, a) + α [r + γ max_a' Q(s', a') − Q(s, a)].\n",
        "Here, s and a are the current state and action, r is the reward received, s' is the next state, γ is the discount factor, and α is the learning rate.\n",
        "\n",
        "# Deep Q-Networks (DQN)\n",
        "DQN is an extension of Q-learning that uses deep neural networks to approximate the Q-function, making it scalable to environments with large or continuous state spaces.\n",
        "In DQN, a neural network is used to approximate the Q-values for all possible actions given a state. The network is trained using the Q-learning update rule, but instead of updating a table of Q-values, the network's weights are adjusted.\n",
        "Experience Replay: To improve stability and efficiency, DQN uses experience replay, where the agent's experiences (state, action, reward, next state) are stored in a replay buffer. Random samples from this buffer are used to update the network, breaking the correlation between consecutive updates.\n",
        "Target Network: DQN also uses a target network, a copy of the Q-network that is updated less frequently. This helps in stabilizing the training by providing a more consistent target for the Q-value updates.\n",
        "\n",
        "# Conclusion\n",
        "Reinforcement learning represents a powerful approach to solving complex decision-making problems by enabling agents to learn through interaction with their environment. With applications ranging from gaming and robotics to healthcare and finance, RL continues to push the boundaries of what AI systems can achieve. Algorithms like Q-learning and Deep Q-Networks have laid the foundation for modern RL, and ongoing research promises to unlock even greater potential in the years to come.\n",
        "\"\"\"\n",
        "\n",
        "# Call the function\n",
        "generate_and_evaluate_chunked(input_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFBZ0HLppVd_",
        "outputId": "22a52da7-7dbc-4721-ffb4-2760dc17f727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Predicted Text:\n",
            " The agent is the learner or decision-maker. The environment is the external system that the agent interacts with. The reward is a numerical value that indicates the immediate benefit of the action. Reward: After taking an action, the agent receives feedback in the form of a reward. The reward is a number that indicates the immediate benefit of the action. The agents goal is to maximize the cumulative reward over time. Policy: The policy is a strategy or mapping. from states to actions. It helps the agent toâââââââââââââââââââââââââââââââââ. Applications of Reinforcement Learning. Reinforcement learning has found applications in various domains. Some of the most prominent applications include: 1. Gaming. RL is widely used in the development of game-playing AI. From classic board games like Go and Chess. to complex video games like StarCraft. Dota. 2. Autonomous Vehicles. RL is used to teach robots how to perform tasks. These vehicles need to make real-time decisions. 4. Energy Management. RL is used in optimizing energy consumption. ». Reinforcement learning Key Algorithms in Reinforcement Learning. Reinforcement Learning. Two of the most fundamental and widely used algorithms are Q-learning. Deep Q-Networks. (DQN). The Q-value for a state-action pair is updated using the following rule: Q(s, a) Q(s, a) + r + max_a Q(s, a) Q(s, a). Here, s and. . . algorithms in Reinforcement Learning. . in. . . . and. . . . . . . . Deep Q-Networks (DQN). DQN uses deep neural networks to approximate the Q-function, making it scalable to environments with large or continuous state spaces. The network is trained using the Q-learning update rule. but instead of updating a table of Q-values. The networks weights are adjusted. Experience Replay. To improve stability and efficiency. DQN. uses a target network. a copy of the Q-network. updated less frequently. This helps in stabilizing the training. Ceeps (DQN). Taylore-learning to make it scalable to the Q-function, its based on large or continuous. RL continues to push the boundaries of what AI systems can achieve. Algorithms have laid the foundation for modern RL. modern RL. RL continues to push the boundaries of what AI systems can achieve. RL continues to push the boundaries. algorithms. like Q-learning. have laid the foundation for modern RL. and ongoing research promises to unlock even greater potential in the years to come. “Reforcement learning represents a powerful approach to solving complex decision-making problems. “. . .\n",
            "\n",
            "BERTScore Results (Similarity between Input and Output):\n",
            "Precision: 0.7024\n",
            "Recall: 0.7186\n",
            "F1 Score: 0.7104\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2be751ffa7e54eac9f33c9d9a41c9b7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a499f0944d9b40a7859b247202804dbf",
              "IPY_MODEL_cb4ec8df99874ba99bc0fd58c51100f3",
              "IPY_MODEL_41a47a1e3d3a41dfa1e20955e4cc9fb3"
            ],
            "layout": "IPY_MODEL_d7043368f7924fff9d9913112ba0a621"
          }
        },
        "a499f0944d9b40a7859b247202804dbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e82a5cd5631b43f1bbd5939b8348db70",
            "placeholder": "​",
            "style": "IPY_MODEL_529dd4c98e33465cbbac79dee268b11f",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "cb4ec8df99874ba99bc0fd58c51100f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6684db7c08924c9db346f5d1a85c7bf1",
            "max": 2324,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1beae3538a354dd0b288225c86ef4552",
            "value": 2324
          }
        },
        "41a47a1e3d3a41dfa1e20955e4cc9fb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15e557ceb6b34435912ee042e3e254ce",
            "placeholder": "​",
            "style": "IPY_MODEL_4dc80394ff5b4ea78dd2823f1ca362f8",
            "value": " 2.32k/2.32k [00:00&lt;00:00, 170kB/s]"
          }
        },
        "d7043368f7924fff9d9913112ba0a621": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e82a5cd5631b43f1bbd5939b8348db70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "529dd4c98e33465cbbac79dee268b11f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6684db7c08924c9db346f5d1a85c7bf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1beae3538a354dd0b288225c86ef4552": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "15e557ceb6b34435912ee042e3e254ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dc80394ff5b4ea78dd2823f1ca362f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5fdf389ba3d462183ea436421ccf03e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_31f985188c084e4c8160eb77f2a24923",
              "IPY_MODEL_4f80833f2eca48d3b28d7c1f9b8507e3",
              "IPY_MODEL_e17e0cd72a3543918599f0df078cbf1a"
            ],
            "layout": "IPY_MODEL_b71aeb916d994c6abe88af6cf5c931ff"
          }
        },
        "31f985188c084e4c8160eb77f2a24923": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f990a171a540447693c9656a35934c2d",
            "placeholder": "​",
            "style": "IPY_MODEL_b12675307ca94efab642c3a9c7103e30",
            "value": "spiece.model: 100%"
          }
        },
        "4f80833f2eca48d3b28d7c1f9b8507e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_832076281efc44849d870da8a138ba46",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e80872b417f64eb1968f54beb0331b76",
            "value": 791656
          }
        },
        "e17e0cd72a3543918599f0df078cbf1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb411dec5b84409c8483df24a1499f92",
            "placeholder": "​",
            "style": "IPY_MODEL_0038fa46ec8943558032271cf7adf426",
            "value": " 792k/792k [00:00&lt;00:00, 9.39MB/s]"
          }
        },
        "b71aeb916d994c6abe88af6cf5c931ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f990a171a540447693c9656a35934c2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b12675307ca94efab642c3a9c7103e30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "832076281efc44849d870da8a138ba46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e80872b417f64eb1968f54beb0331b76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb411dec5b84409c8483df24a1499f92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0038fa46ec8943558032271cf7adf426": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "511ec04965c04886aed63dd77ffdbeb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bfaab799ab064fdcb29ae3a1822335f4",
              "IPY_MODEL_b48c281dc2c5423585c9aeedd70a6d94",
              "IPY_MODEL_3f97cdb9813f409db003dcdbcf611086"
            ],
            "layout": "IPY_MODEL_e3beed58c2b34e968b537bc831392f1c"
          }
        },
        "bfaab799ab064fdcb29ae3a1822335f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b914b17c60f47f08f011e8d88e7843c",
            "placeholder": "​",
            "style": "IPY_MODEL_791a20d89da24cba90f2bf3ebc1bc53c",
            "value": "tokenizer.json: 100%"
          }
        },
        "b48c281dc2c5423585c9aeedd70a6d94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc50cdaa4cee4ef2bba9fc4bf9d53db3",
            "max": 1389353,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b60f06da99d44f9d9edecb3a0f6e7e30",
            "value": 1389353
          }
        },
        "3f97cdb9813f409db003dcdbcf611086": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_acd32c7a771c42e3b9e55d32114c4514",
            "placeholder": "​",
            "style": "IPY_MODEL_ad44de52f49e4a1490ff1ec802c76ffd",
            "value": " 1.39M/1.39M [00:00&lt;00:00, 5.15MB/s]"
          }
        },
        "e3beed58c2b34e968b537bc831392f1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b914b17c60f47f08f011e8d88e7843c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "791a20d89da24cba90f2bf3ebc1bc53c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc50cdaa4cee4ef2bba9fc4bf9d53db3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b60f06da99d44f9d9edecb3a0f6e7e30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "acd32c7a771c42e3b9e55d32114c4514": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad44de52f49e4a1490ff1ec802c76ffd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "764cdb0d489646cb8447745c76b9e2e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a7be8bb0d93f461a9c97942aaf2aa9a8",
              "IPY_MODEL_e2db5d58ae924efe9818cadfe1bc4d76",
              "IPY_MODEL_d474f84f396b4857be788ce9c10d4636"
            ],
            "layout": "IPY_MODEL_8cef8103fb4c49ffbcfd66685d29857e"
          }
        },
        "a7be8bb0d93f461a9c97942aaf2aa9a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87cfae86f8a449baa65808d96891a9fa",
            "placeholder": "​",
            "style": "IPY_MODEL_2fd90303b52942838f0226eab2e01105",
            "value": "config.json: 100%"
          }
        },
        "e2db5d58ae924efe9818cadfe1bc4d76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_187554b2792a468591e5f5060373ae0e",
            "max": 1206,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6d4cacc0eb0b40ee80dcd6bf50ac13bd",
            "value": 1206
          }
        },
        "d474f84f396b4857be788ce9c10d4636": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e41778f495e6448da8c709ce5d4d99dc",
            "placeholder": "​",
            "style": "IPY_MODEL_77feb887c4d54a94a6d417888bdd202f",
            "value": " 1.21k/1.21k [00:00&lt;00:00, 86.4kB/s]"
          }
        },
        "8cef8103fb4c49ffbcfd66685d29857e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87cfae86f8a449baa65808d96891a9fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fd90303b52942838f0226eab2e01105": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "187554b2792a468591e5f5060373ae0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d4cacc0eb0b40ee80dcd6bf50ac13bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e41778f495e6448da8c709ce5d4d99dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77feb887c4d54a94a6d417888bdd202f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a7aad14e9b745e9839e58c13012f8ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a9e68ad6dec4e779c74960dffa659fb",
              "IPY_MODEL_07d3417818fe4de7a149cfd9e955ea51",
              "IPY_MODEL_576601504fab49b5a1a7f606fe0fa068"
            ],
            "layout": "IPY_MODEL_84564f5761ed486b9f204f5745818c00"
          }
        },
        "4a9e68ad6dec4e779c74960dffa659fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43b31bcc4c4742be9f7021bb4f900046",
            "placeholder": "​",
            "style": "IPY_MODEL_cd6b7446965a456aa348dd73e5fcc74a",
            "value": "model.safetensors: 100%"
          }
        },
        "07d3417818fe4de7a149cfd9e955ea51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_534a48d856e44e8c95f8882f8d2100cc",
            "max": 242043056,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb197c37f0904b7e8f62e6b4ba0ea824",
            "value": 242043056
          }
        },
        "576601504fab49b5a1a7f606fe0fa068": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36ef4a6bcd5748a594c5456559e051dc",
            "placeholder": "​",
            "style": "IPY_MODEL_abe70b5e933d454186a08bbdfbdee6d5",
            "value": " 242M/242M [00:01&lt;00:00, 188MB/s]"
          }
        },
        "84564f5761ed486b9f204f5745818c00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43b31bcc4c4742be9f7021bb4f900046": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd6b7446965a456aa348dd73e5fcc74a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "534a48d856e44e8c95f8882f8d2100cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb197c37f0904b7e8f62e6b4ba0ea824": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "36ef4a6bcd5748a594c5456559e051dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abe70b5e933d454186a08bbdfbdee6d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a66667e40594cc3a5633ffb382528b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d1b2366cb2b340e59d4be5595cb60386",
              "IPY_MODEL_a4ce80304364449088444e7ff1b045d9",
              "IPY_MODEL_87f7bc3a5ace408aabde43a227c4ba8b"
            ],
            "layout": "IPY_MODEL_420530e8e7db489c812110453346ae95"
          }
        },
        "d1b2366cb2b340e59d4be5595cb60386": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_827678ed8472458eb87bff3900192650",
            "placeholder": "​",
            "style": "IPY_MODEL_271622ac2ba94c84b373e8e3d7a095f0",
            "value": "generation_config.json: 100%"
          }
        },
        "a4ce80304364449088444e7ff1b045d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7befd46485f742518b18eb299973aa83",
            "max": 147,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1434a0287f42405d9cf97e23a53d95ee",
            "value": 147
          }
        },
        "87f7bc3a5ace408aabde43a227c4ba8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e761bcda8c804b2192970f0803696eb2",
            "placeholder": "​",
            "style": "IPY_MODEL_940352a1030a4ad58398826b29d3d408",
            "value": " 147/147 [00:00&lt;00:00, 6.86kB/s]"
          }
        },
        "420530e8e7db489c812110453346ae95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "827678ed8472458eb87bff3900192650": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "271622ac2ba94c84b373e8e3d7a095f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7befd46485f742518b18eb299973aa83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1434a0287f42405d9cf97e23a53d95ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e761bcda8c804b2192970f0803696eb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "940352a1030a4ad58398826b29d3d408": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73ff14f5da5d4497a82e55279cf7c6b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0035381b00cb4ac9b208272050a3c38b",
              "IPY_MODEL_0f576a0c5ad741f6a82e57bbca766133",
              "IPY_MODEL_a0d8f05d0f7b40778f08dd0508e01b95"
            ],
            "layout": "IPY_MODEL_5aa096f7cf7143e59ae1555b5605cef3"
          }
        },
        "0035381b00cb4ac9b208272050a3c38b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c89f46af55449ea865388d5f0a3fc47",
            "placeholder": "​",
            "style": "IPY_MODEL_034e4963fe684f0fbfc23536de44cb25",
            "value": "Map: 100%"
          }
        },
        "0f576a0c5ad741f6a82e57bbca766133": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a54b74525be4537b769d90d79b9b4f5",
            "max": 10500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38d2d6ef3c6b4b1ab5a806012fc0109b",
            "value": 10500
          }
        },
        "a0d8f05d0f7b40778f08dd0508e01b95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_692e173ac3bf427bb27ff20989c7bf1a",
            "placeholder": "​",
            "style": "IPY_MODEL_93dcc3f0a1ce491b826b57b6df1e48e6",
            "value": " 10500/10500 [00:07&lt;00:00, 1617.79 examples/s]"
          }
        },
        "5aa096f7cf7143e59ae1555b5605cef3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c89f46af55449ea865388d5f0a3fc47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "034e4963fe684f0fbfc23536de44cb25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a54b74525be4537b769d90d79b9b4f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38d2d6ef3c6b4b1ab5a806012fc0109b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "692e173ac3bf427bb27ff20989c7bf1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93dcc3f0a1ce491b826b57b6df1e48e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "467a6a99fabf4f5f8c37bd87a97f2d37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9c26c656d07a47d78bcc452b5c3285af",
              "IPY_MODEL_e7a7ac5bccf6499a90c95d6771f13910",
              "IPY_MODEL_57ca60d1a4b84e1fae7034caced12099"
            ],
            "layout": "IPY_MODEL_bbf74723661044fda0a832298552d5cb"
          }
        },
        "9c26c656d07a47d78bcc452b5c3285af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_973ed182b1f14ed6be7970d4e2678e79",
            "placeholder": "​",
            "style": "IPY_MODEL_01a242f2cd274a3daa5a1a21617ea663",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "e7a7ac5bccf6499a90c95d6771f13910": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6621b54218854e4d9f9a1b6cb5a75d8d",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_15eec6e76b63413c8cb8bd00379d621f",
            "value": 48
          }
        },
        "57ca60d1a4b84e1fae7034caced12099": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_808ad8ff0c8a4a0eab823b32fdd6503e",
            "placeholder": "​",
            "style": "IPY_MODEL_ae2a59e7982542f6ba7ebab55ab9cbe6",
            "value": " 48.0/48.0 [00:00&lt;00:00, 610B/s]"
          }
        },
        "bbf74723661044fda0a832298552d5cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "973ed182b1f14ed6be7970d4e2678e79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01a242f2cd274a3daa5a1a21617ea663": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6621b54218854e4d9f9a1b6cb5a75d8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15eec6e76b63413c8cb8bd00379d621f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "808ad8ff0c8a4a0eab823b32fdd6503e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae2a59e7982542f6ba7ebab55ab9cbe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a0d0097137f4d94b490d3acfdb2cc71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e14e81d7562f4f70b72bfaf7f919e445",
              "IPY_MODEL_0fc8e57d2d1648c19a93fff6bcb95468",
              "IPY_MODEL_7c0cbf6d9f9040cfa170b3f43f3d8e2a"
            ],
            "layout": "IPY_MODEL_0a2b483525164d7c8350a6e94c36573e"
          }
        },
        "e14e81d7562f4f70b72bfaf7f919e445": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cf77318d87341ab8edeb59e8f2b0aaa",
            "placeholder": "​",
            "style": "IPY_MODEL_3e9446d67e57430596a427551daf5cae",
            "value": "config.json: 100%"
          }
        },
        "0fc8e57d2d1648c19a93fff6bcb95468": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5f26cc305d54274824df5118487f26a",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05d7396bf5f348fe97f5d2986c537aee",
            "value": 570
          }
        },
        "7c0cbf6d9f9040cfa170b3f43f3d8e2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f827b768769a4d52a874af47b65acce3",
            "placeholder": "​",
            "style": "IPY_MODEL_4e53a114668941ebadc2f38864a909d4",
            "value": " 570/570 [00:00&lt;00:00, 10.5kB/s]"
          }
        },
        "0a2b483525164d7c8350a6e94c36573e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cf77318d87341ab8edeb59e8f2b0aaa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e9446d67e57430596a427551daf5cae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5f26cc305d54274824df5118487f26a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05d7396bf5f348fe97f5d2986c537aee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f827b768769a4d52a874af47b65acce3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e53a114668941ebadc2f38864a909d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b65afa1aafb4460b909a60b45c83ff5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_97019f1cd07c494683833977b79094b2",
              "IPY_MODEL_83179dd508d641739dee74ba81362093",
              "IPY_MODEL_438556270ef741f0b54f3413e37f1a09"
            ],
            "layout": "IPY_MODEL_48f907265fcf4b08a0fe0113faeac155"
          }
        },
        "97019f1cd07c494683833977b79094b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d8634f690bc4939a4df56a4b1496286",
            "placeholder": "​",
            "style": "IPY_MODEL_b877e6a785a94012b23d2b4453576a7f",
            "value": "vocab.txt: 100%"
          }
        },
        "83179dd508d641739dee74ba81362093": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75bd744823944abdbc7d2ff3905a5f9c",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9fe5c99d30dc433bb7c244ccd13b691d",
            "value": 231508
          }
        },
        "438556270ef741f0b54f3413e37f1a09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f39326c0bf07459fb35a98edcc6c4dcb",
            "placeholder": "​",
            "style": "IPY_MODEL_25e08146d407465b9253ccb4cdb718eb",
            "value": " 232k/232k [00:00&lt;00:00, 3.09MB/s]"
          }
        },
        "48f907265fcf4b08a0fe0113faeac155": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d8634f690bc4939a4df56a4b1496286": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b877e6a785a94012b23d2b4453576a7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75bd744823944abdbc7d2ff3905a5f9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fe5c99d30dc433bb7c244ccd13b691d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f39326c0bf07459fb35a98edcc6c4dcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25e08146d407465b9253ccb4cdb718eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37ece95b21ab4deaa2f8c615724a266c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4901624e2277455f9bcf171362842650",
              "IPY_MODEL_2bcbefe6986a4c45b4c3ea414f8833f1",
              "IPY_MODEL_44f42dc002ca48c296f95c7854c4354a"
            ],
            "layout": "IPY_MODEL_0d6255c74c4247d787c8bd3845eaa9b0"
          }
        },
        "4901624e2277455f9bcf171362842650": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8ea451a4c3e44869794c25926936dfe",
            "placeholder": "​",
            "style": "IPY_MODEL_cfdc521b550b40ba8334ee269f39980d",
            "value": "tokenizer.json: 100%"
          }
        },
        "2bcbefe6986a4c45b4c3ea414f8833f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6010a85bb46e45fab35dcc9fc5c3ce42",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9580fbaefcca47e4a031a292c4810b4e",
            "value": 466062
          }
        },
        "44f42dc002ca48c296f95c7854c4354a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cfa03ce118d42f782ff629d8b9f9360",
            "placeholder": "​",
            "style": "IPY_MODEL_696e483a17c94da0b0921e0883615dea",
            "value": " 466k/466k [00:00&lt;00:00, 2.06MB/s]"
          }
        },
        "0d6255c74c4247d787c8bd3845eaa9b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8ea451a4c3e44869794c25926936dfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfdc521b550b40ba8334ee269f39980d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6010a85bb46e45fab35dcc9fc5c3ce42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9580fbaefcca47e4a031a292c4810b4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3cfa03ce118d42f782ff629d8b9f9360": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "696e483a17c94da0b0921e0883615dea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6fe0fa471d5c4da38ddd7103e6405173": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_24e79d9b468c4b34bfa9e98c8d62deb4",
              "IPY_MODEL_2b2410badb0546339ebb6818a9284a53",
              "IPY_MODEL_51865a4bcd2b4ca793a1a35a7ca58bf3"
            ],
            "layout": "IPY_MODEL_d078e1e5003a4a02972203414f9a2e2c"
          }
        },
        "24e79d9b468c4b34bfa9e98c8d62deb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1906ea5a4e64ce78ed2468dee20eb6a",
            "placeholder": "​",
            "style": "IPY_MODEL_e405d885425a44fe9c20d5335fa44d9d",
            "value": "model.safetensors: 100%"
          }
        },
        "2b2410badb0546339ebb6818a9284a53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99b8e5c1ae7b44848b6662693f6ab098",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_151429d9919847119871bde878c50097",
            "value": 440449768
          }
        },
        "51865a4bcd2b4ca793a1a35a7ca58bf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a353d6f3d8a64c47950545ea644d730c",
            "placeholder": "​",
            "style": "IPY_MODEL_b69e58038c764c4d92f8de1c0b361f23",
            "value": " 440M/440M [00:05&lt;00:00, 37.1MB/s]"
          }
        },
        "d078e1e5003a4a02972203414f9a2e2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1906ea5a4e64ce78ed2468dee20eb6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e405d885425a44fe9c20d5335fa44d9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99b8e5c1ae7b44848b6662693f6ab098": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "151429d9919847119871bde878c50097": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a353d6f3d8a64c47950545ea644d730c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b69e58038c764c4d92f8de1c0b361f23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78085c31a83048a4921e9b5c3ec24944": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d3dd4c68b7ef449b966f18ac3fa66ea9",
              "IPY_MODEL_bb1551dc2ccd4d099c18bd5cb1a66c6e",
              "IPY_MODEL_5f0b24fc57b14089b1f1e21cd826c41b"
            ],
            "layout": "IPY_MODEL_281ee20d4a454f90965f4ce7987737d0"
          }
        },
        "d3dd4c68b7ef449b966f18ac3fa66ea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a99fe19a64142ca8470f57b9f275805",
            "placeholder": "​",
            "style": "IPY_MODEL_fabbf68ab67e445faa9ab9db2de8ca43",
            "value": "spiece.model: 100%"
          }
        },
        "bb1551dc2ccd4d099c18bd5cb1a66c6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6912e0ac7f34aa8bf86a88755a39657",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_daa262d542d742e0b73981b9b65499ed",
            "value": 791656
          }
        },
        "5f0b24fc57b14089b1f1e21cd826c41b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19f483a47f6d48d4b37f4940a2ca099b",
            "placeholder": "​",
            "style": "IPY_MODEL_7972d8da99974d9584897d7a4242a4f0",
            "value": " 792k/792k [00:00&lt;00:00, 3.81MB/s]"
          }
        },
        "281ee20d4a454f90965f4ce7987737d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a99fe19a64142ca8470f57b9f275805": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fabbf68ab67e445faa9ab9db2de8ca43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6912e0ac7f34aa8bf86a88755a39657": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "daa262d542d742e0b73981b9b65499ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "19f483a47f6d48d4b37f4940a2ca099b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7972d8da99974d9584897d7a4242a4f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "120a5391ac734214a9a6daac1fc3c176": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3059e99b914b4ee18eaadb2813901f38",
              "IPY_MODEL_66c5f41624b9407b91d12461085de80c",
              "IPY_MODEL_d82fda7ab884493fa8db5404f6c3616e"
            ],
            "layout": "IPY_MODEL_c1f193a963c949cc952638a6a7a1bd2a"
          }
        },
        "3059e99b914b4ee18eaadb2813901f38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_608ff6b0d2e54b608ae8ec4c31b50e45",
            "placeholder": "​",
            "style": "IPY_MODEL_261fcf6034f84b838096c9c56933c03e",
            "value": "tokenizer.json: 100%"
          }
        },
        "66c5f41624b9407b91d12461085de80c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb880be30710473f9ba6b3da5573759b",
            "max": 1389353,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_091786de70b44567b637c6a253255d3e",
            "value": 1389353
          }
        },
        "d82fda7ab884493fa8db5404f6c3616e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2692dcb2da049329eb8f9fc44cab002",
            "placeholder": "​",
            "style": "IPY_MODEL_c9fceff872b04bba8e6871d3e2df3c23",
            "value": " 1.39M/1.39M [00:00&lt;00:00, 3.97MB/s]"
          }
        },
        "c1f193a963c949cc952638a6a7a1bd2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "608ff6b0d2e54b608ae8ec4c31b50e45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "261fcf6034f84b838096c9c56933c03e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb880be30710473f9ba6b3da5573759b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "091786de70b44567b637c6a253255d3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b2692dcb2da049329eb8f9fc44cab002": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9fceff872b04bba8e6871d3e2df3c23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "664a28fe55b14a5a9ce87d78e1e0e975": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_700a89df2c9c48a8abb4b8af68eb9753",
              "IPY_MODEL_5f4e7c02c0d84e7d84ab7aea8ddf0c65",
              "IPY_MODEL_0daa9a02c8c541fdae8f8fefa82b6236"
            ],
            "layout": "IPY_MODEL_35dbeef556644c62a8f9fffe585350c6"
          }
        },
        "700a89df2c9c48a8abb4b8af68eb9753": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3605eb2aaf5f45f8b9cff2f6ad721913",
            "placeholder": "​",
            "style": "IPY_MODEL_77c0827783b448328eea98a70bc48c1b",
            "value": "config.json: 100%"
          }
        },
        "5f4e7c02c0d84e7d84ab7aea8ddf0c65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abf6b7a2297141b783906a9144f79556",
            "max": 1208,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fdc5785ec53f485c8836aaae0c08afec",
            "value": 1208
          }
        },
        "0daa9a02c8c541fdae8f8fefa82b6236": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d103a146b3ed407196598133dcc94728",
            "placeholder": "​",
            "style": "IPY_MODEL_d1dd141d7c5e45ad8efa1683fd68d449",
            "value": " 1.21k/1.21k [00:00&lt;00:00, 38.2kB/s]"
          }
        },
        "35dbeef556644c62a8f9fffe585350c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3605eb2aaf5f45f8b9cff2f6ad721913": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77c0827783b448328eea98a70bc48c1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abf6b7a2297141b783906a9144f79556": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdc5785ec53f485c8836aaae0c08afec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d103a146b3ed407196598133dcc94728": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1dd141d7c5e45ad8efa1683fd68d449": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4db376fb82b4464db2124cf721331d75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_970b50291e6048e082430f38bdc78ba6",
              "IPY_MODEL_bb819daea1374a28924d63b57c34d6ba",
              "IPY_MODEL_c480612175b0400a9b4ee12f5dd59d4f"
            ],
            "layout": "IPY_MODEL_7e5a8b7441854627bc8673e64d35a227"
          }
        },
        "970b50291e6048e082430f38bdc78ba6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3fbbeac6a234e92bd101708f78d5947",
            "placeholder": "​",
            "style": "IPY_MODEL_b932fdbef0c0452280321e20aa3b4088",
            "value": "model.safetensors: 100%"
          }
        },
        "bb819daea1374a28924d63b57c34d6ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb6c3022107445aaad2bf67a29281da8",
            "max": 891646390,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3dcd12b0b05f487490697d77a185cf9d",
            "value": 891646390
          }
        },
        "c480612175b0400a9b4ee12f5dd59d4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c771e1a7bb6c4d72a886cd26127d92d7",
            "placeholder": "​",
            "style": "IPY_MODEL_c623554889f44d5aa6861a67e2dc3bc8",
            "value": " 892M/892M [00:07&lt;00:00, 222MB/s]"
          }
        },
        "7e5a8b7441854627bc8673e64d35a227": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3fbbeac6a234e92bd101708f78d5947": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b932fdbef0c0452280321e20aa3b4088": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb6c3022107445aaad2bf67a29281da8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3dcd12b0b05f487490697d77a185cf9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c771e1a7bb6c4d72a886cd26127d92d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c623554889f44d5aa6861a67e2dc3bc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07a072b8dfa646d29bdde0d10d0bbc29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_136e9cf52693436e8bca0861880f2f1d",
              "IPY_MODEL_8d7e3a469e4f4e3ab81723b869a15de2",
              "IPY_MODEL_78436bdd9a244e41bd6d6dce32e86bf7"
            ],
            "layout": "IPY_MODEL_ed5189f38f874dc68276fe829e803fb0"
          }
        },
        "136e9cf52693436e8bca0861880f2f1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9771ab5a34e4a04b05b7c96466c5ee5",
            "placeholder": "​",
            "style": "IPY_MODEL_fc603cc1cb7247dfba032009f2171fb3",
            "value": "generation_config.json: 100%"
          }
        },
        "8d7e3a469e4f4e3ab81723b869a15de2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_395c986e8424464db583c8a5d759716c",
            "max": 147,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cc3745357f1b4e2da1a3df7e2b1dc6a8",
            "value": 147
          }
        },
        "78436bdd9a244e41bd6d6dce32e86bf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_783eb1a7aa4c495b8d24d76fb167b834",
            "placeholder": "​",
            "style": "IPY_MODEL_27d74f3875b845918374fcdd9e5aef75",
            "value": " 147/147 [00:00&lt;00:00, 10.1kB/s]"
          }
        },
        "ed5189f38f874dc68276fe829e803fb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9771ab5a34e4a04b05b7c96466c5ee5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc603cc1cb7247dfba032009f2171fb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "395c986e8424464db583c8a5d759716c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc3745357f1b4e2da1a3df7e2b1dc6a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "783eb1a7aa4c495b8d24d76fb167b834": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27d74f3875b845918374fcdd9e5aef75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}